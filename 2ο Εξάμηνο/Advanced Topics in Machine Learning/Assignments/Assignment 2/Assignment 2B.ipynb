{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Assignment 2B.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-25adb20f",
   "language": "python",
   "display_name": "PyCharm (Assignment 2)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "vX1Q82iPi7nj",
    "colab_type": "text"
   },
   "source": [
    "![alt text](https://www.auth.gr/sites/default/files/banner-horizontal-282x100.png)\n",
    "# Advanced Topics in Machine Learning - Assignment 2 - Part B\n",
    "\n",
    "\n",
    "## Multi Instance Learning\n",
    "\n",
    "In this part we try to solve a multi instance learning problem, using the \"Delicous\" dataset from the \"MLTM\" repository mentioned below.\n",
    "\n",
    "The approach we follow is to consider each line of the dataset as a bag of instances and transform these data to a standard supervised classification problem, by first clustering all instances of all bugs using K-Means algorithm in order then to use this information as the feature set to input in the SVM classification algorithm.\n",
    "\n",
    "#### Useful library documentation, references, and resources used on Assignment:\n",
    "\n",
    "* DeliciousMIL dataset: <https://github.com/hsoleimani/MLTM/tree/master/Data/Delicious>\n",
    "* scikit-learn ML library (aka *sklearn*): <http://scikit-learn.org/stable/documentation.html>\n",
    "* K-Means clustering: <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html>\n",
    "* SVM classifier: <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "cxOwsz5xi7nm",
    "colab_type": "text"
   },
   "source": [
    "# 0. __Install packages - Import necessary libraries__"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "gnp7kUMAi7nm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics, cluster\n",
    "from sklearn import svm\n",
    "from pandas import DataFrame"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "V0LzS4upi7nv",
    "colab_type": "text"
   },
   "source": [
    "# 1. __Set desired configuration parameters__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-cfq0JV1nEJ",
    "colab_type": "text"
   },
   "source": [
    "### Define the run configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uf7tY5s9wCMt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Whether it should download the required dataset from github or not.\n",
    "# If set to True, the DATA_PATH must be \"MLTM/Data/Delicious/\"\n",
    "SHOULD_DOWNLOAD_DATASET = False\n",
    "\n",
    "# If the dataset is downloaded from the internet, \n",
    "# the DATA_PATH must be \"MLTM/Data/Delicious/\"\n",
    "# Otherwise it can point to any other local folder that contains the data sets.\n",
    "#DATA_PATH = \"MLTM/Data/Delicious/\"\n",
    "DATA_PATH = \"raw_data\"\n",
    "\n",
    "# The number of clusters to use in the K-Means algorithm.\n",
    "N_CLUSTERS = 25\n",
    "\n",
    "# The portion of documents to keep from the training data. \n",
    "# If set to -1, then the whole set is used and not a portion of it.\n",
    "N_TRAIN_PORTION = 1000\n",
    "\n",
    "# The portion of documents to keep from the test data.\n",
    "# If set to -1, then the whole set is used and not a portion of it.\n",
    "N_TEST_PORTION = 1000"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "plTAeAudi7nr",
    "colab_type": "text"
   },
   "source": [
    "# 2. __Define the required dataset__\n",
    "We will use the *DeliciousMIL* dataset from its GitHub repository that is mentioned above. The dataset consists of 4 separate data files and is optionally downloaded from GitHub based on the configuration parameters above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF7oMPDqzItE",
    "colab_type": "text"
   },
   "source": [
    "### Download the datasets from the Internet (optional)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zVvtA4LNzkbr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "if SHOULD_DOWNLOAD_DATASET:\n",
    "  !git clone https://github.com/hsoleimani/MLTM.git"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlCw2lwCzpq_",
    "colab_type": "text"
   },
   "source": [
    "### Define the paths to the target files that contain the train and test data and labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Wnvh6I_Ai7nr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_data_filename = os.path.join(DATA_PATH, 'train-data.dat')\n",
    "train_labels_filename = os.path.join(DATA_PATH, 'train-label.dat')\n",
    "test_data_filename = os.path.join(DATA_PATH, 'test-data.dat')\n",
    "test_labels_filename = os.path.join(DATA_PATH, 'test-label.dat')"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "uTPSdvZki7nv",
    "colab_type": "text"
   },
   "source": [
    "#3. Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data_file(filename):\n",
    "    \"\"\"\n",
    "    Preprocess a file that contains data in order to bring it in a form where each line consists only from an\n",
    "    array of instances.\n",
    "\n",
    "    :param filename: The path to the target file.\n",
    "    :return: an array that contains the cleaned version of the target file.\n",
    "    \"\"\"\n",
    "    raw_file = open(filename).readlines()\n",
    "    clean_file = []\n",
    "    for line in raw_file:\n",
    "        # Remove the first two <##> entries\n",
    "        line = re.sub('<[0-9]+>', '', line, 2).strip()\n",
    "        # Split the rest of the line by the remaining <##> entries\n",
    "        line = [x.strip() for x in re.split('<[0-9]+>', line)]\n",
    "        clean_file.append(line)\n",
    "\n",
    "    return clean_file\n",
    "\n",
    "def preprocess_labels_file(filename):\n",
    "    \"\"\"\n",
    "    Preprocess a file that contains labels for many classes in order to keep only those that correspond\n",
    "    to the most frequent class.\n",
    "\n",
    "    :param filename: The path to the target file.\n",
    "    :return: a list that contains the labels of the most frequent class.\n",
    "    \"\"\"\n",
    "    data_frame = pd.read_csv(filename, delimiter=' ', header=None)\n",
    "    # Find the index of column with the max sum() and keep only this one.\n",
    "    labels_of_top_class = data_frame.iloc[:][data_frame.sum(axis=0).idxmax()]\n",
    "\n",
    "    return list(labels_of_top_class)\n",
    "\n",
    "def get_portion(size, array_a, array_b):\n",
    "    \"\"\"\n",
    "    Extracts only a randomly selected portion of specified size from the two input arrays.\n",
    "\n",
    "    :param size: The size of the portion to extract. If the number -1 is passed, then the two arrays are not manipulated\n",
    "    and the whole data for each of them is returned.\n",
    "    :param array_a: The contents of the first array. It is expected to represent the contents of the data file.\n",
    "    :param array_b: The contents of the second array. It is expected to represent the contents of the labels file.\n",
    "    :return: two arrays where each one corresponds to the extracted portion of the relevant input arrays.\n",
    "    \"\"\"\n",
    "    if size is -1:\n",
    "        return (array_a, array_b)\n",
    "\n",
    "    # Randomly select (wo replacement) the indices of samples to keep\n",
    "    keep_idx = sorted(np.random.choice(len(array_a), size=size, replace=False))\n",
    "\n",
    "    array_a_portion = [array_a[x] for x in keep_idx]\n",
    "    array_b_portion = [array_b[x] for x in keep_idx]\n",
    "\n",
    "    return (array_a_portion, array_b_portion)\n",
    "\n",
    "def create_dataframe(bags, classes):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame from an array where each line is considered to contain a bag of instances.\n",
    "\n",
    "    :param bags: Array which has m lines, that are supposed to be the bags and each line has n arrays which\n",
    "    are supposed to be the instances.\n",
    "    :param classes: Array which contains the target class for each m line of the bags array.\n",
    "    :return: A pandas DataFrame that contains the columns: [<bagIndex>, <instance>, <class>].\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for bagIndex, bag in enumerate(bags):\n",
    "        for instanceIndex, instance in enumerate(bag):\n",
    "            bag = {'bagIndex': bagIndex, 'instance': instance, 'class': classes[bagIndex]}\n",
    "            data.append(bag)\n",
    "\n",
    "    return DataFrame(data)\n",
    "\n",
    "def calculate_clusters(instances):\n",
    "    \"\"\"\n",
    "    Calculates the clusters of the input array by first computing the TF-IDF vector of each line and\n",
    "    then uses K-Means algorithm by leveraging that vector.\n",
    "\n",
    "    :param instances: The array of data to perform clustering against.\n",
    "    :return: An array that contains the cluster that each line of the input array has been assigned to.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    vectorized_data = vectorizer.fit_transform(instances)\n",
    "    model = cluster.KMeans(n_clusters=N_CLUSTERS, random_state=0)\n",
    "    clusters = model.fit_predict(vectorized_data)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def add_clusters_to_dataframe(clusters, target_data_frame):\n",
    "    \"\"\"\n",
    "    Adds a new column  to the target pandas DataFrame that contains the assigned cluster of each line.\n",
    "\n",
    "    :param clusters: The calculated clusters.\n",
    "    :param target_data_frame: The target pandas DataFrame to append the new column.\n",
    "    \"\"\"\n",
    "    target_data_frame['cluster'] = pd.Series(clusters, index=target_data_frame.index)\n",
    "\n",
    "def get_features_from_cluster(cluster_index):\n",
    "    \"\"\"\n",
    "    Creates an array of zeros of n_klusters size and assigns the value 1 only to the index that is specified\n",
    "    in the incoming parameter.\n",
    "\n",
    "    :param cluster_index: The index of the array to set to 1. It is supposed to reflect the cluster index that an\n",
    "    instance is assigned to.\n",
    "    :return: The array like this example: [0, 1, 0, 0]\n",
    "    \"\"\"\n",
    "    features = [0] * N_CLUSTERS\n",
    "    features[cluster_index] = 1\n",
    "\n",
    "    return features\n",
    "\n",
    "def add_features_to_dataframe(target_data_frame):\n",
    "    \"\"\"\n",
    "    Adds a new column to the target pandas DataFrame that contains the features of this line.\n",
    "\n",
    "    :param target_data_frame: The target pandas DataFrame to append the new column.\n",
    "    \"\"\"\n",
    "    target_data_frame['features'] = target_data_frame.apply(lambda row: get_features_from_cluster(row.cluster), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqkY42jAwjzP",
    "colab_type": "text"
   },
   "source": [
    "# 4. Execute the required operations to perform the classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA1oL3L4wrBg",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "### Preprocess the raw data in order to create the necessary train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4_5ipVjSxEsx",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "X_train = preprocess_data_file(train_data_filename)\n",
    "y_train = preprocess_labels_file(train_labels_filename)\n",
    "X_test = preprocess_data_file(test_data_filename)\n",
    "y_test = preprocess_labels_file(test_labels_filename)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dT7eS50MxFXE",
    "colab_type": "text"
   },
   "source": [
    "### For speeding up the procedure just keep a portion of the data sets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eGPW7JtYxGyt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "X_train_portion, y_train_portion = get_portion(N_TRAIN_PORTION, X_train, y_train)\n",
    "X_test_portion, y_test_portion = get_portion(N_TEST_PORTION, X_test, y_test)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzhUuaIFxIRf",
    "colab_type": "text"
   },
   "source": [
    "### Create a pandas DataFrame from the train data set, that contains the calculated clusters as well as their convertion to features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S3f1ykbcxJgr",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_dataframe = create_dataframe(X_train_portion, y_train_portion)\n",
    "add_clusters_to_dataframe(calculate_clusters(train_dataframe.instance), train_dataframe)\n",
    "add_features_to_dataframe(train_dataframe)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzX9FXnlxKh1",
    "colab_type": "text"
   },
   "source": [
    "### Create a pandas DataFrame from the test data set, that contains the calculated clusters as well as their convertion to features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VNaYoCQqxLii",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "test_dataframe = create_dataframe(X_test_portion, y_test_portion)\n",
    "add_clusters_to_dataframe(calculate_clusters(test_dataframe.instance), test_dataframe)\n",
    "add_features_to_dataframe(test_dataframe)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00AJjlOwxMeX",
    "colab_type": "text"
   },
   "source": [
    "### Create an SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6MjERwPBxNkb",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "svmModel = svm.SVC(C=0.1, kernel='poly', degree=2)\n",
    "svmModel.fit(train_dataframe.features.tolist(), train_dataframe['class'])\n",
    "y_predicted = svmModel.predict(test_dataframe.features.tolist())"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEp98AylxPoC",
    "colab_type": "text"
   },
   "source": [
    "### Calculate the metrics of our classification process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vG24V1XkxQt5",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "accuracy = metrics.accuracy_score(test_dataframe['class'].tolist(), y_predicted)\n",
    "recall = metrics.recall_score(test_dataframe['class'].tolist(), y_predicted, average=\"macro\")\n",
    "precision = metrics.precision_score(test_dataframe['class'].tolist(), y_predicted, average=\"macro\")\n",
    "f1 = metrics.f1_score(test_dataframe['class'].tolist(), y_predicted, average=\"macro\")"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_De9qauxRx5",
    "colab_type": "text"
   },
   "source": [
    "### Print the metrics results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "45P1buwKxSqo",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "outputId": "21b632bb-51f5-4882-9e50-db070b96c43d"
   },
   "source": [
    "print(\"Accuracy: %f\" % accuracy)\n",
    "print(\"Recall: %f\" % recall)\n",
    "print(\"Precision: %f\" % precision)\n",
    "print(\"F1: %f\" % f1)"
   ],
   "execution_count": 88,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy: 0.589995\n",
      "Recall: 0.500000\n",
      "Precision: 0.294998\n",
      "F1: 0.371067\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4TvPLkZ3-Yf",
    "colab_type": "text"
   },
   "source": [
    "# 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is58S9ap4FDd",
    "colab_type": "text"
   },
   "source": [
    "In this experiment we tried to transform the Multi Instance classification problem to a regular supervised classification problem, using K-Means clustering and SVM classifier. In order to let the classification run quickly, we decided to use only a portion of the train and test data sets.\n",
    "\n",
    "The results of our metrics show a moderate value of the _accuracy_ score, which in our tests was around 60% but not good values for _recall_ (~50%), _precision_ (~30%) and _f1_ (~30%) metrics.\n",
    "\n",
    "We tried to experiment with the K value of the K-Means algorithm and we didn't make it to achieve better results than this, regardless of the portion of the train data set we used.\n",
    "\n",
    "We could probably consider that the kind of transformation we used is not the best possible and maybe another kind of transformation, using k-medoids or even an approach without using a transformation like the citation-KNN algorithm may perform better."
   ]
  }
 ]
}