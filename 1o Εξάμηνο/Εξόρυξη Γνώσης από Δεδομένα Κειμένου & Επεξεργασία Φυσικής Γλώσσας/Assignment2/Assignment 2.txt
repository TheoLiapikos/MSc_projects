Assignment 2: Text Classification

Assignment 2: Text Classification with Word Embedding Models

The goal of this assignment is to learn how to use the Gensim library to deal with word embeddings in Python, learn how to evaluate pre-trained word-embedding models and get experience in training a neural document classifier in keras using word embeddings as building blocks for feature sets.

You will implement a document classifier based on a feed-forward neural network, using continuous bags-of-words as features. Primary and recommended machine learning framework is Keras with TensorFlow as a backend.

The assignment is divided in 3 parts.
Part 1: Basic operations with word embeddings

    Train a Word2Vec model on the WikiText dataset (Use the wiki.train.tokens corpus in the WikiText-103 word level corpus, which can be found here: https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/). Find the 5 most similar word pairs from the 10 most frequent words using a Gensim’s function.
    Then, implement a function that retrieves two word vectors using the indexing operator as described in Gensim’s documentation and computes their cosine distance. You may be interested in np.dot() and np.linalg.norm() - see the numpy documentation for details. Compare this to the distance computed by Gensim's functions.
    Visit the NLPL word embedding repository (http://vectors.nlpl.eu/repository/) and download the models with the following identifiers: 

    40. It was trained on the English CoNLL17 corpus, using Continuous Skip-gram algorithm with vector size 100, and window size 10.
    75. It was trained on the English Oil and Gas corpus, using Continuous Bag-of-Words algorithm with vector size 400, and window size 5.
    82. It was trained on the English Common Crawl Corpus, using GloVe algorithm with vector size 300, and window size 10.

    Use your trained and above downloaded pre-trained embeddings to compare the lists of top 20 most frequent words in WikiText, CoNLL17, Oil and Gas, and Common Crawl corpora.

    In Gensim, one can access the model vocabulary sorted by frequency as MODEL.index2word list.
    If a model is saved in the native Gensim format, one can also access the words frequencies in the training corpus (as integers) with the MODEL.wv.vocab[’YOUR_WORD’].count method.
    Note also that if a model is saved as a text file in the word2vec format, its lines are as a rule sorted by frequency as well. Thus, top 20 lines of the file correspond to 20 most frequent words in the training corpus

    Are the lists different? What does it tell you about the nature of these corpora?
    Project top 1000 words from the WikiText corpus in 2d space using t-SNE plot. Find an interesting cluster in the plot.

Part 2: Training a word embedding model

    Train a word embedding model on in-domain data. Precisely, you will train a word embedding model on the sentence classification corpus from the UCI Machine Learning repository, that you also used in the previous assignment.

    Choose any hyperparameters you like, but the resulting model should have at least 4,000 words in its vocabulary.

    Train another model on the same data, but with one hyperparameter different (for example, window size or vector size).
    Do you see any difference in the performance of 2 models? How in your opinion it is related to the changes you’ve made to hyperparameters?

Part 3: Document classification with word embeddings

Distributional semantic models can represent entities larger than words: phrases, sentences and whole documents. This makes it possible to train semantically aware document classifiers. In this task, you will have to train such a classifier using deep neural networks and the same English word embedding models that you got familiar with before.

The classification task itself is the same to that of the previous Assignment, namely the sentence classification corpus from the UCI Machine Learning repository. This corpus contains sentences from the abstract and introduction of 30 scientific articles that come from three different domains (PLOS, ARXIV, JDM) and are classified to five distinct classes (AIMX, OWNX, CONT, BASE, MISC).

The task is again to predict the document class (source) based on the words occurring in this document. This time, we would like you to work with dense low-dimensional document vectors and not with sparse high-dimensional vectors produced by the bag of words approach that was followed in the previous assignment.

Your task is to come up with semantically-aware representations of the documents in the training dataset and to create classifiers able to predict the document class using these representations. The classifiers (again) are supposed to be feed-forward neural networks, but this time they should take as an input not one-hot word representations, but continuous word embeddings.

  3.1  Use the trained / pre-trained word embedding model as an Embedding() layer in Keras, and then perform the summation or averaging of the resulting vector arrays for each document in the computation graph itself, using the Keras built-in functions (Add(), Average(), etc). Note that Gensim supports direct conversion of loaded word embedding models into Keras layers via the get_keras_embedding() method.
  3.2  Implement a version of the classifier which does not rely on any pre-trained word embedding. It should extract the vocabulary of the desired size from the training set, and initialize an Embedding() layer with random vectors for the extracted words. These vectors are then considered to be the parameters of the neural network and are trained along with other weight matrices, thus optimizing word embeddings for this particular classification task. Report on the changes in classification performance and in the training time with this approach.
  3.3  You should experiment with different activation functions, layer dimensionalities and regularization terms, and report the accuracy achieved by the different feed-forward neural network models. You should report Accuracy, macro Recall, macro Precision, and macro F1 score and plots of learning curves on train & test data. Recall that probabilistic classifiers sometimes can produce different results with the same hyperparameters because of different random initializations. Thus, train your best classification architecture 3 times and evaluate it 3 times, reporting the average and the standard deviation. 

Logistics

Assignments are for teams of 2-3 students. For all parts you should deliver a Google Colaboratory Python 3 notebook using text cells to appropriately document your experiments, results and answers to the questions posted in this assignment. You should collaboratively work on GitHub, so that the contribution (commits) of each team member can be measured. Team formation, initial repository acquisition and submission is handled via GitHub classroom. Deadline: 25/1/2019, 23:00. Delayed submissions get a 10% penalty per day, up to 2 days.

Published by Google Drive–Report Abuse–Updated automatically every 5 minutes




Επειδή υπήρξαν ερωτήσεις σχετικά με την δεύτερη εργασία, επαναδιατυπώνουμε τα εξής σημεία. 
 
Στο part2, για την παραμετροποίηση του word2vec μοντέλου να μην συνυπολογιστεί όριο λεξιλογίου.
Επιπλέον, για την αξιολόγηση των δύο word2vec μοντέλων να αξιολογηθεί ο χρόνος εκπαίδευσης και αποτελέσματα λεξιλογίου μέσω συναρτήσεων της gensim (π.χ.: most_similar(), κ.α.).
 
Στο part 3, το trained μοντέλο αναφέρεται στo μοντέλο από το part2, ενώ τα pretrained modes, σε αυτα που αναφέρονται στο part1.
 
Σκοπός της εργασίας είναι να πειραματιστείτε σε όσα διδαχθήκατε στη θεωρία, στο εργαστήριο ΚΑΙ πέρα αυτών. Βαθμολογείστε για τη σωστή διαδικασία που ακολουθείτε, τα πειράματα σας και τη δικαιολόγηση των αποτελεσμάτων (όποια κι αν είναι αυτά). Χαμηλό accuracy, είναι κάτι που μπορεί να συμβεί σε πραγματικά δεδομένα, και εξαρτάται από τη μέθοδο, τα δεδομένα, κλπ.
