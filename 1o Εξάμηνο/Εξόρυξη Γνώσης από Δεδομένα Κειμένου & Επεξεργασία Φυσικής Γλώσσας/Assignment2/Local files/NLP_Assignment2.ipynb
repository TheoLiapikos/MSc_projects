{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "V8h7BXNa_AHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://www.auth.gr/sites/default/files/banner-horizontal-282x100.png)\n",
        "# Text Mining and Natural Language Processing - Assignment 2\n",
        "\n",
        "\n",
        "** Text Classification using Word Embeddings**\n",
        "<br>\n",
        "**Useful library documentation, references, and resources used on Assignment**:\n",
        "\n",
        "* Numpy numerical array library: <https://docs.scipy.org/doc/>\n",
        "* Gensim's word2vec: <https://radimrehurek.com/gensim/models/word2vec.html>\n",
        "* Keras Deep-Learning library: <https://keras.io/layers/embeddings/>\n",
        "* Bokeh interactive plots: <http://bokeh.pydata.org/en/latest/> (we provide plotting code here, but click the thumbnails for more examples to copy-paste)\n",
        "* scikit-learn ML library (aka `sklearn`): <http://scikit-learn.org/stable/documentation.html>\n",
        "* nltk NLP toolkit: <http://www.nltk.org/>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DpiibVTO3lMG",
        "colab_type": "code",
        "outputId": "5902e056-3dc0-4bac-f5a7-df80bdb66268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import bokeh\n",
        "import gensim\n",
        "import numpy as np\n",
        "import re\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GsG--w-sOPzJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 1"
      ]
    },
    {
      "metadata": {
        "id": "8ywrCrDahvTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1. Train a Word2Vec model on the WikiText dataset"
      ]
    },
    {
      "metadata": {
        "id": "fJztXrJRMy00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Download Dataset\n",
        "\n",
        "One could skip the next (time consuming) stages and upload the already pre-processed clean data directly from my personal Google Drive at section 1.1.3.3"
      ]
    },
    {
      "metadata": {
        "id": "FmnIJSbRdgWx",
        "colab_type": "code",
        "outputId": "ac45c557-d089-43f6-ed51-808a865c8fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import necessary Libraries\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Download the dataset ~190MB\n",
        "urllib.request.urlretrieve(\"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\", filename=\"wikitext-103-v1.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('wikitext-103-v1.zip', <http.client.HTTPMessage at 0x7fb34a3dd048>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "u6anXhJHeMkZ",
        "colab_type": "code",
        "outputId": "308db6c5-10f0-4722-cff1-82c24ac89433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Extract only the data of interest\n",
        "# From the .zip file open and read only the training tokens\n",
        "with zipfile.ZipFile('wikitext-103-v1.zip', 'r') as z:\n",
        "  doc = z.open('wikitext-103/wiki.train.tokens', 'r').read()\n",
        "\n",
        "# The first 500 bytes of data\n",
        "print(doc[:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b' \\n = Valkyria Chronicles III = \\n \\n Senj\\xc5\\x8d no Valkyria 3 : <unk> Chronicles ( Japanese : \\xe6\\x88\\xa6\\xe5\\xa0\\xb4\\xe3\\x81\\xae\\xe3\\x83\\xb4\\xe3\\x82\\xa1\\xe3\\x83\\xab\\xe3\\x82\\xad\\xe3\\x83\\xa5\\xe3\\x83\\xaa\\xe3\\x82\\xa23 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TiCt3-UgM5IK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "Ylhj68BonFRe",
        "colab_type": "code",
        "outputId": "ae38c520-ff8a-4d9e-f300-3b76c3c03c29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# Convert bytes to string and then split to paragraphs\n",
        "doc_str = doc.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')\n",
        "\n",
        "# the first 5 paragraphs\n",
        "print(doc_para[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' ', ' = Valkyria Chronicles III = ', ' ', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . ', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C11cpwv1Q1TE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.2.1 Basic pre-processing procedure\n",
        "(***BEWARE!*** Extremly long procedure (> 4 h).  Download already preprocessed dara from this link:\n",
        "[preproc_clean_data.csv](https://drive.google.com/open?id=10c2fnKtjrxtW1VZ4W1yqUqxcFaupAefE))\n",
        "\n",
        "For each paragraph of the data:\n",
        "- Remove multiple space characters\n",
        "- Remove empty tokens\n",
        "- Lower the characters\n",
        "- Remove non-Alpharithmetic characters\n",
        "- Remove arithmetic words\n",
        "- Remove created multiple space characters\n",
        "- Tokenize (split on space characters)\n",
        "- Remove stop-words\n",
        "- Remove words with less than 3 characters\n",
        "- Remove empty paragraphs\n",
        "- Save the remaining tokens to a list\n"
      ]
    },
    {
      "metadata": {
        "id": "JyTt3FjVQzzN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc_para_noEmpties = []\n",
        "for para in doc_para:\n",
        "    para = re.sub(r'\\s+', ' ',para)\n",
        "    if para != ' ':\n",
        "        para = para.lower()\n",
        "        para = re.sub(r'[^a-z0-9]+', ' ',para)\n",
        "        para = re.sub(r' [0-9]+ ', ' ',para)\n",
        "        para = re.sub(r'\\s+', ' ',para)\n",
        "        para = para.split(' ')\n",
        "        para = [word for word in para if word not in stopwords.words('english')]\n",
        "        para = [word for word in para if len(word)>2]\n",
        "        if len(para) == 0:\n",
        "          continue\n",
        "        doc_para_noEmpties.append(para)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_SdSqbbTrvUw",
        "colab_type": "code",
        "outputId": "805f3ab3-164f-4f88-994b-0b71f430e79e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# The first 6 paragraphs of clean data\n",
        "print(doc_para_noEmpties[:6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['valkyria', 'chronicles', 'iii'], ['senj', 'valkyria', 'unk', 'chronicles', 'japanese', 'lit', 'valkyria', 'battlefield', 'commonly', 'referred', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'sega', 'media', 'vision', 'playstation', 'portable', 'released', 'january', 'japan', 'third', 'game', 'valkyria', 'series', 'employing', 'fusion', 'tactical', 'real', 'time', 'gameplay', 'predecessors', 'story', 'runs', 'parallel', 'first', 'game', 'follows', 'nameless', 'penal', 'military', 'unit', 'serving', 'nation', 'gallia', 'second', 'europan', 'war', 'perform', 'secret', 'black', 'operations', 'pitted', 'imperial', 'unit', 'unk', 'raven'], ['game', 'began', 'development', 'carrying', 'large', 'portion', 'work', 'done', 'valkyria', 'chronicles', 'retained', 'standard', 'features', 'series', 'also', 'underwent', 'multiple', 'adjustments', 'making', 'game', 'forgiving', 'series', 'newcomers', 'character', 'designer', 'unk', 'honjou', 'composer', 'hitoshi', 'sakimoto', 'returned', 'previous', 'entries', 'along', 'valkyria', 'chronicles', 'director', 'takeshi', 'ozawa', 'large', 'team', 'writers', 'handled', 'script', 'game', 'opening', 'theme', 'sung', 'may'], ['met', 'positive', 'sales', 'japan', 'praised', 'japanese', 'western', 'critics', 'release', 'received', 'downloadable', 'content', 'along', 'expanded', 'edition', 'november', 'year', 'also', 'adapted', 'manga', 'original', 'video', 'animation', 'series', 'due', 'low', 'sales', 'valkyria', 'chronicles', 'valkyria', 'chronicles', 'iii', 'localized', 'fan', 'translation', 'compatible', 'game', 'expanded', 'edition', 'released', 'media', 'vision', 'would', 'return', 'franchise', 'development', 'valkyria', 'azure', 'revolution', 'playstation'], ['gameplay'], ['previous', 'unk', 'chronicles', 'games', 'valkyria', 'chronicles', 'iii', 'tactical', 'role', 'playing', 'game', 'players', 'take', 'control', 'military', 'unit', 'take', 'part', 'missions', 'enemy', 'forces', 'stories', 'told', 'comic', 'book', 'like', 'panels', 'animated', 'character', 'portraits', 'characters', 'speaking', 'partially', 'voiced', 'speech', 'bubbles', 'partially', 'unvoiced', 'text', 'player', 'progresses', 'series', 'linear', 'missions', 'gradually', 'unlocked', 'maps', 'freely', 'scanned', 'replayed', 'unlocked', 'route', 'story', 'location', 'map', 'varies', 'depending', 'individual', 'player', 'approach', 'one', 'option', 'selected', 'sealed', 'player', 'outside', 'missions', 'player', 'characters', 'rest', 'camp', 'units', 'customized', 'character', 'growth', 'occurs', 'alongside', 'main', 'story', 'missions', 'character', 'specific', 'sub', 'missions', 'relating', 'different', 'squad', 'members', 'game', 'completion', 'additional', 'episodes', 'unlocked', 'higher', 'difficulty', 'found', 'rest', 'game', 'also', 'love', 'simulation', 'elements', 'related', 'game', 'two', 'main', 'heroines', 'although', 'take', 'minor', 'role']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jg0jyPXgeFdl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 Store and Load Data Locally\n",
        "\n",
        "The duration of data pre-processing procedure is extremly long (> 4 h).  After completion I saved (takes a couple of minutes) the clean data to a .csv file, to use it locally from Drive.\n",
        "\n",
        "You can download the .csv file from this link:\n",
        "[preproc_clean_data.csv](https://drive.google.com/open?id=10c2fnKtjrxtW1VZ4W1yqUqxcFaupAefE)"
      ]
    },
    {
      "metadata": {
        "id": "DijjxUPfUWKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3.1 Connect to personal Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "SK0tl14Rirge",
        "colab_type": "code",
        "outputId": "5a68cf92-9dd6-49df-f182-32d62868a3de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# Connect to personal Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IGwyqWvNbq6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3.2 Path to pre-processed clean Data"
      ]
    },
    {
      "metadata": {
        "id": "moJt_kJ4bqSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preproc_clean_data = \"/content/drive/My Drive/NLP Assignment 2/Part1/preproc_clean_data.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkxvT5okrlLh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3.3 Store Clean Data to Drive"
      ]
    },
    {
      "metadata": {
        "id": "2KC_4pSmWisN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save data to local .csv file\n",
        "import csv\n",
        "\n",
        "with open(preproc_clean_data, \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(doc_para_noEmpties)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "02OiE2ZNb_Ck",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.3.4 Load Clean Data from Drive"
      ]
    },
    {
      "metadata": {
        "id": "6bCX1xgwn6gX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recover data from local .csv file\n",
        "import csv\n",
        "\n",
        "doc_para_noEmpties = []\n",
        "with open(preproc_clean_data, \"r\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    for row in reader:\n",
        "        doc_para_noEmpties.append(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nkrt4uZtBLeL",
        "colab_type": "code",
        "outputId": "6f06c675-a80f-45c1-e5c2-4b62c0cc8fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# The first 6 paragraphs from restored data\n",
        "print(doc_para_noEmpties[:6])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['valkyria', 'chronicles', 'iii'], ['senj', 'valkyria', 'unk', 'chronicles', 'japanese', 'lit', 'valkyria', 'battlefield', 'commonly', 'referred', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'sega', 'media', 'vision', 'playstation', 'portable', 'released', 'january', 'japan', 'third', 'game', 'valkyria', 'series', 'employing', 'fusion', 'tactical', 'real', 'time', 'gameplay', 'predecessors', 'story', 'runs', 'parallel', 'first', 'game', 'follows', 'nameless', 'penal', 'military', 'unit', 'serving', 'nation', 'gallia', 'second', 'europan', 'war', 'perform', 'secret', 'black', 'operations', 'pitted', 'imperial', 'unit', 'unk', 'raven'], ['game', 'began', 'development', 'carrying', 'large', 'portion', 'work', 'done', 'valkyria', 'chronicles', 'retained', 'standard', 'features', 'series', 'also', 'underwent', 'multiple', 'adjustments', 'making', 'game', 'forgiving', 'series', 'newcomers', 'character', 'designer', 'unk', 'honjou', 'composer', 'hitoshi', 'sakimoto', 'returned', 'previous', 'entries', 'along', 'valkyria', 'chronicles', 'director', 'takeshi', 'ozawa', 'large', 'team', 'writers', 'handled', 'script', 'game', 'opening', 'theme', 'sung', 'may'], ['met', 'positive', 'sales', 'japan', 'praised', 'japanese', 'western', 'critics', 'release', 'received', 'downloadable', 'content', 'along', 'expanded', 'edition', 'november', 'year', 'also', 'adapted', 'manga', 'original', 'video', 'animation', 'series', 'due', 'low', 'sales', 'valkyria', 'chronicles', 'valkyria', 'chronicles', 'iii', 'localized', 'fan', 'translation', 'compatible', 'game', 'expanded', 'edition', 'released', 'media', 'vision', 'would', 'return', 'franchise', 'development', 'valkyria', 'azure', 'revolution', 'playstation'], ['gameplay'], ['previous', 'unk', 'chronicles', 'games', 'valkyria', 'chronicles', 'iii', 'tactical', 'role', 'playing', 'game', 'players', 'take', 'control', 'military', 'unit', 'take', 'part', 'missions', 'enemy', 'forces', 'stories', 'told', 'comic', 'book', 'like', 'panels', 'animated', 'character', 'portraits', 'characters', 'speaking', 'partially', 'voiced', 'speech', 'bubbles', 'partially', 'unvoiced', 'text', 'player', 'progresses', 'series', 'linear', 'missions', 'gradually', 'unlocked', 'maps', 'freely', 'scanned', 'replayed', 'unlocked', 'route', 'story', 'location', 'map', 'varies', 'depending', 'individual', 'player', 'approach', 'one', 'option', 'selected', 'sealed', 'player', 'outside', 'missions', 'player', 'characters', 'rest', 'camp', 'units', 'customized', 'character', 'growth', 'occurs', 'alongside', 'main', 'story', 'missions', 'character', 'specific', 'sub', 'missions', 'relating', 'different', 'squad', 'members', 'game', 'completion', 'additional', 'episodes', 'unlocked', 'higher', 'difficulty', 'found', 'rest', 'game', 'also', 'love', 'simulation', 'elements', 'related', 'game', 'two', 'main', 'heroines', 'although', 'take', 'minor', 'role']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "W1cBbTCfjwUH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.4 Training the Word2Vec model on Dataset"
      ]
    },
    {
      "metadata": {
        "id": "Wqbhrp28kt0D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 1.1.4.1 Setting model's parameters:\n",
        "- **window** (int, optional) – Maximum distance between the current and predicted word within a sentence\n",
        "- **size** (int, optional) – Dimensionality of the word vectors\n",
        "- **sg** ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW\n",
        "- **min_count** (int, optional) – Ignores all words with total frequency lower than this\n",
        "- **workers** (int, optional) – Use these many worker threads to train the model"
      ]
    },
    {
      "metadata": {
        "id": "olFLWr9qh0fl",
        "colab_type": "code",
        "outputId": "f7ab1005-51a6-4a33-fd44-2fdc13df12e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(window=4,size=100,sg=1,min_count=10,workers = -1)\n",
        "model.build_vocab(doc_para_noEmpties)  # Building the model vocabulary\n",
        "model.train(doc_para_noEmpties,total_examples=model.corpus_count,epochs=model.iter)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "lBszLshgh0Y6",
        "colab_type": "code",
        "outputId": "10356b37-597a-4344-ec33-52d747548eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# Model's Vocabulary\n",
        "vocab = model.wv.vocab\n",
        "print('Vocabulary size:')\n",
        "print(len(vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:\n",
            "111839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ciRyk-iqh0Oh",
        "colab_type": "code",
        "outputId": "a6a867b0-0ec4-445c-9977-65a94c70b9ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# 10 most frequent words\n",
        "mfw = model.wv.index2word[:10]\n",
        "print(mfw)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unk', 'first', 'one', 'also', 'two', 'new', 'time', 'would', 'game', 'later']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7SzqYCvnpb5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1.5 Find the 5 most similar word pairs from the 10 most frequent words\n",
        "I wil use the *most_similar()* function to compare the 10 most frequent words against each other and compute their similarities"
      ]
    },
    {
      "metadata": {
        "id": "bBToNIs3h0Hb",
        "colab_type": "code",
        "outputId": "3f7b2083-b9bd-4e8c-dd44-70e4314a763a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "import operator\n",
        "\n",
        "# Use dictionary structure to store word pairs and their similarity\n",
        "similar_pairs = {}\n",
        "\n",
        "# Compare each of the 10 most frequent words against the others and find similarity\n",
        "for p1 in mfw:\n",
        "  # restrict_vocab parameter restricts comparison to the 10 most frequent words\n",
        "    pairs = model.wv.most_similar(p1, restrict_vocab=10)\n",
        "    for p2,sim in pairs:\n",
        "        similar_pairs[(p1, p2)] = sim\n",
        "\n",
        "# Sort dictionary's data by values\n",
        "sorted_similar_pairs = sorted(similar_pairs.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "# Since each pair appears twice, I keep every other element of the ordered list\n",
        "sorted_similar_pairs = sorted_similar_pairs[0:10:2]\n",
        "\n",
        "# Print pairs\n",
        "for pair in sorted_similar_pairs:\n",
        "  print(pair)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('unk', 'later'), 0.18580488860607147)\n",
            "(('one', 'would'), 0.1591222584247589)\n",
            "(('also', 'new'), 0.13493970036506653)\n",
            "(('two', 'new'), 0.1329299658536911)\n",
            "(('unk', 'game'), 0.13059712946414948)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "CuyBXtMrvjfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Implement a function that retrieves two word vectors and computes their cosine distance"
      ]
    },
    {
      "metadata": {
        "id": "st2AsYNthz_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A function taking as input a trained word2vec model and two words (strings) and\n",
        "# then manually computes and returns their cosine distance\n",
        "def cosVecDist(w2vModel, str1, str2):\n",
        "  '''\n",
        "  Compute cosine distance of 2 words.\n",
        "  Inputs:\n",
        "    - A trained word2vec model\n",
        "    - First word (string)\n",
        "    - Second word (string)\n",
        "  Returns:\n",
        "    Cosine distance\n",
        "  '''\n",
        "  # Use model to find vector representation of strings\n",
        "  vstr1 = w2vModel.wv[str1]\n",
        "  vstr2 = w2vModel.wv[str2]\n",
        "  # Cosine distance of two vectors equals to the dot product of vectors\n",
        "  # divided by the product of vectrors' legths\n",
        "  # The dot product of two 1-D vectors can be computed using numpy.dot() function\n",
        "  dotProd = np.dot(vstr1,vstr2)\n",
        "  # The length of a 1-D vector can be computed using numpy.linalg.norm() function\n",
        "  # The default value of ord parameter (ord=None), returns the 2-norm of vectors\n",
        "  length1 = np.linalg.norm(vstr1, ord=None)\n",
        "  length2 = np.linalg.norm(vstr2, ord=None)\n",
        "  # Cosine distance of the vectors\n",
        "  cosDist = dotProd/(length1*length2)\n",
        "  return(cosDist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6vdLS5cUdzHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cosVecDist?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_r14X-pAxXOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Comparing the two approaches\n",
        "\n",
        "Compute Cosine Distance of 5 random word pairs from vocabulary using 2 methods:\n",
        "\n",
        "- Custom *cosVecDist()* function defined above\n",
        "- The in-built *wv.similarity()* function of trained model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "55tWMmEyqSEO",
        "outputId": "9bd0e39d-b708-42d8-f13e-67fc4ecc12bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# I use the trained model from previous step\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Create 5 random pairs of words from model's vocabulary\n",
        "vocab = model.wv.index2word\n",
        "list = []\n",
        "for i in range(5):\n",
        "  list.append(random.choices(vocab, k=2))\n",
        "\n",
        "# DataFrame to store the results\n",
        "headers = ['Word 1', 'Word 2', 'Custom Function', 'Model\\'s Function']\n",
        "cosDist = pd.DataFrame(columns = headers)\n",
        "\n",
        "for pair in list:\n",
        "  results = []\n",
        "  results.append(pair[0])\n",
        "  results.append(pair[1])\n",
        "  results.append(cosVecDist(model, pair[0], pair[1]))\n",
        "  results.append(model.wv.similarity(pair[0], pair[1]))\n",
        "  \n",
        "  new_row = pd.Series(results, index = headers)\n",
        "  cosDist = cosDist.append(new_row, ignore_index=1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "H_70QU3ayh6t",
        "colab_type": "code",
        "outputId": "b2b29afd-6562-433c-8c69-34bc77e104da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "# Print the results\n",
        "print('Computing Cosine Distance with 2 different methods:')\n",
        "cosDist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Computing Cosine Distance with 2 different methods:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Custom Function</th>\n",
              "      <th>Model's Function</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>brolga</td>\n",
              "      <td>debussy</td>\n",
              "      <td>-0.069307</td>\n",
              "      <td>-0.069307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>adas</td>\n",
              "      <td>smirk</td>\n",
              "      <td>-0.032157</td>\n",
              "      <td>-0.032157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>lobbied</td>\n",
              "      <td>confiscating</td>\n",
              "      <td>-0.078562</td>\n",
              "      <td>-0.078562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>decebalus</td>\n",
              "      <td>mounts</td>\n",
              "      <td>0.181189</td>\n",
              "      <td>0.181189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>smithers</td>\n",
              "      <td>ponytail</td>\n",
              "      <td>-0.052828</td>\n",
              "      <td>-0.052828</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Word 1        Word 2  Custom Function  Model's Function\n",
              "0     brolga       debussy        -0.069307         -0.069307\n",
              "1       adas         smirk        -0.032157         -0.032157\n",
              "2    lobbied  confiscating        -0.078562         -0.078562\n",
              "3  decebalus        mounts         0.181189          0.181189\n",
              "4   smithers      ponytail        -0.052828         -0.052828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "eHOp5OQE1P7r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Conclusions\n",
        "\n",
        "Both methods delivers exactly the same result, so it's obvious that in-built *wv.similarity()* function uses cosine distance to express the similarity of two words"
      ]
    },
    {
      "metadata": {
        "id": "z_ANqLSxMarZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.3.     Visit the NLPL word embedding repository and download the models with the following identifiers: \n",
        "\n",
        "- 40. It was trained on the English CoNLL17 corpus, using Continuous Skip-gram algorithm with vector size 100, and window size 10.\n",
        "- 75. It was trained on the English Oil and Gas corpus, using Continuous Bag-of-Words algorithm with vector size 400, and window size 5.\n",
        "- 82. It was trained on the English Common Crawl Corpus, using GloVe algorithm with vector size 300, and window size 10."
      ]
    },
    {
      "metadata": {
        "id": "IkgUSbdINvOs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3NRo4v5qf5z2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 CoNLL17 corpus"
      ]
    },
    {
      "metadata": {
        "id": "ChjgZImvM-cY",
        "colab_type": "code",
        "outputId": "21aaf9eb-5022-4915-ff3f-c8284b28d154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading CoNLL17 corpus ~1.5GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/40.zip\", filename=\"40.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('40.zip', <http.client.HTTPMessage at 0x7fb288d09080>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "KJYLIL4vgB34",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 Oil and Gas corpus"
      ]
    },
    {
      "metadata": {
        "id": "xJlfAgU1M-DX",
        "colab_type": "code",
        "outputId": "203b5e57-4cc1-49bf-ba1a-f7ffd7798d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading Oil and Gas corpus ~0.4GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/75.zip\", filename=\"75.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('75.zip', <http.client.HTTPMessage at 0x7fb288d09160>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "cW1qJ2IEgEFz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Common Crawl corpus"
      ]
    },
    {
      "metadata": {
        "id": "WpNsKTukxUYt",
        "colab_type": "code",
        "outputId": "087d5654-242e-460c-92d4-a2cfc825f1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading Oil and Gas corpus ~2.3GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/82.zip\", filename=\"82.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('82.zip', <http.client.HTTPMessage at 0x7fb288d09668>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "Xlton0kxNAC4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.4. Create the lists of top 20 most frequent words in WikiText, CoNLL17, Oil and Gas, and Common Crawl corpora"
      ]
    },
    {
      "metadata": {
        "id": "kMRXghbfPDVs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_Uw9qYMhz4U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dictionary structure to store 20 most frequent words of each corpus\n",
        "twentyFreqWords = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eq5EROnfbr4w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.1 Wiki Text"
      ]
    },
    {
      "metadata": {
        "id": "FZX6o7S_hMc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just use the wv.index2word function on the model trained at stage 1.4"
      ]
    },
    {
      "metadata": {
        "id": "wYvIWs6dcxP4",
        "colab_type": "code",
        "outputId": "6014626f-b407-438d-c5c7-a37b94447ad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# 20 most frequent words\n",
        "wiki_mfw = model.wv.index2word[:20]\n",
        "twentyFreqWords['WikiText'] = wiki_mfw\n",
        "print(twentyFreqWords['WikiText'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unk', 'first', 'one', 'also', 'two', 'new', 'time', 'would', 'game', 'later', 'three', 'film', 'may', 'year', 'made', 'second', 'season', 'years', 'world', 'war']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u_7MXE1EiKUL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the downloaded pre-trained embeddings I follow a different approach. Since each model is saved as a text file in the word2vec format, its lines are as a rule sorted by frequency. Each word and its vector corresponds to a paragraph of the .txt file. Furthermore, the first paragraph of the .txt file holds informations about the length of the vocabulary and the dimensionality of the vectors. So I just have to read the first few ten thousands of bytes, accordind to their vector's dimensionality and make sure to include at least the first 22 paragraphs.\n",
        "\n",
        "For each model I repeat the following procedure:\n",
        "- Extract and read *part* of the model.txt file stored in the downloaded .zip file\n",
        "- Convert the text bytes to string and split to paragraphs \n",
        "- Tokenize each paragraph to words\n",
        "- Ignore the first paragraph and keep the first word of the following 20 paragraphs to list\n"
      ]
    },
    {
      "metadata": {
        "id": "n5wDtjHeUMZI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.2 CoNLL17 corpus"
      ]
    },
    {
      "metadata": {
        "id": "Xt5M1d9Yhzp2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use the model.txt file inside the .zip file\n",
        "# Read only the first 20000 bytes\n",
        "with zipfile.ZipFile('40.zip', 'r') as z:\n",
        "  doc40 = z.open('model.txt', 'r').read(20000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4Xgtb-7TVoW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert bytes to string and then split to paragraphs\n",
        "doc_str = doc40.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uigW1pb8hzh9",
        "colab_type": "code",
        "outputId": "7f4fa604-0d17-4046-ee36-9dc08af5cf60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Check if sample includes at least the first 22 paragraphs\n",
        "len(doc_para)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "EBLkJMD5FZK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "para_list = []\n",
        "for i in range(20):\n",
        "  # Ignore the first paragraph\n",
        "  para = doc_para[i+1].split(' ')[0]\n",
        "  para_list.append(para)\n",
        "\n",
        "# Save list to dictionary\n",
        "twentyFreqWords['40:CoNLL15'] = para_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KelxareuaPSo",
        "colab_type": "code",
        "outputId": "6d4b1f82-a57c-4231-8d3f-bf2dd324d293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(twentyFreqWords['40:CoNLL15'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['</s>', ',', 'the', '.', 'of', 'and', 'to', 'a', 'in', '-', ')', '(', ':', 'for', 'is', '\"', 'on', 'i', 'that', 'with']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gU09fUxWUaVY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.3 Oil and Gas corpus"
      ]
    },
    {
      "metadata": {
        "id": "21W_tU1HU6Vd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Same procedure as above"
      ]
    },
    {
      "metadata": {
        "id": "vs52GZpHEWTH",
        "colab_type": "code",
        "outputId": "47dce092-f73d-4785-df23-fecf39aa666f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('75.zip', 'r') as z:\n",
        "  doc75 = z.open('model.txt', 'r').read(80000)\n",
        "\n",
        "doc_str = doc75.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')\n",
        "\n",
        "len(doc_para)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "BR3ftSkeRLvo",
        "colab_type": "code",
        "outputId": "42b62bf7-f303-4fa5-b927-cda1159d8158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "para_list = []\n",
        "for i in range(20):\n",
        "  para = doc_para[i+1].split(' ')[0]\n",
        "  para_list.append(para)\n",
        "\n",
        "twentyFreqWords['75:OilAndGas'] = para_list\n",
        "\n",
        "print(twentyFreqWords['75:OilAndGas'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lrb', 'rrb', 'sediment', 'fault', 'datum', 'basin', 'sample', 'area', 'study', 'model', 'result', 'zone', 'water', 'rock', 'time', 'formation', 'high', 'surface', 'increase', 'change']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LE6H-iUFVnyE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.4.4 Common Crawl corpus"
      ]
    },
    {
      "metadata": {
        "id": "CVqMzsupVyca",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Same procedure as above"
      ]
    },
    {
      "metadata": {
        "id": "9CDkr2eRhzY2",
        "colab_type": "code",
        "outputId": "652c54c1-f668-4131-c8c6-1826226a0307",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('82.zip', 'r') as z:\n",
        "  doc82 = z.open('model.txt', 'r').read(60000)\n",
        "  \n",
        "doc_str = doc82.decode(\"utf-8\")\n",
        "doc_para  = doc_str.split('\\n')\n",
        "\n",
        "len(doc_para)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "5GyqryFnWuIC",
        "colab_type": "code",
        "outputId": "0640b719-f5b0-4497-c6f2-3b695c281c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "para_list = []\n",
        "for i in range(20):\n",
        "  para = doc_para[i+1].split(' ')[0]\n",
        "  para_list.append(para)\n",
        "\n",
        "twentyFreqWords['82:CommonCrawl'] = para_list\n",
        "\n",
        "print(twentyFreqWords['82:CommonCrawl'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['the', ',', '.', 'and', 'to', 'of', 'a', 'in', 'is', 'that', 'i', 'for', 'it', 'you', 'on', \"'s\", 'with', '-rrb-', '-lrb-', 'as']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ayNaEb5htpc2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.5. Comparison of the 4 word lists"
      ]
    },
    {
      "metadata": {
        "id": "p4F01j4DgooL",
        "colab_type": "code",
        "outputId": "f83a94e4-a44c-44ec-e932-1ab856729bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "print(pd.DataFrame.from_dict(twentyFreqWords))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   40:CoNLL15 75:OilAndGas 82:CommonCrawl WikiText\n",
            "0        </s>          lrb            the      unk\n",
            "1           ,          rrb              ,    first\n",
            "2         the     sediment              .      one\n",
            "3           .        fault            and     also\n",
            "4          of        datum             to      two\n",
            "5         and        basin             of      new\n",
            "6          to       sample              a     time\n",
            "7           a         area             in    would\n",
            "8          in        study             is     game\n",
            "9           -        model           that    later\n",
            "10          )       result              i    three\n",
            "11          (         zone            for     film\n",
            "12          :        water             it      may\n",
            "13        for         rock            you     year\n",
            "14         is         time             on     made\n",
            "15          \"    formation             's   second\n",
            "16         on         high           with   season\n",
            "17          i      surface          -rrb-    years\n",
            "18       that     increase          -lrb-    world\n",
            "19       with       change             as      war\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fq8H-IL7uGdd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###1.5.1 Conclusions"
      ]
    },
    {
      "metadata": {
        "id": "DToa6OhsuWSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Comparing the 4 lists I conclude the following:\n",
        "\n",
        "-  CoNLL15 DataSet is composed exclusively by stop words.\n",
        "-  OilAndGas DataSet contains clean pre-processed data. All stop words are removed.\n",
        "-  CommonCrawl DataSet is composed exclusively by stop words.\n",
        "-  WikiText DataSet contains clean pre-processed data. All stop words are removed."
      ]
    },
    {
      "metadata": {
        "id": "CY1neSNcud0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.6. Project top 1000 words from the WikiText corpus in 2d space using t-SNE plot"
      ]
    },
    {
      "metadata": {
        "id": "fIs9lDUowAJ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I use the model trained at stage 1.4"
      ]
    },
    {
      "metadata": {
        "id": "bBZL60xDgoXj",
        "colab_type": "code",
        "outputId": "68e5257f-72d0-4c09-e718-22ca4b3c5eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Get the 1000 most frequent words from the model\n",
        "mfw1000 = model.wv.index2word[:1000]\n",
        "print(mfw1000[:20])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['unk', 'first', 'one', 'also', 'two', 'new', 'time', 'would', 'game', 'later', 'three', 'film', 'may', 'year', 'made', 'second', 'season', 'years', 'world', 'war']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N2bZ-N1CgoHN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "outputId": "1c5d6715-212c-401d-88d7-bf992a82de37"
      },
      "cell_type": "code",
      "source": [
        "# Get the vector representation of each word in the list\n",
        "mfw1000_vecs = model[mfw1000]\n",
        "\n",
        "# Vextor representation of word 'first'\n",
        "print(mfw1000_vecs[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-3.8228966e-03 -3.0721286e-03  7.5730321e-04  3.4493171e-03\n",
            "  1.5073986e-03 -3.1901891e-03  2.8959340e-03  1.8256524e-03\n",
            " -2.2177890e-04  2.5203689e-03 -1.3921830e-03  4.2542773e-03\n",
            " -4.7026989e-03  5.5330253e-05  1.4703446e-03  1.9372784e-03\n",
            "  4.8127472e-03  2.4324760e-03  4.1342825e-03  3.6828709e-03\n",
            "  1.7254860e-03  4.9369279e-03 -4.9714823e-03  1.6858283e-03\n",
            "  2.7052706e-04 -2.2433395e-03  1.3138643e-03  3.2607110e-03\n",
            " -4.4448446e-03 -3.4150633e-03  6.0526066e-04  1.6490713e-03\n",
            " -3.5419562e-03 -3.7937241e-03 -4.5053433e-03  2.8506089e-03\n",
            "  6.9959002e-04 -8.0726779e-05  1.8209197e-03 -1.7690843e-04\n",
            "  3.0920901e-03 -3.2257789e-04  2.6363258e-03 -3.7198169e-03\n",
            " -4.9859793e-03 -2.2676482e-04 -1.1726737e-03 -4.2450014e-03\n",
            "  1.5886213e-03 -4.7967900e-04 -9.5021311e-04 -3.0139505e-03\n",
            "  4.5128129e-03  1.2703591e-03 -7.3688995e-04 -3.3563776e-03\n",
            " -2.8209428e-03 -2.3495979e-03  4.9033812e-03 -3.6185647e-03\n",
            "  2.5642963e-04  4.4768029e-03  1.1063453e-03  1.6596304e-03\n",
            "  3.6923590e-03 -1.3130769e-03 -2.2201175e-03  1.6285611e-03\n",
            " -2.9791596e-03 -1.9231475e-03  2.7923516e-03 -4.8577618e-03\n",
            "  3.0141938e-03 -1.6514566e-03  4.1869381e-03 -3.1932638e-04\n",
            " -4.8081665e-03  2.5988331e-03  1.6218473e-03  4.4053756e-03\n",
            "  2.8201889e-03 -9.0334390e-04 -4.7818902e-03  1.1365917e-03\n",
            "  1.9014472e-03 -3.7089260e-03  1.9178630e-03  4.5745582e-03\n",
            " -5.2367023e-04 -1.8489311e-03 -4.3130293e-03 -3.2733970e-03\n",
            " -3.1904317e-03 -4.0840702e-03 -2.9974181e-04  4.6514752e-03\n",
            " -4.4474779e-03 -1.7354529e-03  4.3712487e-03  6.6833349e-04]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "cb3dfb4e-096c-4f25-ad4a-c2a7d2045d7c",
        "id": "djkIu9rWkGAb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        }
      },
      "cell_type": "code",
      "source": [
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from sklearn.manifold import TSNE\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "# Function that allows to create graphs for specific parts of the whole word list\n",
        "def ctsne(model, start=0, finish=1000):\n",
        "  words = model.wv.index2word[start:finish]\n",
        "  vectors = model[words]\n",
        "  tsne = TSNE(n_components=2, random_state=42)\n",
        "  w_vecs_tsne = tsne.fit_transform(vectors)\n",
        "\n",
        "  p = figure(tools=\"pan,wheel_zoom,box_zoom,reset,save\",\n",
        "             toolbar_location=\"above\",\n",
        "             title='Word2Vec T-SNE for the '+str(len(vectors))+' most common words',\n",
        "             plot_width=800)\n",
        "\n",
        "  source = ColumnDataSource(data=dict(x1=w_vecs_tsne[:,0],\n",
        "                                      x2=w_vecs_tsne[:,1],\n",
        "                                      names=words))\n",
        "\n",
        "  p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
        "\n",
        "  labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
        "                    text_font_size=\"7pt\", text_color=\"#555555\",\n",
        "                    source=source, text_align='center')\n",
        "  p.add_layout(labels)\n",
        "\n",
        "  show(p)\n",
        "  \n",
        "\n",
        "ctsne(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div class=\"bk-root\">\n",
              "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
              "        <span id=\"1001\">Loading BokehJS ...</span>\n",
              "    </div>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(\"1001\");\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
              "    }\n",
              "    finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.info(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(js_urls, callback) {\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = js_urls.length;\n",
              "    for (var i = 0; i < js_urls.length; i++) {\n",
              "      var url = js_urls[i];\n",
              "      var s = document.createElement('script');\n",
              "      s.src = url;\n",
              "      s.async = false;\n",
              "      s.onreadystatechange = s.onload = function() {\n",
              "        root._bokeh_is_loading--;\n",
              "        if (root._bokeh_is_loading === 0) {\n",
              "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
              "          run_callbacks()\n",
              "        }\n",
              "      };\n",
              "      s.onerror = function() {\n",
              "        console.warn(\"failed to load library \" + url);\n",
              "      };\n",
              "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "    }\n",
              "  };var element = document.getElementById(\"1001\");\n",
              "  if (element == null) {\n",
              "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
              "    return false;\n",
              "  }\n",
              "\n",
              "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    \n",
              "    function(Bokeh) {\n",
              "      \n",
              "    },\n",
              "    function(Bokeh) {\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
              "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
              "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
              "      for (var i = 0; i < inline_js.length; i++) {\n",
              "        inline_js[i].call(root, root.Bokeh);\n",
              "      }if (force === true) {\n",
              "        display_loaded();\n",
              "      }} else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(js_urls, function() {\n",
              "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div class=\"bk-root\" id=\"70ca52cc-2b36-4eff-adcb-32d3486755a8\" data-root-id=\"1003\"></div>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "    \n",
              "  var docs_json = {\"19e3da3c-b0c0-401d-ad63-1a49d6c5939b\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1013\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1018\",\"type\":\"LinearAxis\"}],\"plot_width\":800,\"renderers\":[{\"id\":\"1013\",\"type\":\"LinearAxis\"},{\"id\":\"1017\",\"type\":\"Grid\"},{\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"id\":\"1022\",\"type\":\"Grid\"},{\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"id\":\"1042\",\"type\":\"LabelSet\"}],\"title\":{\"id\":\"1002\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1028\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"1005\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1009\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1007\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1011\",\"type\":\"LinearScale\"}},\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"formatter\":{\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1014\",\"type\":\"BasicTicker\"}},\"id\":\"1013\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1030\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"1038\",\"type\":\"Scatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"size\":{\"units\":\"screen\",\"value\":8},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"}},\"id\":\"1039\",\"type\":\"Scatter\"},{\"attributes\":{\"callback\":null,\"data\":{\"names\":[\"unk\",\"first\",\"one\",\"also\",\"two\",\"new\",\"time\",\"would\",\"game\",\"later\",\"three\",\"film\",\"may\",\"year\",\"made\",\"second\",\"season\",\"years\",\"world\",\"war\",\"000\",\"however\",\"used\",\"song\",\"series\",\"album\",\"many\",\"team\",\"city\",\"part\",\"north\",\"became\",\"number\",\"united\",\"several\",\"including\",\"four\",\"well\",\"early\",\"state\",\"south\",\"music\",\"day\",\"episode\",\"said\",\"following\",\"states\",\"known\",\"american\",\"although\",\"could\",\"work\",\"began\",\"released\",\"like\",\"called\",\"high\",\"people\",\"end\",\"million\",\"british\",\"since\",\"around\",\"long\",\"national\",\"life\",\"best\",\"found\",\"september\",\"west\",\"back\",\"along\",\"another\",\"five\",\"show\",\"took\",\"use\",\"area\",\"final\",\"group\",\"august\",\"century\",\"received\",\"october\",\"john\",\"school\",\"due\",\"line\",\"government\",\"june\",\"east\",\"single\",\"system\",\"march\",\"july\",\"home\",\"november\",\"games\",\"third\",\"general\",\"april\",\"much\",\"large\",\"set\",\"major\",\"left\",\"based\",\"family\",\"york\",\"house\",\"place\",\"company\",\"army\",\"history\",\"december\",\"road\",\"played\",\"included\",\"name\",\"king\",\"january\",\"six\",\"band\",\"character\",\"wrote\",\"according\",\"led\",\"main\",\"within\",\"last\",\"near\",\"times\",\"described\",\"men\",\"river\",\"death\",\"video\",\"air\",\"named\",\"next\",\"battle\",\"record\",\"release\",\"late\",\"still\",\"league\",\"original\",\"make\",\"way\",\"top\",\"ship\",\"route\",\"production\",\"species\",\"even\",\"great\",\"force\",\"man\",\"club\",\"small\",\"old\",\"storm\",\"february\",\"public\",\"though\",\"days\",\"built\",\"held\",\"university\",\"play\",\"among\",\"division\",\"service\",\"role\",\"book\",\"french\",\"german\",\"side\",\"members\",\"story\",\"white\",\"power\",\"match\",\"point\",\"water\",\"player\",\"often\",\"order\",\"career\",\"continued\",\"former\",\"park\",\"local\",\"black\",\"town\",\"without\",\"came\",\"despite\",\"england\",\"considered\",\"take\",\"party\",\"half\",\"island\",\"county\",\"support\",\"form\",\"songs\",\"development\",\"version\",\"moved\",\"military\",\"written\",\"period\",\"different\",\"become\",\"president\",\"english\",\"forces\",\"lost\",\"building\",\"given\",\"court\",\"london\",\"tropical\",\"country\",\"international\",\"little\",\"church\",\"performance\",\"gave\",\"never\",\"similar\",\"tour\",\"street\",\"lead\",\"attack\",\"track\",\"returned\",\"using\",\"published\",\"royal\",\"produced\",\"short\",\"went\",\"position\",\"class\",\"recorded\",\"children\",\"television\",\"stated\",\"highway\",\"aircraft\",\"land\",\"total\",\"good\",\"western\",\"ships\",\"run\",\"include\",\"live\",\"making\",\"per\",\"control\",\"father\",\"star\",\"week\",\"women\",\"fire\",\"throughout\",\"upon\",\"right\",\"love\",\"australia\",\"head\",\"less\",\"characters\",\"design\",\"central\",\"age\",\"players\",\"young\",\"instead\",\"light\",\"writing\",\"title\",\"away\",\"rock\",\"seven\",\"across\",\"james\",\"remained\",\"months\",\"red\",\"station\",\"sea\",\"died\",\"return\",\"developed\",\"ten\",\"result\",\"previous\",\"political\",\"night\",\"law\",\"william\",\"southern\",\"hurricane\",\"eight\",\"center\",\"low\",\"reported\",\"office\",\"felt\",\"areas\",\"noted\",\"various\",\"eventually\",\"america\",\"race\",\"style\",\"announced\",\"son\",\"field\",\"created\",\"reached\",\"act\",\"miles\",\"college\",\"win\",\"construction\",\"award\",\"sent\",\"japanese\",\"seen\",\"followed\",\"together\",\"performed\",\"served\",\"member\",\"full\",\"george\",\"northern\",\"range\",\"level\",\"common\",\"killed\",\"taken\",\"able\",\"campaign\",\"points\",\"works\",\"action\",\"established\",\"chart\",\"born\",\"originally\",\"success\",\"body\",\"others\",\"front\",\"appeared\",\"thought\",\"australian\",\"modern\",\"features\",\"located\",\"project\",\"post\",\"special\",\"get\",\"playing\",\"football\",\"started\",\"stage\",\"bridge\",\"addition\",\"caused\",\"critics\",\"feet\",\"rather\",\"present\",\"fourth\",\"decided\",\"behind\",\"site\",\"formed\",\"added\",\"sold\",\"heavy\",\"championship\",\"every\",\"david\",\"director\",\"see\",\"popular\",\"strong\",\"saw\",\"damage\",\"completed\",\"population\",\"almost\",\"command\",\"important\",\"open\",\"soon\",\"close\",\"fleet\",\"eastern\",\"awards\",\"initially\",\"art\",\"kingdom\",\"guns\",\"opened\",\"non\",\"case\",\"ground\",\"region\",\"leading\",\"free\",\"weeks\",\"michael\",\"coast\",\"worked\",\"france\",\"recording\",\"working\",\"mother\",\"towards\",\"hall\",\"union\",\"event\",\"scene\",\"ever\",\"sound\",\"records\",\"scored\",\"help\",\"henry\",\"training\",\"provided\",\"navy\",\"least\",\"wanted\",\"cup\",\"average\",\"brought\",\"featured\",\"either\",\"allowed\",\"put\",\"hit\",\"wife\",\"victory\",\"troops\",\"generally\",\"studio\",\"placed\",\"washington\",\"robert\",\"example\",\"runs\",\"events\",\"magazine\",\"far\",\"europe\",\"council\",\"mid\",\"joined\",\"designed\",\"hours\",\"summer\",\"squadron\",\"human\",\"list\",\"nine\",\"opening\",\"believed\",\"start\",\"replaced\",\"official\",\"japan\",\"media\",\"mph\",\"brown\",\"operation\",\"base\",\"european\",\"involved\",\"minutes\",\"earlier\",\"police\",\"rest\",\"finished\",\"society\",\"successful\",\"met\",\"possible\",\"saying\",\"crew\",\"radio\",\"shot\",\"month\",\"significant\",\"association\",\"big\",\"particularly\",\"review\",\"highest\",\"largest\",\"films\",\"test\",\"beginning\",\"process\",\"available\",\"research\",\"change\",\"forced\",\"captain\",\"taking\",\"outside\",\"cast\",\"better\",\"charles\",\"come\",\"increased\",\"turned\",\"ended\",\"must\",\"social\",\"stone\",\"section\",\"reviews\",\"real\",\"units\",\"community\",\"thomas\",\"lower\",\"passed\",\"praised\",\"business\",\"san\",\"going\",\"relationship\",\"middle\",\"additional\",\"future\",\"move\",\"appearance\",\"hill\",\"space\",\"thus\",\"goal\",\"canada\",\"enough\",\"chief\",\"ordered\",\"winds\",\"previously\",\"cross\",\"signed\",\"elements\",\"gold\",\"rights\",\"board\",\"california\",\"program\",\"staff\",\"positive\",\"paul\",\"round\",\"lake\",\"infantry\",\"parts\",\"female\",\"critical\",\"interest\",\"castle\",\"spent\",\"told\",\"give\",\"plan\",\"gun\",\"find\",\"overall\",\"lines\",\"musical\",\"type\",\"brother\",\"attempt\",\"committee\",\"directed\",\"failed\",\"money\",\"required\",\"groups\",\"news\",\"germany\",\"hand\",\"india\",\"district\",\"evidence\",\"sometimes\",\"grand\",\"prior\",\"cover\",\"entire\",\"term\",\"asked\",\"battalion\",\"election\",\"personal\",\"port\",\"reception\",\"smith\",\"press\",\"bill\",\"room\",\"whose\",\"study\",\"claimed\",\"complete\",\"already\",\"green\",\"introduced\",\"square\",\"background\",\"plot\",\"usually\",\"turn\",\"novel\",\"indian\",\"past\",\"students\",\"person\",\"governor\",\"idea\",\"education\",\"quickly\",\"limited\",\"loss\",\"score\",\"effects\",\"peter\",\"speed\",\"stars\",\"response\",\"movement\",\"issue\",\"flight\",\"mark\",\"food\",\"surface\",\"debut\",\"commander\",\"arrived\",\"richard\",\"especially\",\"minister\",\"blue\",\"services\",\"anti\",\"decision\",\"britain\",\"woman\",\"fifth\",\"village\",\"department\",\"teams\",\"shows\",\"bay\",\"structure\",\"industry\",\"operations\",\"larger\",\"writer\",\"length\",\"science\",\"queen\",\"episodes\",\"becoming\",\"earth\",\"remaining\",\"voice\",\"centre\",\"estimated\",\"material\",\"carried\",\"defeated\",\"living\",\"latter\",\"cut\",\"pressure\",\"civil\",\"car\",\"scenes\",\"commercial\",\"leaving\",\"virginia\",\"god\",\"changes\",\"running\",\"might\",\"suggested\",\"entered\",\"report\",\"nearly\",\"los\",\"percent\",\"appointed\",\"influence\",\"approximately\",\"date\",\"depression\",\"pop\",\"naval\",\"winning\",\"language\",\"shortly\",\"islands\",\"empire\",\"view\",\"finally\",\"today\",\"child\",\"self\",\"network\",\"dark\",\"tracks\",\"hot\",\"proposed\",\"leader\",\"daughter\",\"course\",\"current\",\"wide\",\"producer\",\"higher\",\"destroyed\",\"experience\",\"size\",\"spanish\",\"provide\",\"railway\",\"fact\",\"subsequently\",\"nature\",\"compared\",\"lack\",\"married\",\"brigade\",\"immediately\",\"intended\",\"soviet\",\"supported\",\"face\",\"regular\",\"agreed\",\"cost\",\"awarded\",\"natural\",\"whether\",\"contract\",\"helped\",\"museum\",\"fans\",\"manager\",\"guitar\",\"chicago\",\"planned\",\"professional\",\"related\",\"health\",\"officer\",\"chinese\",\"create\",\"upper\",\"longer\",\"soldiers\",\"fort\",\"jackson\",\"done\",\"trade\",\"edward\",\"ball\",\"buildings\",\"plans\",\"corps\",\"captured\",\"changed\",\"appear\",\"federal\",\"private\",\"friends\",\"something\",\"associated\",\"yet\",\"officers\",\"key\",\"regiment\",\"singles\",\"books\",\"effect\",\"feature\",\"remains\",\"billboard\",\"mostly\",\"johnson\",\"initial\",\"lord\",\"countries\",\"lyrics\",\"mixed\",\"really\",\"pacific\",\"probably\",\"extended\",\"minor\",\"numerous\",\"moving\",\"friend\",\"got\",\"forward\",\"information\",\"copies\",\"always\",\"location\",\"issued\",\"difficult\",\"energy\",\"includes\",\"leave\",\"atlantic\",\"christian\",\"status\",\"singer\",\"ran\",\"conference\",\"dance\",\"suffered\",\"via\",\"pass\",\"florida\",\"unit\",\"albums\",\"edition\",\"problems\",\"know\",\"dead\",\"likely\",\"meeting\",\"fight\",\"increase\",\"roman\",\"cause\",\"independent\",\"removed\",\"inspired\",\"texas\",\"pre\",\"china\",\"wind\",\"poor\",\"fell\",\"terms\",\"primary\",\"majority\",\"movie\",\"offered\",\"keep\",\"creek\",\"winter\",\"broadcast\",\"double\",\"minute\",\"goals\",\"impact\",\"standard\",\"mary\",\"valley\",\"entertainment\",\"attacks\",\"individual\",\"takes\",\"allow\",\"composed\",\"raised\",\"wall\",\"need\",\"canadian\",\"sales\",\"male\",\"damaged\",\"ranked\",\"africa\",\"hour\",\"tons\",\"artist\",\"appears\",\"daily\",\"revealed\",\"cyclone\",\"selected\",\"discovered\",\"fighting\",\"model\",\"active\",\"era\",\"russian\",\"conditions\",\"greater\",\"unable\",\"wing\",\"inside\",\"deep\",\"mission\",\"launched\",\"respectively\",\"particular\",\"capital\",\"issues\",\"needed\",\"numbers\",\"certain\",\"morning\",\"angeles\",\"smaller\",\"lived\",\"contains\",\"jack\",\"attempted\",\"shown\",\"makes\",\"deal\",\"martin\",\"whole\",\"occurred\",\"yards\",\"attacked\",\"culture\",\"sister\",\"iii\",\"hard\",\"ice\",\"fall\",\"twenty\",\"actor\",\"elected\",\"festival\",\"annual\",\"reach\",\"referred\",\"defeat\",\"horse\",\"foreign\",\"religious\",\"traditional\",\"combat\",\"market\",\"theme\",\"19th\",\"audience\",\"largely\",\"schools\",\"think\",\"resulted\",\"metal\",\"interview\",\"assigned\",\"girl\",\"covered\",\"mass\",\"listed\",\"variety\",\"collection\",\"sun\",\"heart\",\"prince\",\"peak\",\"competition\",\"commission\",\"earned\",\"complex\",\"commented\",\"expected\",\"starting\",\"congress\",\"closed\",\"attention\",\"performances\",\"direct\",\"giving\",\"hospital\",\"refused\",\"economic\",\"greatest\",\"italian\",\"word\",\"stories\"],\"x1\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"shape\":[1000]},\"x2\":{\"__ndarray__\":\"hQuivzeW08GwWCPBGBqPvw9l9UBkiS7B278uQdLrh0F7ID3BZJhPQWW8aEHDtnjAmcDlQHIyK0GE5eK+K51WQAM6WMFbN4TB1w9uv2OQssHI/LPB+XVSQJD0DMCEpqzBMbPNQVPJs7+lVo1Al6QtvxYiBkFCcKlBCOUzwfqp/sBtS1ZBXZSjQAyEWkGmoETBIHXLwQEBPsEfuee/zOmPwWnQl0FMQizAVCg9vgwD3cAch/JAwsSnwGrotj5eQxzB5/AkQQvIL8F01gFBlprBQc4it8DA3NpB3/eJwJiqkkBTs1dAMSstwP+ynUFS4gHBkQiNvjqEjkAJoMnADXktQCMXST+eBZJBXq4RQCQhI0Eg4mLB3UVhwReI7EAdoOdAmgAxQSGzpMAydae/6GIVwcn9osGs1WjAfC9ZQZk9t0EEv5DB17OIwe2YwUHhZHHBvysGQGwER0AnyGJBg5eiwd0XWMEY49FBhPdzwXWg/8C6hKVBhcpWvn0NB0EpAGRAzMuvQTUGC0DOSYhBTGMwwRooz8DxbZ+/WvuUQEHysEEyN1pBflzyPzgBP8CLylZB4IDbwBLEKUEKSEpAcPK9P6jcOEEFanLBT9BXwCnFk8FhlbJBtg5PQYuoXUFylapBS1a4wUEng0ELC8vAvYWsQanREUH9yJxBi5wxwM6Nz0ASyxJAQ/XyQNsjKMBaO6JARiCRQN6otsGDcge/D6ylQOVRKUBcIQPBbS5fvlQsY8GK+n9BNlUuwcVOKEHcXMTAagiXwJky8MASxsfAWFuUvrakzUCLoJs//6uQQXjZLcHdcdvBgSVyQXKe1UD9xqM+FgMvQDDIScFTx39AgmaJwad5gEEKncpAWgSUwC0ZAcHViXvAjl0kQaqMrEHtNQ5BbQsvQWZmN8EY3KY/HC6YQAdvSsEX74xBOkazwYp2LUH0OgzBdC2TQeLNbED6kpZBQKYcQRiqIEGZLCpB1cyywDHrHcHvqA1B5BEwutThcj+JpZvBPqfRwZyZJL4GUKDBVNUgQVk4SUGw/cnARTQ7QcIOKEEt8kDAbx9ewVTZCMEnGzXAoLipP9+DncEAFstAoO53PyHW2cA3q6hAtduEQB0tS8DodZHBVdJrQPhbp8HX9IrBlBIqwZcgs0EIjLzBfgOPwQJo+sCyYcDAhmGEQT2E/0CvH0pBge1rwLPgfkHkZzZA9nIqwfpBmMB+c7jBAJdrQFGtgUE0r5HBxD52QbbHPsFywUzBetVhwR7zgcHS84tBK0SVQayMl0C/eQy/agCBQdY630GiX51B6p9mwTj7H8Fru5rANUEXQWmqnsGVdxlB/n+zwM16d8HpHA/Bg6V0QfDuB8H+1jHBZ3JHwGalZsHzZRjBxNSUwBD9RcCiYyI+ASiEwOr6LEDJZT9B7liPwSNvh8Fz/VDAeVS9QPPg8UAIl9PBH9WdQDqgJj9tt/XAkU6jwa97d8FKR6PAs8AHwfZx3UCcFA9B1nINQU6pqcFLwVRAHiiBwTZTKEA+ms/AGLumQa8oukHovAjBnspOQHxiMUGVNYvB3OM/wJr/t8DFhcpAnY/MwWFNoMFss5RBn1DFvfN5lkGL3xxB4jV3wXQrrcA2M/DA7d28QftdUMFPt5hAoW1eQBuKEr8epEDBp15WQbWKfMAOpYPBIW0PP8r+lkHVKb1BkSXWwMAnkEH8uqfBrvADQXJmt8EInErA2QkQQcrQT8HLQaE/GsUTQVjP8z9Eh0xA1kZMQZN9H8H6n3HAZPdEQQP3+sD7QcxBi98bwZQCQMEgEy7AimcRQXYqvsF0RmTBrve/Qbt+1j6u8ai/kraowWiaZsG6RTNBzlaJwfbJE8FGGRTBw8GuQFDHesGmOc3A+1DnQOwKJkGtTs1A+oRHwVhuLcAOea3Bo2KGwB54EEF944BB03+xQWyGqEDReovAR9lIQSqQkkELdAdBKVw5QSsxwEAu4lXBsfMAwV4BwUF1DifB20MPQJmPgEDPZHBACMAtQVxAMEAuo6y/qp5hwXjdXcFNvxk/sFE2Qe1nJkB2cuTAkfBMQbr4db9UIqVAleohwA2dtUHRD7jBeNhHQdrhrcHd2YHBJQGbQRV000BHTZbAEhlIQTzHQcDmYrvAo6QlQdt/Y8HJod6/uIkFQfb4pMH0mM1AK1UAwWdDCUHbiwpBcruEQYPLucEjCpTBFGAhQfRA1EEQ1phA+ZhgwQr0HECw9Yy/Txh5P1Jq6cBLY1JAa0pTQTXbzD8NFtlANoxowHACFsGF1kfBOweCQKRhc0CBcgVB/VtsQaaNhsArgKhA2qktwbAOCkDJuQjBO/36voo5pkEs6pnAJPvCwKUWxcBhiodBwihkwTMJg8HD8YzBEs1pwYrIK8C2439A9+RdwSw23MGuvdNA+SBBQfw4tEHH8TVBzBmtQRyTf8G6YVlBmjOPQEQpHMGvyEVAkN5uQRjhIsC+MLLAPYSUQRBZD8EDKZPBwSLRQNp7VMD/2E4/4SLDQQnQYsGRi0VBZfKTQXoclcDs2p5BhMDfP/TlLcFPKaO+AKJgQbPEsUE/QRlBXoxDwUP4y8DomLpAt9grwUs/FcGi2nNB3qUxwQ1rAL+4YCq+aj8gwS2B2kFAsOjA64suQaUxnEG0wDFAknuyv3Q/E0F1phjBkvdPvxbCbUCOky5BKIgnwZL12sA5K1TBY6VDQcFxgcG+votBZJ8MwZk0uT/PWhvBp8tTwQUe/8DgQzm/H5RJwdZJUcHJS9q/hJzGwVRMZMH9JZtBjYi5QKVorsDJRCXBLzK0wVZuP0AHEJ/BFJXywCBUM8GQtjBBCYhAwdVYusG/sRHATV+dQeSid0FORD/ArVXAQN3af8GwKmZBL4NCwfIhDEErO/hAfWGtP2DImUBt/UlBiS+cwAqhc8HwkxpBSyFBQPkg+77ijWXA0JgqwU7B1sApaYnAwZ51QFZAoMEiegvArxtFQEaDyUCjIYbBEYAAwIWVO8DAJ5Y/U240QfFZZ8HrxZFAmy5VwdyizkGsRJ/AyOuQv+9o9z9j3J3BAU49wTIs3r9y7QBBo1HVwCrkmkGjMdpAcwRCwAps8UC+MGfB7pxmQWZbRL7Se0hBTxYowJ20sEBaEshAHaaOQDWO4EChIU1Bp/a6QZN4OEHeVD9Bf7QdQWZKNUE3XWpBHVBmwUDlH8HCPgFBMXalQBvm1z+NqUrAbxQ9QZ3TpsFvGxVAjw6Sv5ZRhUAbGWTB++adwdlNm8GP4TtB7DTawOAcnkH+FJ1B69ICQI+1k0EHawJBTz5Bwc7TPcGYlj9ATLNUwZbx4UBQuVhBobl9wCxe1kBDzb7A94vOQE8vbr9y1GVB3yM8wSzaDsGwWAxBZK0iwc97tj8h2UxBenGNQT3Nmr/cACtBgGXYP1M7psDcO4HBheBEQVMbXMDEnYfBz2i9wTbUgMFQLyrBOTYsPy3mFEF/ZtBAtFyOQeySTUEKAgVB312xwCk5TkERozNAPME+wfvgND7WCq7B9GSHQeu6EUE6Fu5AqDGKwUqMVsC4Vl5BW8siP2vFlME5phfBWDrcwJYLSsG1t7RBhDFNQdHUPUH5bzTB4t4cQHNidEHWgjxBfIyOwSpcdMEp9rm/YO84QIn4CkF7tIJBPbxsQQ39zsCXqZbBrKsXwf/Lx0DftEXALtOBPzwqLsCLQ/FA8TNpQVWAfD93l8dBrzOrQBvNA8FiUv7AutJmQQwI/sDf9wvByS2fwaQ9MsB6jxK/w9WlQGaKfUH2AphBxHcYQcTV28BIAjNBvauVPo570UDOGEZAUvslwJq1k8DT7dG99HJWwOKF+MBJ2uxA4+iNwB3UC8FHbm1BiGe1P4Lux0D1SJTBMAcUQUO/0D//Oy1AOIqrQZHdk0Fjx6PA2CtfwZ7zxsAWW1fBftsJwPtMYkE28CdA2LvswCnhgDtphhPBiWoTQCwbckFTSrRA928VQMubykD14QxBXN4ZwIAUdkG9WF5AS6WjwTivtEBU2w4/o3K2wLnTG8HdgwXBBzAowXBwvkDn2JO/bAi2wXgx2sEShgjAMxk0wVOklUEUUQDBS4dAQF6mH0CgmXhBL+ZvQZaZqEEAsyTB7Al6wPyNpz8KpyXBzr1GwKsxlMGdNCzBBT8PQXmU5UAcC95B392HwfFT7cB8JAdB5PyOQLnMw8AGBG3B56lnwdHEoj5tNkBBLr+FwZOgIcBOSes/epzZwdV/HkGn5RW/Q0mCwWu5kkGmSehA7P7qPoaKhcDM1gpB/adJQI7arUEn4m7Ae9FgQNYQW0Fel7JAwH1EweaxOUFXuuFAWkPjQA0ABMGAVaVA3YniP813YkCOZKTAN0QYwVxjWcBoFgjBJ8rJwTsiFcEGpKvAnD72wMvvj0GOs95AXI05Qd2hfcF0f0rB2xbFQDtLkr5ogNnAA2nNwTu7fEH1VLDAQROJwTgViEAWrpPBoKlDwWnqiT+YA6i+gRQzQZtJAEGs7bHB21txwMgP3MHyrIVAC2WiwAkXYj60mYjB2YZWwHrKJEHqAQjBGR69weZZCUGBUq9BQ/SDQZiGacEDBQVBdXF8QYpbeUFCdQi+/xsfQRPdNsF3H4vAoyB1QVe8ab4+vkfAbO2HQW1/ckHeN8+/62uPwFJAU0BQt8TASxzSQVcppECbD4FBKdUcwPgOq0DB6mZBfMarwK0e7MDOTa0/lvUBQHXNP8HgyhXBd4uFPlET27+lipTB7eXLv2GvwMH4q/BAPPRNQK6VrMAsyoVBTgAcQasbrsBwghI/Q3Ykwcoh979mvXlBue6YQewJVMGOeDBBpRijwbHlBMFzMLHBGLCuwZuSj0FwrK3AzyMnwGAtqcHfRIDAEWY8wWrHVMFpzSHAm9WtP9FSFEFM2klBlT+MQXXzq0FnB+vAMNSNwSYr6kDGkNlAy5ZNwfkxysBYIeW/PX0Twf/E9EDDaR4/B+F3QVF9hsFpwrHAyL6aQV5DQsEj7PDABvE/wBcOfsE1Q7ZB443LwJx05cC6SYDBg+JWQWoqjsHXD5RBfSgYP8dlhUEcnMZA8sXPv9eENMGIO/XAGn4CwSuZbEEiIrxAPUlNQY9op8H/WERAPsFCQc0kh0DEJX/BvjQlwbvjrcC0/stBP6SWwYCqg8CsmxTB4PnhQJGsOMGB2WfB6zV8QYo5xD9Z6M7B0Zg0QXbrfcEBGizB6MnZwRmyxsCazKzB7DvxQOd7i8F3oB9AH3GdwAczpUCdZNhANckiQJ3vPcFBSD/Au676QMend8GFzXrB7fwqwDUDdUHFv9ZAOicoQOj6SMCUg3hBHTavwJMLWUGVDIi/CZuhQRJln0AkW5NBv9JHwQ==\",\"dtype\":\"float32\",\"shape\":[1000]}},\"selected\":{\"id\":\"1051\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1050\",\"type\":\"UnionRenderers\"}},\"id\":\"1036\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"},\"text\":{\"field\":\"names\"},\"text_align\":\"center\",\"text_color\":{\"value\":\"#555555\"},\"text_font_size\":{\"value\":\"7pt\"},\"x\":{\"field\":\"x1\"},\"y\":{\"field\":\"x2\"},\"y_offset\":{\"value\":6}},\"id\":\"1042\",\"type\":\"LabelSet\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1014\",\"type\":\"BasicTicker\"}},\"id\":\"1017\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"formatter\":{\"id\":\"1047\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1003\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1019\",\"type\":\"BasicTicker\"}},\"id\":\"1022\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{\"plot\":null,\"text\":\"Word2Vec T-SNE for the 1000 most common words\"},\"id\":\"1002\",\"type\":\"Title\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1023\",\"type\":\"PanTool\"},{\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"id\":\"1026\",\"type\":\"ResetTool\"},{\"id\":\"1027\",\"type\":\"SaveTool\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1030\",\"type\":\"BoxAnnotation\"}},\"id\":\"1025\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"}},\"id\":\"1041\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"data_source\":{\"id\":\"1036\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1038\",\"type\":\"Scatter\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1039\",\"type\":\"Scatter\"},\"selection_glyph\":null,\"view\":{\"id\":\"1041\",\"type\":\"CDSView\"}},\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"SaveTool\"}],\"root_ids\":[\"1003\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.4\"}};\n",
              "  var render_items = [{\"docid\":\"19e3da3c-b0c0-401d-ad63-1a49d6c5939b\",\"roots\":{\"1003\":\"70ca52cc-2b36-4eff-adcb-32d3486755a8\"}}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        embed_document(root);\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "      attempts++;\n",
              "      if (attempts > 100) {\n",
              "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        clearInterval(timer);\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "tags": [],
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "1003"
            }
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "SEPoS3dcCoIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###1.6.1 Find an interesting cluster in the plot"
      ]
    },
    {
      "metadata": {
        "id": "It5EMPQEC3pU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To be able to distinguish between different words in the graph, I took advantage of the function defined above to create many subgraphs using small word list slices (1-50, 51-100 etc.).\n",
        "\n",
        "Tried to locate words about the general concept of 'measure of time' during a day or a year. Found the following words (coordinates is just an approximation and differ from those on the whole graph):\n",
        "\n",
        "time(10,0), times(25,25), age(15,15), minute(0,15), minutes(10,-5), hour(15,20), hours(-75,20), days(65,50), week(-25,15), weeks(-30,10), month(35,10), months(5,-20), year(10,-45), years(10,10), January(-40,-50), February(-30,60), March(30,20), May(30,40), July(45,25), September(30,55), October(5,35), November(-30,40), December(55,-40).\n",
        "\n",
        "The overwhelming majority of these words (with only few exceptions) are put together on the graph creating a small cluster.\n",
        "\n",
        "Furthermore, another interesting observation is that the lines (vectos) connecting couples of words souch as (week,weeks), (time,times), (minute,minutes), (month,months) are parallel to each other, having about the same length.\n"
      ]
    },
    {
      "metadata": {
        "id": "gS9qvVqAotVW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YXcAS5g5ossQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "DeKwKgrCsGa-"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 2"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vdVKXLvusGbM"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1. Train a word embedding model on the sentence classification corpus from the UCI Machine Learning repository"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JJklXqD0sGbX"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.1 Download Dataset"
      ]
    },
    {
      "metadata": {
        "id": "yP7r3zO4osjy",
        "colab_type": "code",
        "outputId": "ebe79c42-fc9b-4ad9-d5b4-08bd107480a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import re\n",
        "from os import listdir\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "szv6EWoWosb4",
        "colab_type": "code",
        "outputId": "4a3b3ebf-2b99-47cd-b8db-1208bce508f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00311/SentenceCorpus.zip\", filename=\"SentenceCorpus.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('SentenceCorpus.zip', <http.client.HTTPMessage at 0x7f565c9014e0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "metadata": {
        "id": "4jmXLOpuICW0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.2 Data Pre-Processing\n",
        "First I extract the whole corpus to Google Drive. Then I do the following:\n",
        "- I read all the files from the 'labeled_articles' folder, corresponding to just the first reviewer (since in this part I don't mind about labels)\n",
        "- Split each file to its sentences\n",
        "- Save sentences to list\n",
        "- I define lists of stop words to be removed from corpus.\n",
        "\n",
        "Then for each document (sentence) of the data:\n",
        "- Lower the characters\n",
        "- Remove non-Alpharithmetic characters\n",
        "- Remove multiple space characters\n",
        "- Tokenize (split on space characters)\n",
        "- Remove stop-words\n",
        "- Remove words with less than 2 characters\n",
        "- Remove empty sentences\n",
        "- Save the remaining sentences (list of tokens) to a list"
      ]
    },
    {
      "metadata": {
        "id": "c-ZevFVNplt0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# After execution each single line of the training documents is an element of corpus list \n",
        "corpus = []\n",
        "# The official stopworda coming with Corpus\n",
        "of_stopwords = []\n",
        "extra_words_to_remove = ['citation','number','symbol','misc','aimx','ownx','cont','base','of','','abstract','introduction']\n",
        "\n",
        "# Extract and store datafiles for local access\n",
        "with zipfile.ZipFile('SentenceCorpus.zip', 'r') as z:\n",
        "  z.extractall()\n",
        "  # Handling the training files\n",
        "  file_list = sorted(listdir('SentenceCorpus/labeled_articles/'))\n",
        "  for file_name in file_list:\n",
        "    if file_name.endswith('1.txt'):\n",
        "      file = z.read('SentenceCorpus/labeled_articles/' + file_name).decode('utf-8')\n",
        "      corpus.extend(file.split('\\n'))\n",
        "  # The official stop words\n",
        "  file = z.read('SentenceCorpus/word_lists/stopwords.txt').decode('utf-8')\n",
        "  of_stopwords.extend(file.split('\\n')[1:-1])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9dTahGPZlLV8",
        "colab_type": "code",
        "outputId": "d38b022f-82c7-4e24-b4de-cd8ddd8b768a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1597
        }
      },
      "cell_type": "code",
      "source": [
        "file_list"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.DS_Store',\n",
              " 'arxiv_annotate10_7_1.txt',\n",
              " 'arxiv_annotate10_7_2.txt',\n",
              " 'arxiv_annotate10_7_3.txt',\n",
              " 'arxiv_annotate1_13_1.txt',\n",
              " 'arxiv_annotate1_13_2.txt',\n",
              " 'arxiv_annotate1_13_3.txt',\n",
              " 'arxiv_annotate2_66_1.txt',\n",
              " 'arxiv_annotate2_66_2.txt',\n",
              " 'arxiv_annotate2_66_3.txt',\n",
              " 'arxiv_annotate3_80_1.txt',\n",
              " 'arxiv_annotate3_80_2.txt',\n",
              " 'arxiv_annotate3_80_3.txt',\n",
              " 'arxiv_annotate4_168_1.txt',\n",
              " 'arxiv_annotate4_168_2.txt',\n",
              " 'arxiv_annotate4_168_3.txt',\n",
              " 'arxiv_annotate5_240_1.txt',\n",
              " 'arxiv_annotate5_240_2.txt',\n",
              " 'arxiv_annotate5_240_3.txt',\n",
              " 'arxiv_annotate6_52_1.txt',\n",
              " 'arxiv_annotate6_52_2.txt',\n",
              " 'arxiv_annotate6_52_3.txt',\n",
              " 'arxiv_annotate7_268_1.txt',\n",
              " 'arxiv_annotate7_268_2.txt',\n",
              " 'arxiv_annotate7_268_3.txt',\n",
              " 'arxiv_annotate8_81_1.txt',\n",
              " 'arxiv_annotate8_81_2.txt',\n",
              " 'arxiv_annotate8_81_3.txt',\n",
              " 'arxiv_annotate9_279_1.txt',\n",
              " 'arxiv_annotate9_279_2.txt',\n",
              " 'arxiv_annotate9_279_3.txt',\n",
              " 'jdm_annotate10_210_1.txt',\n",
              " 'jdm_annotate10_210_2.txt',\n",
              " 'jdm_annotate10_210_3.txt',\n",
              " 'jdm_annotate1_103_1.txt',\n",
              " 'jdm_annotate1_103_2.txt',\n",
              " 'jdm_annotate1_103_3.txt',\n",
              " 'jdm_annotate2_107_1.txt',\n",
              " 'jdm_annotate2_107_2.txt',\n",
              " 'jdm_annotate2_107_3.txt',\n",
              " 'jdm_annotate3_120_1.txt',\n",
              " 'jdm_annotate3_120_2.txt',\n",
              " 'jdm_annotate3_120_3.txt',\n",
              " 'jdm_annotate4_220_1.txt',\n",
              " 'jdm_annotate4_220_2.txt',\n",
              " 'jdm_annotate4_220_3.txt',\n",
              " 'jdm_annotate5_228_1.txt',\n",
              " 'jdm_annotate5_228_2.txt',\n",
              " 'jdm_annotate5_228_3.txt',\n",
              " 'jdm_annotate6_32_1.txt',\n",
              " 'jdm_annotate6_32_2.txt',\n",
              " 'jdm_annotate6_32_3.txt',\n",
              " 'jdm_annotate7_265_1.txt',\n",
              " 'jdm_annotate7_265_2.txt',\n",
              " 'jdm_annotate7_265_3.txt',\n",
              " 'jdm_annotate8_177_1.txt',\n",
              " 'jdm_annotate8_177_2.txt',\n",
              " 'jdm_annotate8_177_3.txt',\n",
              " 'jdm_annotate9_45_1.txt',\n",
              " 'jdm_annotate9_45_2.txt',\n",
              " 'jdm_annotate9_45_3.txt',\n",
              " 'plos_annotate10_1140_1.txt',\n",
              " 'plos_annotate10_1140_2.txt',\n",
              " 'plos_annotate10_1140_3.txt',\n",
              " 'plos_annotate1_6_1.txt',\n",
              " 'plos_annotate1_6_2.txt',\n",
              " 'plos_annotate1_6_3.txt',\n",
              " 'plos_annotate2_336_1.txt',\n",
              " 'plos_annotate2_336_2.txt',\n",
              " 'plos_annotate2_336_3.txt',\n",
              " 'plos_annotate3_798_1.txt',\n",
              " 'plos_annotate3_798_2.txt',\n",
              " 'plos_annotate3_798_3.txt',\n",
              " 'plos_annotate4_1052_1.txt',\n",
              " 'plos_annotate4_1052_2.txt',\n",
              " 'plos_annotate4_1052_3.txt',\n",
              " 'plos_annotate5_1375_1.txt',\n",
              " 'plos_annotate5_1375_2.txt',\n",
              " 'plos_annotate5_1375_3.txt',\n",
              " 'plos_annotate6_1032_1.txt',\n",
              " 'plos_annotate6_1032_2.txt',\n",
              " 'plos_annotate6_1032_3.txt',\n",
              " 'plos_annotate7_1233_1.txt',\n",
              " 'plos_annotate7_1233_2.txt',\n",
              " 'plos_annotate7_1233_3.txt',\n",
              " 'plos_annotate8_123_1.txt',\n",
              " 'plos_annotate8_123_2.txt',\n",
              " 'plos_annotate8_123_3.txt',\n",
              " 'plos_annotate9_1187_1.txt',\n",
              " 'plos_annotate9_1187_2.txt',\n",
              " 'plos_annotate9_1187_3.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "hnvvEWTbSITP",
        "colab_type": "code",
        "outputId": "3f073502-7fc5-4f75-deda-6b0b6460d306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# A sentence from a training article\n",
        "corpus[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MISC\\tThe Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied\\r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "kMrDXe45Ny3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# After execution each element of corpus list (line of the training documents) is\n",
        "# transformed to a list of clean tokens as element in preproc_corpus list\n",
        "preproc_corpus = []\n",
        "for doc in corpus:\n",
        "  doc = doc.lower()\n",
        "  doc = re.sub(r'[^a-z0-9]+', ' ',doc)\n",
        "  doc = re.sub(r'\\s+', ' ',doc)\n",
        "  doc = doc.split(' ')\n",
        "  doc = [word for word in doc if word not in of_stopwords+extra_words_to_remove+stopwords.words('english')]\n",
        "  doc = [word for word in doc if len(word)>1]\n",
        "  if len(doc) > 0:\n",
        "    preproc_corpus.append(doc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0xPr_7ASoji",
        "colab_type": "code",
        "outputId": "b8ac9abc-87d6-4720-dcbd-d68bdded0b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "# The same above sentence after pre-processing\n",
        "preproc_corpus[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['minimum',\n",
              " 'description',\n",
              " 'length',\n",
              " 'principle',\n",
              " 'online',\n",
              " 'sequence',\n",
              " 'estimation',\n",
              " 'prediction',\n",
              " 'proper',\n",
              " 'learning',\n",
              " 'setup',\n",
              " 'studied']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "metadata": {
        "id": "3aXbP-PIU2xX",
        "colab_type": "code",
        "outputId": "71b738b0-f22b-4f09-d16b-326e0e212549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(corpus), len(preproc_corpus)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1120, 1039)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 188
        }
      ]
    },
    {
      "metadata": {
        "id": "e0spAVGaVKRF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.3 Train a basic word embedding model"
      ]
    },
    {
      "metadata": {
        "id": "p2i3vIZYKPYQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define the W2V model within a function so to be able to set the hyperparameters\n",
        "def customW2Vmodel(window=4,size=100,sg=1,min_count=1,workers=-1,negative=5):\n",
        "  model = Word2Vec(window=window,size=size,sg=sg,min_count=min_count,workers=workers,negative=negative)\n",
        "  model.build_vocab(preproc_corpus)  # Building the model vocabulary\n",
        "  model.train(preproc_corpus,total_examples=model.corpus_count,epochs=model.iter)\n",
        "  return(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zXhQK8QaLMWL",
        "colab_type": "code",
        "outputId": "1ebd099f-4727-410c-e187-ee3210d0b5a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model0 = customW2Vmodel(window=4,size=100,sg=1,min_count=1,workers=-1,negative=5)\n",
        "print('Vocabulary size: ',len(model0.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 85.7 ms, sys: 5.03 ms, total: 90.8 ms\n",
            "Wall time: 96.3 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JyNw-NJU4QGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.1.3.1 Save trained model to Drive\n",
        "\n",
        "This is the trained model to use in Part 3 of the Assignment.\n",
        "\n",
        "path to model: '/content/drive/My Drive/NLP Assignment 2/Part3/embedding_word2vec.txt'"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d6cd90dc-7da5-4834-efd5-84f72a40a1b9",
        "id": "zIPkD5bE6sm0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Connect to personal Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eBOXuxYG4PkB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drive path to trained model\n",
        "embedding_word2vec_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/embedding_word2vec.txt'\n",
        "\n",
        "# Save model in ASCII (word2vec) format\n",
        "model0.wv.save_word2vec_format(embedding_word2vec_filename, binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MPha83k5ekbA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.2 Train other models on the same data, but with one hyperparameter different (for example, window size or vector size)\n",
        "\n",
        "I will study the effect of the following hyperparameters:\n",
        "- vector dimensionality (size)\n",
        "- window size (window)\n",
        "- minimum token frequency (min_count)\n",
        "- negative sampling (negative)\n",
        "- training algorithm (sg)"
      ]
    },
    {
      "metadata": {
        "id": "U9OIM16Ny2Iu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Adjusting vector dimensionality (size)\n",
        "\n",
        "I will adjust the vector dimensionality from 100 to 250 and then to 500"
      ]
    },
    {
      "metadata": {
        "id": "5N7-kdUWU2Re",
        "colab_type": "code",
        "outputId": "818eaf44-f91e-4156-bbcd-cda04935ad63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model1 = customW2Vmodel(size=250)\n",
        "print('Vocabulary size: ',len(model1.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 97.8 ms, sys: 1.92 ms, total: 99.7 ms\n",
            "Wall time: 106 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a9a9e99a-e41b-438a-cd39-cbcf2810a5cb",
        "id": "nq66UhnA7b4B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model11 = customW2Vmodel(size=500)\n",
        "print('Vocabulary size: ',len(model1.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 114 ms, sys: 2.94 ms, total: 117 ms\n",
            "Wall time: 123 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "48Q61a2S0v7j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Adjusting window size (window)\n",
        "\n",
        "I will adjust the window size from 4 to 8"
      ]
    },
    {
      "metadata": {
        "id": "6GgRoCSxU2EP",
        "colab_type": "code",
        "outputId": "c2ab5e67-26e5-4e57-cebc-4baba4bc1580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model2 = customW2Vmodel(window=8)\n",
        "print('Vocabulary size: ',len(model2.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 91.5 ms, sys: 5.53 ms, total: 97 ms\n",
            "Wall time: 100 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "YPeLfydZ1MEx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 Adjusting minimum token frequency (min_count)\n",
        "\n",
        "I will adjust the minimum token frequency from 1 to 5"
      ]
    },
    {
      "metadata": {
        "id": "sHgCpSmH1NB6",
        "colab_type": "code",
        "outputId": "666affa1-89b1-4e0a-d17e-668e5ce7a514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model3 = customW2Vmodel(min_count=5)\n",
        "print('Vocabulary size: ',len(model3.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  870\n",
            "CPU times: user 39.4 ms, sys: 3.04 ms, total: 42.4 ms\n",
            "Wall time: 46.9 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrfhJFirGzOK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.4 Adjusting negative sampling (negative)\n",
        "\n",
        "I will adjust the negative sampling from 5 to 10"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "da4fc75a-66aa-4ad9-c6f4-518bb2e8cae2",
        "id": "BLbY6hFoGwp5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model4 = customW2Vmodel(negative=10)\n",
        "print('Vocabulary size: ',len(model4.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 94.2 ms, sys: 3.78 ms, total: 98 ms\n",
            "Wall time: 102 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "_70tNxvUHpDa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.5 Adjusting training algorithm (sg=0 for CBOW; sg=1 for skip-gram)\n",
        "\n",
        "I will adjust the training algorithm from skip-gram to CBOW"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "3b6b0034-ca5b-4be8-8e15-50049732502e",
        "id": "zxdVzFifHnJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "model5 = customW2Vmodel(sg=0)\n",
        "print('Vocabulary size: ',len(model5.wv.vocab))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size:  3887\n",
            "CPU times: user 104 ms, sys: 1.77 ms, total: 106 ms\n",
            "Wall time: 110 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xs0koOPbPAzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QRZkQfe5w0TP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3. Compare models with respect to Performance\n",
        "\n",
        "I choose a set of metrics to compare models' performance, including training time and results delivering from various in-built functions"
      ]
    },
    {
      "metadata": {
        "id": "88rONMZaLDfb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model to compare\n",
        "models = [('Basic Model',model0),('Vector Dimensionality 250',model1),\n",
        "          ('Vector Dimensionality 500',model11),('Window Size',model2),\n",
        "          ('Min Token Frequency',model3),('Negative Sampling',model4),\n",
        "          ('Train Algorithm',model5)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wl3G2KcqXoiq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 Training time"
      ]
    },
    {
      "metadata": {
        "id": "k-dgceTUYQp0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Conclusions\n",
        "Regarding on hyperparameters' effect on model's training time  I conclude the following:\n",
        "- Doubling window size (window from 4 to 8) and negative sampling (negative from 5 to 10) only slightly impairs the model's training time (from 91ms to 97ms and 98ms respectively)\n",
        "- Increasing vector's dimensionality (size from 100 to 250 and 500) gradually (but not proportionately) impairs the model's training time (from 91ms to 100ms and 117ms respectively)\n",
        "- Increasing 5 times the minimum token frequency (min_count from 1 to 5) decreases 2 times the model's training time (from 91ms to 42ms)\n",
        "- Changing training algorithm (from skip-gram to CBOW) significantly increases the model's training time (from 91ms to 110ms)"
      ]
    },
    {
      "metadata": {
        "id": "ZcpWrsAvYSdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.2 10 best pairs of words\n",
        "\n",
        "For each model compute and return the best pair words for the first 10 most frequent word of vocabulary using model's 'similar_pairs' built-in function"
      ]
    },
    {
      "metadata": {
        "id": "CWaa5ZxW4IeF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m_hDRsmz-B7Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that uses the model to determine the N best pairs of tokens, based on\n",
        "# similarity, among all possible pairs in the vocabolary\n",
        "def bestSimPairs(model,N,vocab=None):\n",
        "  import operator\n",
        "  \n",
        "  # Use dictionary structure to store word pairs and their similarities\n",
        "  similar_pairs = {}\n",
        "  if(vocab == None):\n",
        "    vocab = model.wv.index2word\n",
        "    \n",
        "  # Compare each word of the vocabulary against all others and find similarities\n",
        "  for p1 in vocab:\n",
        "    pairs = model.wv.most_similar(p1,topn=1)\n",
        "    p2,sim = pairs[0]\n",
        "    similar_pairs[(p1, p2)] = sim\n",
        "\n",
        "  # Sort dictionary's data by values\n",
        "  sorted_similar_pairs = sorted(similar_pairs.items(),key=operator.itemgetter(1),reverse=True)\n",
        "\n",
        "  # Since every pair appears twice, I keep every other element of the ordered list\n",
        "  sorted_similar_pairs = sorted_similar_pairs[0:2*N:2]\n",
        "  \n",
        "  return(sorted_similar_pairs) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "PgKxMCFyb-wy",
        "outputId": "ea4b091a-e4d2-437d-82d2-3ca1df3d4160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1513
        }
      },
      "cell_type": "code",
      "source": [
        "vocab = None\n",
        "\n",
        "best_pairs_by_method = {}\n",
        "\n",
        "for (name,model) in models:\n",
        "  best_pairs_by_method[name] = bestSimPairs(model, 10, vocab)\n",
        "\n",
        "print(pd.DataFrame.from_dict(best_pairs_by_method))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                                      Basic Model  \\\n",
            "0          ((noisy, driving), 0.4894424080848694)   \n",
            "1      ((works, assemblies), 0.48557597398757935)   \n",
            "2         ((simpler, burst), 0.48217469453811646)   \n",
            "3  ((inserts, contribution), 0.47997167706489563)   \n",
            "4              ((deal, care), 0.4799291491508484)   \n",
            "5   ((employ, concurrently), 0.46593475341796875)   \n",
            "6              ((go, halali), 0.4637524485588074)   \n",
            "7         ((fit, employing), 0.45622801780700684)   \n",
            "8  ((specifically, maximize), 0.4561006724834442)   \n",
            "9             ((isps, odour), 0.4546169638633728)   \n",
            "\n",
            "                                 Min Token Frequency  \\\n",
            "0            ((variants, times), 0.4278267025947571)   \n",
            "1  ((explicitly, psychological), 0.4277902841567993)   \n",
            "2             ((new, involving), 0.4151037931442261)   \n",
            "3              ((human, hetero), 0.4103476405143738)   \n",
            "4           ((believed, times), 0.39652907848358154)   \n",
            "5          ((processing, show), 0.39632487297058105)   \n",
            "6    ((generally, constraints), 0.39540350437164307)   \n",
            "7               ((input, usage), 0.3948977291584015)   \n",
            "8               ((maker, shown), 0.3939894437789917)   \n",
            "9               ((favor, impact), 0.393761545419693)   \n",
            "\n",
            "                                Negative Sampling  \\\n",
            "0          ((noisy, driving), 0.4894424080848694)   \n",
            "1      ((works, assemblies), 0.48557597398757935)   \n",
            "2         ((simpler, burst), 0.48217469453811646)   \n",
            "3  ((inserts, contribution), 0.47997167706489563)   \n",
            "4              ((deal, care), 0.4799291491508484)   \n",
            "5   ((employ, concurrently), 0.46593475341796875)   \n",
            "6              ((go, halali), 0.4637524485588074)   \n",
            "7         ((fit, employing), 0.45622801780700684)   \n",
            "8  ((specifically, maximize), 0.4561006724834442)   \n",
            "9             ((isps, odour), 0.4546169638633728)   \n",
            "\n",
            "                                  Train Algorithm  \\\n",
            "0          ((noisy, driving), 0.4894424080848694)   \n",
            "1      ((works, assemblies), 0.48557597398757935)   \n",
            "2         ((simpler, burst), 0.48217469453811646)   \n",
            "3  ((inserts, contribution), 0.47997167706489563)   \n",
            "4              ((deal, care), 0.4799291491508484)   \n",
            "5   ((employ, concurrently), 0.46593475341796875)   \n",
            "6              ((go, halali), 0.4637524485588074)   \n",
            "7         ((fit, employing), 0.45622801780700684)   \n",
            "8  ((specifically, maximize), 0.4561006724834442)   \n",
            "9             ((isps, odour), 0.4546169638633728)   \n",
            "\n",
            "                       Vector Dimensionality 250  \\\n",
            "0  ((dynamically, pioneered), 0.342629075050354)   \n",
            "1        ((desire, humans), 0.32499808073043823)   \n",
            "2            ((target, lag), 0.3200412690639496)   \n",
            "3    ((preference, search), 0.31591999530792236)   \n",
            "4     ((maximizing, actual), 0.3146668076515198)   \n",
            "5    ((skipping, currently), 0.3127231001853943)   \n",
            "6        ((backbone, reals), 0.3062945604324341)   \n",
            "7       ((progressed, sce), 0.30408334732055664)   \n",
            "8    ((however, hindering), 0.30358678102493286)   \n",
            "9    ((serving, criticized), 0.3035593628883362)   \n",
            "\n",
            "                         Vector Dimensionality 500  \\\n",
            "0     ((skipping, currently), 0.25298449397087097)   \n",
            "1        ((entirely, select), 0.24366194009780884)   \n",
            "2  ((feedback, prerequisite), 0.22531679272651672)   \n",
            "3      ((genes, appropriate), 0.22392675280570984)   \n",
            "4              ((owned, etc), 0.22205279767513275)   \n",
            "5               ((search, vc), 0.2196393609046936)   \n",
            "6              ((rule, worth), 0.2168198823928833)   \n",
            "7           ((repress, curry), 0.2161499559879303)   \n",
            "8   ((presented, unfavorable), 0.2155771255493164)   \n",
            "9           ((effective, 50), 0.21528270840644836)   \n",
            "\n",
            "                                      Window Size  \n",
            "0          ((noisy, driving), 0.4894424080848694)  \n",
            "1      ((works, assemblies), 0.48557597398757935)  \n",
            "2         ((simpler, burst), 0.48217469453811646)  \n",
            "3  ((inserts, contribution), 0.47997167706489563)  \n",
            "4              ((deal, care), 0.4799291491508484)  \n",
            "5   ((employ, concurrently), 0.46593475341796875)  \n",
            "6              ((go, halali), 0.4637524485588074)  \n",
            "7         ((fit, employing), 0.45622801780700684)  \n",
            "8  ((specifically, maximize), 0.4561006724834442)  \n",
            "9             ((isps, odour), 0.4546169638633728)  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LR0Diwwmzi_H"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2.1 Conclusions\n",
        "Regarding on hyperparameters' effect on results returning from model's 'similar_pairs' built-in function I conclude the following:\n",
        "- Adjusting window size,  negative sampling, training algorithm doesn't affect the returning word pairs\n",
        "- Adjusting vector's dimensionality and minimum token frequency radically affects the returning word pairs since the most frequent words are totally diferrent compared to the basic model\n"
      ]
    },
    {
      "metadata": {
        "id": "r6Xo4ecmBsuY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.3 Compute words' cosine distance"
      ]
    },
    {
      "metadata": {
        "id": "qkTrJ_17Tfv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each model find the cosine distance of specific pairs of words using model's 'similarity' built-in function"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CcxK2fKyT-WF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts a trained word2vec model and list of pairs of words and\n",
        "# computes the cosine distance of each pair\n",
        "def cosDist(model, pairs):\n",
        "  cos_dist = []\n",
        "  \n",
        "  for pair in pairs:\n",
        "    w1,w2 = pair\n",
        "    cos_dist.append(model.wv.similarity(w1,w2))\n",
        "    \n",
        "  return(cos_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kKLTeUb8T-W3",
        "outputId": "4139c50f-d47a-45fd-9fc9-9a177d23232f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Create 5 random pairs of words from the smallest vocabulary\n",
        "vocab = model3.wv.index2word\n",
        "list = []\n",
        "for i in range(5):\n",
        "  list.append(random.choices(vocab, k=2))\n",
        "\n",
        "cosDist_by_method = {}\n",
        "cosDist_by_method['List of pairs'] = list\n",
        "\n",
        "# Use the methods to determine the cosine distance of words in each pair\n",
        "for (name,model) in models:\n",
        "  cosDist_by_method[name] = cosDist(model, list)\n",
        "\n",
        "print(pd.DataFrame.from_dict(cosDist_by_method).set_index('List of pairs'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                       Basic Model  Min Token Frequency  Negative Sampling  \\\n",
            "List of pairs                                                                \n",
            "[exchange, easy]         -0.004057            -0.004057          -0.004057   \n",
            "[stimuli, complexity]    -0.035405            -0.035405          -0.035405   \n",
            "[responsible, easy]      -0.038228            -0.038228          -0.038228   \n",
            "[implies, could]          0.135873             0.135873           0.135873   \n",
            "[recent, correct]         0.156587             0.156587           0.156587   \n",
            "\n",
            "                       Train Algorithm  Vector Dimensionality 250  \\\n",
            "List of pairs                                                       \n",
            "[exchange, easy]             -0.004057                   0.079440   \n",
            "[stimuli, complexity]        -0.035405                  -0.130871   \n",
            "[responsible, easy]          -0.038228                   0.022298   \n",
            "[implies, could]              0.135873                   0.104804   \n",
            "[recent, correct]             0.156587                   0.107683   \n",
            "\n",
            "                       Vector Dimensionality 500  Window Size  \n",
            "List of pairs                                                  \n",
            "[exchange, easy]                        0.004655    -0.004057  \n",
            "[stimuli, complexity]                  -0.134064    -0.035405  \n",
            "[responsible, easy]                     0.019640    -0.038228  \n",
            "[implies, could]                        0.056555     0.135873  \n",
            "[recent, correct]                      -0.003747     0.156587  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "b2zZ5o6f4Vkv"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.3.3.1 Conclusions\n",
        "Regarding on hyperparameters' effect on results returning from model's 'similarity' built-in function I conclude the following:\n",
        "- Adjusting window size,  negative sampling, training algorithm and minimum token frequency doesn't affect the returning words' cosine distance\n",
        "- Adjusting vector's dimensionality radically alters the returning words' cosine distance\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RgEXfXBCX2Kg"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.4 Compute phrases' cosine distance"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "C_k2i3o4X374"
      },
      "cell_type": "markdown",
      "source": [
        "For each model find the cosine distance of specific phrases using model's 'n_similarity' built-in function"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "OxeygS2lYLO4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts a trained model and list of pairs of lists of words or\n",
        "# strings and computes the cosine distance of each pair. Strings in a pair should\n",
        "# have same length\n",
        "def cosDistPhrase(model, lists):\n",
        "  cos_phrase_dist = []\n",
        "  \n",
        "  for pair in lists:\n",
        "    p1,p2 = pair\n",
        "    cos_phrase_dist.append(model.wv.n_similarity(p1,p2))\n",
        "    \n",
        "  return(cos_phrase_dist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bUQC_NWrYNgK",
        "outputId": "a22e7638-eedd-4e41-9325-671421081455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1027
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Create 5 pairs of random 4-element lists of words from the smallest vocabulary\n",
        "vocab = model3.wv.index2word\n",
        "lists = []\n",
        "for i in range(5):\n",
        "  lists.append([random.choices(vocab, k=4), random.choices(vocab, k=4)])\n",
        "\n",
        "cosDistPhrase_by_method = {}\n",
        "cosDistPhrase_by_method['Lists of pairs'] = lists\n",
        "\n",
        "# Use the methods to determine the cosine distance of words in each list\n",
        "for (name,model) in models:\n",
        "  cosDistPhrase_by_method[name] = cosDistPhrase(model, lists)\n",
        "\n",
        "print(pd.DataFrame.from_dict(cosDistPhrase_by_method).set_index('Lists of pairs'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    Basic Model  \\\n",
            "Lists of pairs                                                    \n",
            "[[decisions, instances, cellular, testing], [un...     0.044305   \n",
            "[[output, involving, questions, disutility], [p...    -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...     0.117287   \n",
            "[[preferences, probability, value, six], [proce...    -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...     0.147153   \n",
            "\n",
            "                                                    Min Token Frequency  \\\n",
            "Lists of pairs                                                            \n",
            "[[decisions, instances, cellular, testing], [un...             0.044305   \n",
            "[[output, involving, questions, disutility], [p...            -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...             0.117287   \n",
            "[[preferences, probability, value, six], [proce...            -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...             0.147153   \n",
            "\n",
            "                                                    Negative Sampling  \\\n",
            "Lists of pairs                                                          \n",
            "[[decisions, instances, cellular, testing], [un...           0.044305   \n",
            "[[output, involving, questions, disutility], [p...          -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...           0.117287   \n",
            "[[preferences, probability, value, six], [proce...          -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...           0.147153   \n",
            "\n",
            "                                                    Train Algorithm  \\\n",
            "Lists of pairs                                                        \n",
            "[[decisions, instances, cellular, testing], [un...         0.044305   \n",
            "[[output, involving, questions, disutility], [p...        -0.087051   \n",
            "[[hard, place, age, majority], [network, belief...         0.117287   \n",
            "[[preferences, probability, value, six], [proce...        -0.095530   \n",
            "[[would, complexity, context, solution], [viewe...         0.147153   \n",
            "\n",
            "                                                    Vector Dimensionality 250  \\\n",
            "Lists of pairs                                                                  \n",
            "[[decisions, instances, cellular, testing], [un...                  -0.088979   \n",
            "[[output, involving, questions, disutility], [p...                  -0.043583   \n",
            "[[hard, place, age, majority], [network, belief...                  -0.010387   \n",
            "[[preferences, probability, value, six], [proce...                  -0.098134   \n",
            "[[would, complexity, context, solution], [viewe...                   0.067941   \n",
            "\n",
            "                                                    Vector Dimensionality 500  \\\n",
            "Lists of pairs                                                                  \n",
            "[[decisions, instances, cellular, testing], [un...                  -0.059622   \n",
            "[[output, involving, questions, disutility], [p...                  -0.023088   \n",
            "[[hard, place, age, majority], [network, belief...                   0.022662   \n",
            "[[preferences, probability, value, six], [proce...                  -0.086583   \n",
            "[[would, complexity, context, solution], [viewe...                   0.042429   \n",
            "\n",
            "                                                    Window Size  \n",
            "Lists of pairs                                                   \n",
            "[[decisions, instances, cellular, testing], [un...     0.044305  \n",
            "[[output, involving, questions, disutility], [p...    -0.087051  \n",
            "[[hard, place, age, majority], [network, belief...     0.117287  \n",
            "[[preferences, probability, value, six], [proce...    -0.095530  \n",
            "[[would, complexity, context, solution], [viewe...     0.147153  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "upw8ZahC5mzM"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.3.4.1 Conclusions\n",
        "Regarding on hyperparameters' effect on results returning from model's 'n_similarity' built-in function I conclude the following:\n",
        "- Adjusting window size,  negative sampling, training algorithm and minimum token frequency doesn't affect the returning phrases' cosine distance\n",
        "- Adjusting vector's dimensionality radically alters the returning phrases' cosine distance\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "05SlVR2qTKxO"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.5 Find which word doesn't match with the others"
      ]
    },
    {
      "metadata": {
        "id": "BkpoucYzCIQE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each model find which word from the given lists doesn’t go with the others using model's 'doesnt_match' built-in function"
      ]
    },
    {
      "metadata": {
        "id": "sDCjrKvXaXGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts lists of words and uses the model to determine which word\n",
        "# doesn't match with the others\n",
        "def doesntMatch(model, lists):\n",
        "  dont_match = []\n",
        "  \n",
        "  for list in lists:\n",
        "    dont_match.append(model.doesnt_match(list))\n",
        "    \n",
        "  return(dont_match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PTjlqzdzJKZO",
        "colab_type": "code",
        "outputId": "fb2a6138-f66f-4090-d96d-1092a8ec9ac0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1062
        }
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Create 5 random 5-element lists from the smallest vocabulary\n",
        "vocab = model3.wv.index2word\n",
        "lists = []\n",
        "for i in range(5):\n",
        "  lists.append(random.choices(vocab, k=5))\n",
        "\n",
        "doesnt_match_by_method = {}\n",
        "doesnt_match_by_method['Lists of words'] = lists\n",
        "\n",
        "# Use the methods to determine which word doesn't match in each list\n",
        "for (name,model) in models:\n",
        "  doesnt_match_by_method[name] = doesntMatch(model, lists)\n",
        "\n",
        "print(pd.DataFrame.from_dict(doesnt_match_by_method).set_index('Lists of words'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                Basic Model  \\\n",
            "Lists of words                                                \n",
            "[polynomial, firing, length, three, fixed]            three   \n",
            "[real, consequence, involving, splicing, task]  consequence   \n",
            "[framing, dual, couple, bernoulli, option]        bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]     analyzed   \n",
            "[able, instances, couple, procedure, whether]        couple   \n",
            "\n",
            "                                               Min Token Frequency  \\\n",
            "Lists of words                                                       \n",
            "[polynomial, firing, length, three, fixed]                   three   \n",
            "[real, consequence, involving, splicing, task]         consequence   \n",
            "[framing, dual, couple, bernoulli, option]               bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]            analyzed   \n",
            "[able, instances, couple, procedure, whether]               couple   \n",
            "\n",
            "                                               Negative Sampling  \\\n",
            "Lists of words                                                     \n",
            "[polynomial, firing, length, three, fixed]                 three   \n",
            "[real, consequence, involving, splicing, task]       consequence   \n",
            "[framing, dual, couple, bernoulli, option]             bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]          analyzed   \n",
            "[able, instances, couple, procedure, whether]             couple   \n",
            "\n",
            "                                               Train Algorithm  \\\n",
            "Lists of words                                                   \n",
            "[polynomial, firing, length, three, fixed]               three   \n",
            "[real, consequence, involving, splicing, task]     consequence   \n",
            "[framing, dual, couple, bernoulli, option]           bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]        analyzed   \n",
            "[able, instances, couple, procedure, whether]           couple   \n",
            "\n",
            "                                               Vector Dimensionality 250  \\\n",
            "Lists of words                                                             \n",
            "[polynomial, firing, length, three, fixed]                         three   \n",
            "[real, consequence, involving, splicing, task]               consequence   \n",
            "[framing, dual, couple, bernoulli, option]                          dual   \n",
            "[speed, analyzed, variants, predict, feedback]                     speed   \n",
            "[able, instances, couple, procedure, whether]                     couple   \n",
            "\n",
            "                                               Vector Dimensionality 500  \\\n",
            "Lists of words                                                             \n",
            "[polynomial, firing, length, three, fixed]                         three   \n",
            "[real, consequence, involving, splicing, task]               consequence   \n",
            "[framing, dual, couple, bernoulli, option]                     bernoulli   \n",
            "[speed, analyzed, variants, predict, feedback]                   predict   \n",
            "[able, instances, couple, procedure, whether]                     couple   \n",
            "\n",
            "                                                Window Size  \n",
            "Lists of words                                               \n",
            "[polynomial, firing, length, three, fixed]            three  \n",
            "[real, consequence, involving, splicing, task]  consequence  \n",
            "[framing, dual, couple, bernoulli, option]        bernoulli  \n",
            "[speed, analyzed, variants, predict, feedback]     analyzed  \n",
            "[able, instances, couple, procedure, whether]        couple  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Pb5eYZPd6V9o"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.3.5.1 Conclusions\n",
        "Regarding on hyperparameters' effect on results returning from model's 'doesnt_match' built-in function I conclude the following:\n",
        "- Adjusting window size,  negative sampling, training algorithm and minimum token frequency doesn't affect the returning not matching word\n",
        "- Adjusting vector's dimensionality slightly alters the returning not matching word\n"
      ]
    },
    {
      "metadata": {
        "id": "q0IYiShkglx7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.6 Predict output word"
      ]
    },
    {
      "metadata": {
        "id": "zRFeh2WcglR3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For each model get the probability distribution of the center word given context words using model's 'predict_output_word' built-in function"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iNNbXmHxg-OF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function that accepts lists of words as context and uses the model to determine\n",
        "# the most probable center word\n",
        "def findCenter(model, lists):\n",
        "  center = []\n",
        "  \n",
        "  for list in lists:\n",
        "    center.append(model.predict_output_word(list, topn=1))\n",
        "    \n",
        "  return(center)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jKVLlCr8hARL",
        "outputId": "d153d0e7-f895-4e3f-e0a3-a810701e8652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "cell_type": "code",
      "source": [
        "# List of 5 tokenized random phrases from corpus\n",
        "lists = [['pay','their','taxes','despite','low','likelihood'],\n",
        "        ['incorporate','potentially','given','loss','function'],\n",
        "        ['challenges','accurate','realistic','modeling'],\n",
        "        ['derive','macroscopic','statistics','different','types'],\n",
        "        ['critical','rapid','encoding','novel','information']]\n",
        "\n",
        "center_by_method = {}\n",
        "\n",
        "# Use the methods to determine the center word for each list of context words\n",
        "for (name,model) in models:\n",
        "  center_by_method[name] = findCenter(model, lists)\n",
        "\n",
        "print(pd.DataFrame.from_dict(center_by_method))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Basic Model      Min Token Frequency  \\\n",
            "0  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "1  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "2  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "3  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "4  [(model, 0.00025726782)]  [(model, 0.0011494253)]   \n",
            "\n",
            "          Negative Sampling           Train Algorithm  \\\n",
            "0  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "1  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "2  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "3  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "4  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "\n",
            "  Vector Dimensionality 250 Vector Dimensionality 500  \\\n",
            "0  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "1  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "2  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "3  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "4  [(model, 0.00025726782)]  [(model, 0.00025726782)]   \n",
            "\n",
            "                Window Size  \n",
            "0  [(model, 0.00025726782)]  \n",
            "1  [(model, 0.00025726782)]  \n",
            "2  [(model, 0.00025726782)]  \n",
            "3  [(model, 0.00025726782)]  \n",
            "4  [(model, 0.00025726782)]  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "j7Qe5XoY7VFK"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.3.6.1 Conclusions\n",
        "Regarding on hyperparameters' effect on results returning from model's 'predict_output_word' built-in function I conclude the following:\n",
        "- All models return exactly the same result no matter the model used or the word context. I can't explain this result"
      ]
    },
    {
      "metadata": {
        "id": "-Bvv-0zLNx3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QFUUJcUotIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DPHStF87k2e2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Part 3"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GpY8JG_xQ7LZ"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.0. Prepare the data to be used in this part of Assignment\n",
        "\n",
        "- Download DataSet and extract it locally to Google Drive\n",
        "- Extract vocabulary from DataSet and save it locally to Google Drive\n",
        "- Prepare and clean the documents to be used for the training and evaluation of the models (X variable) and save it locally to Google Drive\n",
        "- Prepare the labels of the documents (y variable) and save it locally to Google Drive"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "135f79f1-9ddc-48e2-ca2d-68d31e65942e",
        "id": "ixLV9Y7_M0Mg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# Connect to personal Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "q5_FYseGLYxq",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drive paths used to store data\n",
        "training_docs_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/SentenceCorpus/labeled_articles/'\n",
        "official_stopwords = '/content/drive/My Drive/NLP Assignment 2/Part3/SentenceCorpus/word_lists/stopwords.txt'\n",
        "vocab_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/vocab'\n",
        "vocab_list_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/vocab.txt'\n",
        "embedding_word2vec_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/embedding_word2vec.txt'\n",
        "X_var_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/X_var.npy'\n",
        "y_var_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/y_var.npy'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DmMEsHK5Tnaz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.0.1 Download and extract DataSet locally to Drive\n",
        "\n",
        "Once downloaded and extracted on Google Drive, DataSet can be accessed locally for future use"
      ]
    },
    {
      "metadata": {
        "id": "p-gzqcMANitW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pNludKVRpSiZ",
        "colab_type": "code",
        "outputId": "bb69fdf0-0f9f-43d8-95a7-e59513b7dbd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00311/SentenceCorpus.zip\", filename=\"SentenceCorpus.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('SentenceCorpus.zip', <http.client.HTTPMessage at 0x7feb7c9a71d0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "HjIoE_lfNojr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with zipfile.ZipFile('SentenceCorpus.zip', 'r') as z:\n",
        "  z.extractall('/content/drive/My Drive/NLP Assignment 2/Part3/')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AV_9ZLiIIoG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3.0.2 Extract the DataSet's vocabulary and save it locally to Drive"
      ]
    },
    {
      "metadata": {
        "id": "f0OxSO3jtyTO",
        "colab_type": "code",
        "outputId": "b4d72446-aa29-4192-dcd8-bddd22f28e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "cbf1d3d9-a240-4181-c43f-140f19c715c5",
        "id": "IlO_IMuvLYyU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "\t# split into tokens by white space\n",
        "  doc = doc.lower()\n",
        "  tokens = doc.split()\n",
        "  # remove punctuation from each token\n",
        "  table = str.maketrans('', '', punctuation)\n",
        "  tokens = [w.translate(table) for w in tokens]\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  # filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  tokens = [w for w in tokens if not w in extra_words_to_remove]\n",
        "  # filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "  # load doc\n",
        "  doc = load_doc(filename)\n",
        "  # clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "  # update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory  \n",
        "def process_docs(directory, vocab):\n",
        "  # walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "    # keep docs only drom the first reviewer\n",
        "    if filename.endswith('1.txt'):\n",
        "      # create the full path of the file to open\n",
        "      path = directory+filename\n",
        "      # add doc to vocab\n",
        "      add_doc_to_vocab(path,vocab)\n",
        "\n",
        "# Save list to file\n",
        "def save_list(lines, filename):\n",
        "  # convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "  # open file\n",
        "  file = open(filename, 'w')\n",
        "  # write text\n",
        "  file.write(data)\n",
        "  # close file\n",
        "  file.close()\n",
        "\n",
        "# Add all docs to vocab\n",
        "# Define vocabulary\n",
        "vocab = Counter()\n",
        "\n",
        "extra_words_to_remove = ['citation','number','symbol','misc',\n",
        "                         'aimx','ownx','cont','base','of','',\n",
        "                         'abstract','introduction']\n",
        "\n",
        "# Decompress and manage DataSet\n",
        "# Get training documents from respective directories\n",
        "process_docs(training_docs_directory, vocab)\n",
        "\n",
        "# The size of the vocab\n",
        "print(\"\\nVocabulary size:\", len(vocab))\n",
        "# Top words in the vocab\n",
        "print(\"\\nMost common words: \\n\", vocab.most_common(50))\n",
        "\n",
        "# Keep tokens with a min occurence\n",
        "min_occurence = 1\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurence]\n",
        "print(\"\\nUpdated Vocabulary size:\", len(tokens))\n",
        "\n",
        "# Save vocabulary as a list\n",
        "save_list(tokens, vocab_list_filename)\n",
        "\n",
        "# Save vocabulary as a Counter object (dictionary)\n",
        "import pickle\n",
        "with open(vocab_filename, 'wb') as outputfile:\n",
        "  pickle.dump(vocab, outputfile)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "Vocabulary size: 3968\n",
            "\n",
            "Most common words: \n",
            " [('model', 96), ('models', 58), ('one', 56), ('may', 50), ('many', 48), ('also', 48), ('based', 47), ('however', 46), ('proteins', 46), ('two', 45), ('data', 45), ('results', 43), ('learning', 41), ('loss', 38), ('participants', 38), ('selfcontrol', 38), ('used', 37), ('using', 36), ('behavior', 36), ('different', 35), ('new', 35), ('well', 35), ('thus', 34), ('stochastic', 34), ('section', 33), ('information', 32), ('studies', 32), ('ion', 32), ('general', 31), ('et', 31), ('al', 31), ('conflict', 31), ('formula', 31), ('neurons', 31), ('splicing', 31), ('expected', 30), ('use', 30), ('first', 30), ('choice', 30), ('study', 30), ('face', 30), ('network', 29), ('example', 29), ('task', 29), ('large', 28), ('analysis', 28), ('problem', 28), ('paper', 27), ('function', 27), ('individual', 27)]\n",
            "\n",
            "Updated Vocabulary size: 3968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HL2qvmrY3C-t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.0.3 Prepare clean data and save it locally to Drive"
      ]
    },
    {
      "metadata": {
        "id": "3PXmeO-6UM6x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.0.3.1 Libraries and Functions to use"
      ]
    },
    {
      "metadata": {
        "id": "p8hGhWCbtTTk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the data pre-processing I will follow different approach than the Part 2 (2.1.2) of the Assignment, with various predefined functions and hopefully the same results."
      ]
    },
    {
      "metadata": {
        "id": "hlkRADPsUL65",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from os import listdir\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import operator\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "\n",
        "\n",
        "classes = ['aimx','base','cont','misc','ownx']\n",
        "extra_words_to_remove = ['citation','number','symbol','of','','abstract','introduction']\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into list of clean tokens\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "  clean_lines = []\n",
        "  lines = doc.splitlines()\n",
        "  for line in lines:\n",
        "    line = line.lower()\n",
        "    # split into tokens by white space\n",
        "    tokens = line.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tokens = [w for w in tokens if not w in extra_words_to_remove]\n",
        "    # filter out tokens not in vocab\n",
        "    tokens = [w for w in tokens if w in list(vocab)+classes]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    clean_lines.append(tokens)\n",
        "  return clean_lines\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, reviewer):\n",
        "  lines = []\n",
        "  # walk through all files in the folder\n",
        "  filelist = sorted(listdir(directory))\n",
        "  for filename in filelist:\n",
        "    # keep docs only from a specific reviewer\n",
        "    if filename.endswith(reviewer):\n",
        "      # create the full path of the file to open\n",
        "      path = directory + filename\n",
        "      # load and clean the doc\n",
        "      doc = load_doc(path)\n",
        "      doc_lines = doc_to_clean_lines(doc, vocab)\n",
        "      doc_lines = [l for l in doc_lines if len(l) > 0]\n",
        "      doc_lines = [l for l in doc_lines if l[0] in classes]\n",
        "      # add lines to list\n",
        "      lines += doc_lines\n",
        "  return lines\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yE2oAw7nUuN_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.0.3.2 Get the training data and labels\n",
        "\n",
        "Each line of the documents is reviewed from 3 different reviewers. \n",
        "\n",
        "So for each line I find and kepp he predominant label."
      ]
    },
    {
      "metadata": {
        "id": "Vr1bapqMU1Tt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Corpus doc (X) and labels (y) from DataSet\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "# Get training documents from respective directories\n",
        "# Each element is a single line (list of clean tokens) of the training documents\n",
        "# Getting corpus docs (X) and labels from first reviewer's files\n",
        "training_docs = process_docs(training_docs_directory, vocab, '1.txt')\n",
        "y1 = []   # Labels from first reviewer\n",
        "for line in training_docs:\n",
        "  y1.append(line[0])\n",
        "  X.append(line[1:])\n",
        "\n",
        "# Labels from second reviewer\n",
        "y2 = []\n",
        "training_docs2 = process_docs(training_docs_directory, vocab, '2.txt')\n",
        "for line in training_docs2:\n",
        "  y2.append(line[0])\n",
        "# Labels from third reviewer\n",
        "y3 = []\n",
        "training_docs3 = process_docs(training_docs_directory, vocab, '3.txt')\n",
        "for line in training_docs3:\n",
        "  y3.append(line[0])\n",
        "\n",
        "# Get the predominant label for each doc\n",
        "for i in range(len(y1)):\n",
        "    dic = {'aimx': 0, 'base': 0, 'cont': 0, 'misc': 0, 'ownx': 0, }\n",
        "    dic[y1[i]] += 1\n",
        "    dic[y2[i]] += 1\n",
        "    dic[y3[i]] += 1\n",
        "    sorted_dic = sorted(dic.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    y.append(sorted_dic[0][0])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mAU3BkTTwug1",
        "colab_type": "code",
        "outputId": "410259e9-4d98-4edb-adfc-3aba868d036c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "y[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['misc',\n",
              " 'misc',\n",
              " 'misc',\n",
              " 'aimx',\n",
              " 'ownx',\n",
              " 'ownx',\n",
              " 'ownx',\n",
              " 'misc',\n",
              " 'cont',\n",
              " 'misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "_RXvSfJ8VEDx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.0.3.3 Handling categorical y values"
      ]
    },
    {
      "metadata": {
        "id": "CyOp47tSVDt9",
        "colab_type": "code",
        "outputId": "f86e0abb-12b8-405e-8c4a-114d3ae032dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "# First transform categorical values to integers\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "print('Original y labels: ', classes)\n",
        "print('Integer encoded y labels: ', le.transform(classes))\n",
        "\n",
        "# Then use One Hot encoding\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "y = y.reshape(len(y),1)\n",
        "y = ohe.fit_transform(y)\n",
        "# One could use only the initial Integer encoder and not the following One Hot.\n",
        "# Then during Network compilation should use loss='sparse_categorical_crossentropy'\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original y labels:  ['aimx', 'base', 'cont', 'misc', 'ownx']\n",
            "Integer encoded y labels:  [0 1 2 3 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "6800eb06-47e3-456a-fc69-5d60ff6f867b",
        "id": "exzXj2u5PdjN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "cell_type": "code",
      "source": [
        "y[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "LdM0kgfhX1nQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.0.3.4 Save X and y variables to Drive"
      ]
    },
    {
      "metadata": {
        "id": "WVQppX6jX1PF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use numpy function to store lists as np.array to Drive\n",
        "import numpy as np\n",
        "\n",
        "np.save(X_var_filename,np.array(X))\n",
        "np.save(y_var_filename,y)  # y already an np.array\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKiFrtTkeIwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lGnA6xESlIrA"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Use trained / pre-trained word embedding models as an Embedding() layer in Keras\n",
        "\n",
        "The aim of Part 3.1 of Assignement is to use trained/pre-trained word embedding models as an Embedding layer on a Neural Network Classifier (using Keras), in order to classify the documents of the sentence classification corpus.\n",
        "\n",
        "The Embedding layer will be the first layer of the Classifier. It is best understood as a dictionary that maps integer indices (which stand for specific words) to dense vectors. It takes integers as input, it looks up these integers in an internal dictionary, and it returns the associated vectors.\n",
        "\n",
        "The Embedding layer takes as input a 2D tensor of integers, of shape (*samples*, *sequence_length*), where each entry is a sequence of integers. All sequences in samples must have the same length, because they must be packed into a single tensor, so sequences that are shorter than others should be padded with zeros.\n",
        "\n",
        "The Embedding layer returns a 3D floating-point tensor, of shape (*samples*, *sequence_length*, *embedding_dimensionality*). Such a 3D tensor can then be used to train a single Dense layer on top for classification. In order to feed the Dense layer, the 3D tensor produced by Embedding layer must be transformed to a 2D tensor. I will use 2 different approaches:\n",
        "- A Flatten layer between Embedding and Dense layers, which flattens the 3D tensor into a 2D tensor, of shape (*samples*, *sequence_length* ** embedding_dimensionality*).\n",
        "- A Lambda layer between Embedding and Dense layers, which uses Keras built-in functions (such as sum() or mean()) to merge all vectors of a sample into a single vector of the same dimensionality. So Lambda layer turns the 3D tensor into a 2D tensor, of shape (*samples*, *embedding_dimensionality*)."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "8GbMv2fF0caY"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1.0 (Optional) Load stored data from Drive\n",
        "Follow procedure at Part 3.0 to prepare the data or just load the stored pre-processed data from Drive"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "0f7bec6c-eca9-4715-dab6-d2328ca3360a",
        "id": "Ritc57sr6wA0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Connect to personal Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_JZZ8h-e0car",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/vocab'\n",
        "X_var_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/X_var.npy'\n",
        "y_var_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/y_var.npy'\n",
        "\n",
        "# BEWARE: In this part vocabulary (variable vocab) must be a Counter\n",
        "# object, not just a list\n",
        "import pickle\n",
        "with open(vocab_filename, 'rb') as inputfile:\n",
        "  vocab = pickle.load(inputfile)\n",
        "\n",
        "import numpy as np\n",
        "# Load X variable\n",
        "X = list(np.load(X_var_filename))\n",
        "# Load y variable (encoded)\n",
        "y = np.load(y_var_filename)  # y is used as an np.array\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kqsfXWz7IvAc"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 Libraries, Functions and NN Classifiers' definitions\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qI4O-J3IIvAk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drive paths used to store data\n",
        "training_docs_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/SentenceCorpus/labeled_articles/'\n",
        "official_stopwords = '/content/drive/My Drive/NLP Assignment 2/Part3/SentenceCorpus/word_lists/stopwords.txt'\n",
        "vocab_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/vocab'\n",
        "vocab_list_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/vocab.txt'\n",
        "embedding_word2vec_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/embedding_word2vec.txt'\n",
        "X_var_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/X_var.npy'\n",
        "y_var_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/y_var.npy'\n",
        "\n",
        "w2v_40_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/40/'\n",
        "w2v_40_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/40/model.txt'\n",
        "\n",
        "w2v_75_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/75/'\n",
        "w2v_75_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/75/model.txt'\n",
        "\n",
        "w2v_82_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/82/'\n",
        "w2v_82_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/82/model.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "VnFoxXd2IvBD",
        "outputId": "b1d11bd7-be75-4e34-ef87-b84d5062d640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Libraries and Functions used for embeddings\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "\n",
        "# Load embedding as a dict\n",
        "def load_embedding(filename, vec_dim, codec='utf-8'):\n",
        "  # create a map of words to vectors\n",
        "  embeddings_index = dict()\n",
        "  # Load embedding into memory\n",
        "  f = open(filename,encoding=codec)\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    # The first word is the keyword\n",
        "    word = values[0]\n",
        "    # The last vec_dim words is the vector (dimension values) of keyword\n",
        "    coefs = asarray(values[-vec_dim:], dtype='float32')\n",
        "    # Key is string word, value is numpy array for vector\n",
        "    embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "  return(embeddings_index)\n",
        "\n",
        "# Create a weight matrix for words in training docs\n",
        "def get_weight_matrix(embedding, tokenizer, emb_dim):\n",
        "  # total vocabulary size plus 1 for unknown words\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  # Define weight matrix dimensions with all 0\n",
        "  embedding_matrix = zeros((vocab_size, emb_dim))\n",
        "  # Step vocab, store vectors using the Tokenizer's integer mapping\n",
        "  for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    # Procceed only if the word found in the embedding_index\n",
        "    # Words not found in the embedding index will be all zeros\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "  return(embedding_matrix)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "61wKVo6mnREw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Definition of various NN Classifiers used with embeddings\n",
        "\n",
        "# Flatten model\n",
        "# Using a Flatten layer before the Dense layer\n",
        "def flattenModel(embedding_layer,activation,losses,optimizer,epochs):\n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  print(\"\\nModel Summary:\\n\", model.summary())\n",
        "  \n",
        "  model.compile(loss=losses, optimizer=optimizer, metrics=['accuracy'])\n",
        "  \n",
        "  model.fit(X_train, y_train, epochs=epochs, verbose=2, validation_data=(X_test, y_test))\n",
        "  loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "  print('\\nTest Accuracy: %f' % (acc*100))\n",
        "\n",
        "\n",
        "# Average lambda model\n",
        "# Using a Lambda layer before the Dense layer to AVERAGE the embeddings over\n",
        "# the words of incoming docs  \n",
        "def averageModel(embedding_layer,activation,losses,optimizer,epochs):\n",
        "  import keras\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1)))\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  print(\"\\nModel Summary:\\n\", model.summary())\n",
        "  \n",
        "  model.compile(loss=losses, optimizer=optimizer, metrics=['accuracy'])\n",
        "  \n",
        "  model.fit(X_train, y_train, epochs=epochs, verbose=2, validation_data=(X_test, y_test))\n",
        "  loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "  print('\\nTest Accuracy: %f' % (acc*100))\n",
        "  \n",
        "  \n",
        "# Sum lambda model\n",
        "# Using a Lambda layer before the Dense layer to SUM the embeddings over the\n",
        "# words of incoming docs\n",
        "def sumModel(embedding_layer,activation,losses,optimizer,epochs):\n",
        "  import keras\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(embedding_layer)\n",
        "  model.add(keras.layers.Lambda(lambda x: keras.backend.sum(x, axis=1)))\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  print(\"\\nModel Summary:\\n\", model.summary())\n",
        "  \n",
        "  model.compile(loss=losses, optimizer=optimizer, metrics=['accuracy'])\n",
        "  \n",
        "  model.fit(X_train, y_train, epochs=epochs, verbose=2, validation_data=(X_test, y_test))\n",
        "  loss, acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "  print('\\nTest Accuracy: %f' % (acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4JxJmNP0Ki_R"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 Split data, encode and pad sequences"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7ZjLGEpCKi_f",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Fit the tokenizer on the training documents\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# Define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Encode sequences\n",
        "encoded_X = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s) for s in X])\n",
        "X_pad = pad_sequences(encoded_X, maxlen=max_length, padding='post')\n",
        "\n",
        "# Split training-testing data\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_pad,y,test_size=0.2,\n",
        "                                               stratify=y, random_state=42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Bq7pwb_3HLZ2"
      },
      "cell_type": "markdown",
      "source": [
        "###3.1.3 Using the trained word embedding model from Part 2"
      ]
    },
    {
      "metadata": {
        "id": "J99isbVnOoho",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.3.1 Download word embedding model\n",
        "Model is already saved on Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "UlPkvpaKHEeZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_word2vec_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/embedding_word2vec.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7mlSmvt5Hl-e"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.3.2 Prepare the embedding layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "v8a15PEwHl-3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vector dimensionality of downloaded embedding\n",
        "emb_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a5ae5a5f-62d1-43d7-cbda-792b5ded4d0c",
        "id": "rgAXu2EPHl_Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load embedding as a dict\n",
        "embeddings_index = load_embedding(embedding_word2vec_filename, emb_dim)\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3888 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8SB_9TV4Hl_6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = get_weight_matrix(embeddings_index, tokenizer, emb_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "enR635PtHmAL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the embedding layer using embedding_matrix as weights\n",
        "# The dimensionality of layer should be equal to that of downloaded embedding (output_dim = emb_dim)\n",
        "embedding_layer = Embedding(vocab_size, output_dim = emb_dim, weights=[embedding_matrix],\n",
        "                            input_length=max_length, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "J7D2f3krHmAg"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.3.3 Execute the 3 NN Classifiers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UnLCFsmBHmAm",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model's parameters\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "epochs = 15\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "f9ab451e-451f-4d82-fe61-bfa33e280ddb",
        "id": "N4IeeosoHmBB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the flatten model\n",
        "flattenModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 27,005\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 1.5989 - acc: 0.5524 - val_loss: 1.5863 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.5738 - acc: 0.6012 - val_loss: 1.5641 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.5500 - acc: 0.6024 - val_loss: 1.5422 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.5270 - acc: 0.6037 - val_loss: 1.5210 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.5046 - acc: 0.6037 - val_loss: 1.5011 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.4832 - acc: 0.6037 - val_loss: 1.4814 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.4623 - acc: 0.6037 - val_loss: 1.4629 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.4424 - acc: 0.6037 - val_loss: 1.4447 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.4231 - acc: 0.6037 - val_loss: 1.4272 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.4044 - acc: 0.6024 - val_loss: 1.4108 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.3867 - acc: 0.6024 - val_loss: 1.3945 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.3694 - acc: 0.6024 - val_loss: 1.3791 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.3528 - acc: 0.6024 - val_loss: 1.3643 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.3368 - acc: 0.6000 - val_loss: 1.3500 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.3213 - acc: 0.6012 - val_loss: 1.3363 - val_acc: 0.6000\n",
            "\n",
            "Test Accuracy: 60.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "23fe64cd-4204-42df-c0ce-bff348857572",
        "id": "qL_jXvtFHmBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the average model\n",
        "averageModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 392,605\n",
            "Trainable params: 505\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 1.5983 - acc: 0.5524 - val_loss: 1.5863 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.5756 - acc: 0.6000 - val_loss: 1.5640 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.5535 - acc: 0.6000 - val_loss: 1.5430 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.5325 - acc: 0.6000 - val_loss: 1.5224 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.5120 - acc: 0.6000 - val_loss: 1.5026 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.4925 - acc: 0.6000 - val_loss: 1.4832 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.4735 - acc: 0.6000 - val_loss: 1.4648 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.4554 - acc: 0.6000 - val_loss: 1.4470 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.4378 - acc: 0.6000 - val_loss: 1.4302 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.4211 - acc: 0.6000 - val_loss: 1.4136 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.4048 - acc: 0.6000 - val_loss: 1.3980 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.3892 - acc: 0.6000 - val_loss: 1.3830 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.3743 - acc: 0.6000 - val_loss: 1.3683 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.3597 - acc: 0.6000 - val_loss: 1.3543 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.3458 - acc: 0.6000 - val_loss: 1.3409 - val_acc: 0.6000\n",
            "\n",
            "Test Accuracy: 60.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "7bc7fa2b-75f9-44c0-e614-a735cfebb14e",
        "id": "H2icQTvnHmB-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the sum model\n",
        "sumModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "lambda_4 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 392,605\n",
            "Trainable params: 505\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 1.5977 - acc: 0.4073 - val_loss: 1.5861 - val_acc: 0.4976\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.5740 - acc: 0.5183 - val_loss: 1.5632 - val_acc: 0.5317\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.5510 - acc: 0.5537 - val_loss: 1.5412 - val_acc: 0.5610\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.5290 - acc: 0.5756 - val_loss: 1.5200 - val_acc: 0.5610\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.5077 - acc: 0.5927 - val_loss: 1.4996 - val_acc: 0.5902\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.4872 - acc: 0.5927 - val_loss: 1.4799 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.4675 - acc: 0.5939 - val_loss: 1.4608 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.4484 - acc: 0.6000 - val_loss: 1.4424 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.4302 - acc: 0.6000 - val_loss: 1.4249 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.4126 - acc: 0.6000 - val_loss: 1.4081 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.3959 - acc: 0.6000 - val_loss: 1.3921 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.3797 - acc: 0.6000 - val_loss: 1.3768 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.3643 - acc: 0.6000 - val_loss: 1.3616 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.3492 - acc: 0.6000 - val_loss: 1.3475 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.3349 - acc: 0.6000 - val_loss: 1.3340 - val_acc: 0.6000\n",
            "\n",
            "Test Accuracy: 60.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uwGiTZQL1_e6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.3.4 Conclusions\n",
        "\n",
        "All 3 classifiers perform exactly the same as expected since used embedding layer is already trained on our dataset"
      ]
    },
    {
      "metadata": {
        "id": "LV4v872hHEPZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZqB0jItRgms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###PROBLEM working with downloaded pre-trained word embeddings on Colab\n",
        "\n",
        "Whenever I try to access the model.txt file directly from a compressed downloaded w2v model, causes the Colab session to collapse, probably because of the size. That's not the case when I run the .ipynb notebook locally through Jupyter Lab.\n",
        "\n",
        "The only solution I found is to use this workaround:\n",
        "\n",
        "*   The first time to use the particular embedding:\n",
        "  *   Download the compressed file\n",
        "  *   Extract all the contents to a specific folder on Google Drive\n",
        "*  Then every time needed to use the embedding just used the model.txt file directly from Drive \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "BkmAeVMnsUY4"
      },
      "cell_type": "markdown",
      "source": [
        "###3.1.4 Using the English CoNLL17 corpus pre-trained word embedding model\n",
        "It was trained using Continuous Skip-gram algorithm with vector size 100, and window size 10"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "WcqUg8Z0sUZJ"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.4.1 Download and Extract embedding"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pPJUIO61sUZU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "273269a7-8924-4fb9-95c2-a105efaeff4f",
        "id": "2PZQjBWEtJdM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading CoNLL17 corpus ~1.5GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/40.zip\", filename=\"40.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('40.zip', <http.client.HTTPMessage at 0x7f1de90d10f0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IB0C6468sUaL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drive directory to store downloaded embedding\n",
        "w2v_40_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/40/'\n",
        "w2v_40_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/40/model.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5aAUXqgMsUad",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract downloaded embeddind to specific Drive folder\n",
        "with zipfile.ZipFile('40.zip', 'r') as z:\n",
        "  z.extractall(w2v_40_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "JXeArCOOsUaw"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.4.2 Prepare the embedding layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p2MOUXSDsUa2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vector dimensionality of downloaded embedding\n",
        "emb_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "85e06964-395c-4d74-a6a4-8f98eb3b4d83",
        "id": "bmZ6BcjvhrPK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load embedding as a dict\n",
        "embeddings_index = load_embedding(w2v_40_filename, emb_dim, codec='ISO-8859-1')\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 4023778 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ybTFbrREhrPr",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = get_weight_matrix(embeddings_index, tokenizer, emb_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P0Cio0yShrP8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the embedding layer using embedding_matrix as weights\n",
        "# The dimensionality of layer should be equal to that of downloaded embedding (output_dim = emb_dim)\n",
        "embedding_layer = Embedding(vocab_size, output_dim = emb_dim, weights=[embedding_matrix],\n",
        "                            input_length=max_length, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "liplBc2psUcS"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.4.3 Execute the 3 NN Classifiers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "a1vzWE4ssUcZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model's parameters\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "epochs = 15\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "5f864f8e-ac31-4b85-b280-b90c65d1388e",
        "id": "2FMzaZ8YsUcl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the flatten model\n",
        "flattenModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 27,005\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 1.1899 - acc: 0.5768 - val_loss: 1.0530 - val_acc: 0.5902\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.9566 - acc: 0.6305 - val_loss: 1.0097 - val_acc: 0.6098\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.8324 - acc: 0.6634 - val_loss: 1.0013 - val_acc: 0.6341\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.7560 - acc: 0.7049 - val_loss: 0.9777 - val_acc: 0.6537\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.6755 - acc: 0.7561 - val_loss: 0.9727 - val_acc: 0.6488\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.6156 - acc: 0.7780 - val_loss: 0.9647 - val_acc: 0.6585\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.5670 - acc: 0.8085 - val_loss: 0.9550 - val_acc: 0.6537\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.5226 - acc: 0.8366 - val_loss: 0.9531 - val_acc: 0.6537\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.4852 - acc: 0.8524 - val_loss: 0.9492 - val_acc: 0.6439\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.4530 - acc: 0.8756 - val_loss: 0.9594 - val_acc: 0.6390\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.4241 - acc: 0.8890 - val_loss: 0.9481 - val_acc: 0.6390\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.3978 - acc: 0.9000 - val_loss: 0.9517 - val_acc: 0.6341\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.3734 - acc: 0.9159 - val_loss: 0.9559 - val_acc: 0.6341\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.3542 - acc: 0.9232 - val_loss: 0.9568 - val_acc: 0.6439\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.3345 - acc: 0.9232 - val_loss: 0.9572 - val_acc: 0.6390\n",
            "\n",
            "Test Accuracy: 63.902439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "28775886-efe1-4ff1-9afa-fb7312662d09",
        "id": "S_wAaughsUdD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the average model\n",
        "averageModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 392,605\n",
            "Trainable params: 505\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 1.4891 - acc: 0.6000 - val_loss: 1.4511 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.4116 - acc: 0.6000 - val_loss: 1.3815 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.3488 - acc: 0.6000 - val_loss: 1.3256 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.2992 - acc: 0.6000 - val_loss: 1.2817 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.2613 - acc: 0.6000 - val_loss: 1.2455 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.2301 - acc: 0.6000 - val_loss: 1.2188 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.2065 - acc: 0.6000 - val_loss: 1.1965 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.1879 - acc: 0.6000 - val_loss: 1.1784 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.1733 - acc: 0.6000 - val_loss: 1.1640 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.1618 - acc: 0.6000 - val_loss: 1.1522 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.1521 - acc: 0.6000 - val_loss: 1.1427 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.1441 - acc: 0.6000 - val_loss: 1.1349 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.1375 - acc: 0.6000 - val_loss: 1.1283 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.1321 - acc: 0.6000 - val_loss: 1.1224 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.1272 - acc: 0.6000 - val_loss: 1.1172 - val_acc: 0.6000\n",
            "\n",
            "Test Accuracy: 60.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "8eeafd19-cd81-4244-eec0-6d68ba066627",
        "id": "lhDsDD3_sUdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the sum model\n",
        "sumModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "lambda_4 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 392,605\n",
            "Trainable params: 505\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 2.7012 - acc: 0.3024 - val_loss: 1.6802 - val_acc: 0.5220\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.7590 - acc: 0.5244 - val_loss: 1.5989 - val_acc: 0.5317\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.6515 - acc: 0.5000 - val_loss: 1.5367 - val_acc: 0.5366\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.5765 - acc: 0.5146 - val_loss: 1.4791 - val_acc: 0.5268\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.5055 - acc: 0.5280 - val_loss: 1.4369 - val_acc: 0.5415\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.4404 - acc: 0.5256 - val_loss: 1.3887 - val_acc: 0.5561\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.3890 - acc: 0.5402 - val_loss: 1.3659 - val_acc: 0.5415\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.3493 - acc: 0.5317 - val_loss: 1.3285 - val_acc: 0.5561\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.3164 - acc: 0.5646 - val_loss: 1.3141 - val_acc: 0.5317\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.2854 - acc: 0.5476 - val_loss: 1.2868 - val_acc: 0.5707\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.2516 - acc: 0.5732 - val_loss: 1.2674 - val_acc: 0.5610\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.2216 - acc: 0.5598 - val_loss: 1.2471 - val_acc: 0.5707\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.2085 - acc: 0.5780 - val_loss: 1.2254 - val_acc: 0.5659\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.1804 - acc: 0.5805 - val_loss: 1.2100 - val_acc: 0.5463\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.1622 - acc: 0.5805 - val_loss: 1.1972 - val_acc: 0.5512\n",
            "\n",
            "Test Accuracy: 55.121951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-O3IqUrZIfMp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3.1.5 Using the English Oil and Gas corpus pre-trained word embedding model\n",
        "It was trained using Continuous Bag-of-Words algorithm with vector size 400, and window size 5"
      ]
    },
    {
      "metadata": {
        "id": "GccB6cCOLia3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.5.1 Download and Extract embedding"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xqM8o7OWLwfg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "471ee169-3148-4b56-f6da-0b3085edd08c",
        "id": "ioOBpKP1MYtF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading Oil and Gas corpus ~0.4GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/75.zip\", filename=\"75.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('75.zip', <http.client.HTTPMessage at 0x7f05cb372160>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "aL8tXwJnQhAt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drive directory to store downloaded embedding\n",
        "w2v_75_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/75/'\n",
        "w2v_75_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/75/model.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6M9AmbhNM44b",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract downloaded embeddind to specific Drive folder\n",
        "with zipfile.ZipFile('75.zip', 'r') as z:\n",
        "  z.extractall(w2v_75_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pxQDjHReSdvn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.5.2 Prepare the embedding layer"
      ]
    },
    {
      "metadata": {
        "id": "ySGa47HvZHBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vector dimensionality of downloaded embedding\n",
        "emb_dim = 400"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uGwWLDrrSVZy",
        "colab_type": "code",
        "outputId": "12fd2427-fa38-4628-9c2b-a9db424e3c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load embedding as a dict\n",
        "embeddings_index = load_embedding(w2v_75_filename, emb_dim)\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 285056 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cW7t83-4Zkjr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = get_weight_matrix(embeddings_index, tokenizer, emb_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rnxBwCPUa_tY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the embedding layer using embedding_matrix as weights\n",
        "# The dimensionality of layer should be equal to that of downloaded embedding (output_dim = emb_dim)\n",
        "embedding_layer = Embedding(vocab_size, output_dim = emb_dim, weights=[embedding_matrix],\n",
        "                            input_length=max_length, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xh_DRWSIa6VO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.5.3 Execute the 3 NN Classifiers"
      ]
    },
    {
      "metadata": {
        "id": "YX-V3apfpAOv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model's parameters\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "epochs = 15\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9ABfvZpvepbk",
        "colab_type": "code",
        "outputId": "b252d557-6dc0-4c36-e969-c632440f1384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the flatten model\n",
        "flattenModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 400)           1568400   \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 21600)             0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 5)                 108005    \n",
            "=================================================================\n",
            "Total params: 1,676,405\n",
            "Trainable params: 108,005\n",
            "Non-trainable params: 1,568,400\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 1.2447 - acc: 0.5256 - val_loss: 1.1632 - val_acc: 0.5756\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.4728 - acc: 0.8598 - val_loss: 1.1787 - val_acc: 0.6390\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.2809 - acc: 0.9427 - val_loss: 1.1853 - val_acc: 0.6293\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.1972 - acc: 0.9768 - val_loss: 1.2477 - val_acc: 0.6537\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.1517 - acc: 0.9829 - val_loss: 1.2749 - val_acc: 0.6439\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.1224 - acc: 0.9878 - val_loss: 1.3069 - val_acc: 0.6341\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.1016 - acc: 0.9963 - val_loss: 1.3411 - val_acc: 0.6293\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.0868 - acc: 0.9976 - val_loss: 1.3475 - val_acc: 0.6293\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0747 - acc: 0.9988 - val_loss: 1.3785 - val_acc: 0.6293\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0655 - acc: 0.9988 - val_loss: 1.4036 - val_acc: 0.6293\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0577 - acc: 0.9988 - val_loss: 1.4159 - val_acc: 0.6098\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0515 - acc: 0.9988 - val_loss: 1.4434 - val_acc: 0.6098\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0464 - acc: 0.9988 - val_loss: 1.4595 - val_acc: 0.6146\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0422 - acc: 0.9988 - val_loss: 1.4838 - val_acc: 0.6098\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0384 - acc: 1.0000 - val_loss: 1.4995 - val_acc: 0.6098\n",
            "\n",
            "Test Accuracy: 60.975610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b660a35b-7c9a-4b46-9f5d-c0f74c39c15d",
        "id": "ifPSbGnNglx8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the average model\n",
        "averageModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 400)           1568400   \n",
            "_________________________________________________________________\n",
            "lambda_6 (Lambda)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 5)                 2005      \n",
            "=================================================================\n",
            "Total params: 1,570,405\n",
            "Trainable params: 2,005\n",
            "Non-trainable params: 1,568,400\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 1.5394 - acc: 0.4549 - val_loss: 1.4654 - val_acc: 0.5854\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.3980 - acc: 0.5915 - val_loss: 1.3578 - val_acc: 0.6049\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.3076 - acc: 0.6000 - val_loss: 1.2933 - val_acc: 0.5951\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.2512 - acc: 0.6012 - val_loss: 1.2538 - val_acc: 0.5951\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.2159 - acc: 0.6024 - val_loss: 1.2270 - val_acc: 0.5951\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.1898 - acc: 0.6024 - val_loss: 1.2072 - val_acc: 0.5902\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.1703 - acc: 0.6061 - val_loss: 1.1918 - val_acc: 0.5902\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.1534 - acc: 0.6061 - val_loss: 1.1786 - val_acc: 0.5902\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.1392 - acc: 0.6073 - val_loss: 1.1672 - val_acc: 0.5902\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.1270 - acc: 0.6110 - val_loss: 1.1573 - val_acc: 0.5902\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.1150 - acc: 0.6098 - val_loss: 1.1479 - val_acc: 0.5854\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.1046 - acc: 0.6134 - val_loss: 1.1396 - val_acc: 0.5854\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.0946 - acc: 0.6134 - val_loss: 1.1326 - val_acc: 0.5854\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.0848 - acc: 0.6134 - val_loss: 1.1245 - val_acc: 0.5854\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.0758 - acc: 0.6122 - val_loss: 1.1177 - val_acc: 0.5854\n",
            "\n",
            "Test Accuracy: 58.536585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "5ba786f7-508a-451b-8552-d10d26859822",
        "id": "_7x__RurhMnv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the sum model\n",
        "sumModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 400)           1568400   \n",
            "_________________________________________________________________\n",
            "lambda_10 (Lambda)           (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 5)                 2005      \n",
            "=================================================================\n",
            "Total params: 1,570,405\n",
            "Trainable params: 2,005\n",
            "Non-trainable params: 1,568,400\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 2.9736 - acc: 0.3646 - val_loss: 2.6568 - val_acc: 0.4293\n",
            "Epoch 2/15\n",
            " - 0s - loss: 2.1001 - acc: 0.4683 - val_loss: 2.3317 - val_acc: 0.4293\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.7266 - acc: 0.5122 - val_loss: 2.1188 - val_acc: 0.4732\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.4669 - acc: 0.5707 - val_loss: 2.0609 - val_acc: 0.4780\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.3399 - acc: 0.5878 - val_loss: 1.9084 - val_acc: 0.4878\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.1740 - acc: 0.6256 - val_loss: 1.8340 - val_acc: 0.4878\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.0680 - acc: 0.6439 - val_loss: 1.8119 - val_acc: 0.4683\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.9842 - acc: 0.6598 - val_loss: 1.7457 - val_acc: 0.4976\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.9054 - acc: 0.6744 - val_loss: 1.7145 - val_acc: 0.5024\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.8563 - acc: 0.6829 - val_loss: 1.6827 - val_acc: 0.5122\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.8050 - acc: 0.6976 - val_loss: 1.6474 - val_acc: 0.5415\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.7589 - acc: 0.7110 - val_loss: 1.6263 - val_acc: 0.5317\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.7091 - acc: 0.7293 - val_loss: 1.6223 - val_acc: 0.5366\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.6861 - acc: 0.7463 - val_loss: 1.6095 - val_acc: 0.5463\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.6583 - acc: 0.7341 - val_loss: 1.5948 - val_acc: 0.5317\n",
            "\n",
            "Test Accuracy: 53.170732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dqKyDEo5SU-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ck2Jb1NW1yWP"
      },
      "cell_type": "markdown",
      "source": [
        "###3.1.6 Using the English Common Crawl corpus pre-trained word embedding model\n",
        "It was trained using GloVe algorithm with vector size 300, and window size 10"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "VqczRFtG1yWp"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.6.1 Download and Extract embedding"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jSj7lEgU1yW3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import zipfile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "447302ac-62db-4ac9-ab82-8d7a5df03c8d",
        "id": "ftT-YKoI2baP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading Oil and Gas corpus ~2.3GB\n",
        "urllib.request.urlretrieve(\"http://vectors.nlpl.eu/repository/11/82.zip\", filename=\"82.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('82.zip', <http.client.HTTPMessage at 0x7ff43e7c1390>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "r--BqPQB1yX2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Drive directory to store downloaded embedding\n",
        "w2v_82_directory = '/content/drive/My Drive/NLP Assignment 2/Part3/82/'\n",
        "w2v_82_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/82/model.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ihdQFdgS1yYJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract downloaded embeddind to specific Drive folder\n",
        "with zipfile.ZipFile('82.zip', 'r') as z:\n",
        "  z.extractall(w2v_82_directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "b7pNRMV11yYZ"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.6.2 Prepare the embedding layer"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TcYh_sqT1yYe",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vector dimensionality of downloaded embedding\n",
        "emb_dim = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "44645046-e860-4e6a-a59e-c9c5c37e5703",
        "id": "nWw8uQ5gr7hu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load embedding as a dict\n",
        "embeddings_index = load_embedding(w2v_82_filename, emb_dim)\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 1998680 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZB5-DGNMr7ic",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = get_weight_matrix(embeddings_index, tokenizer, emb_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AUejA1EGr7iv",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the embedding layer using embedding_matrix as weights\n",
        "# The dimensionality of layer should be equal to that of downloaded embedding (output_dim = emb_dim)\n",
        "embedding_layer = Embedding(vocab_size, output_dim = emb_dim, weights=[embedding_matrix],\n",
        "                            input_length=max_length, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "3DT7_HugvHHi"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.6.3 Execute the 3 NN Classifiers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "vTvrv3GOu9XE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model's parameters\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "epochs = 15\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "38891b90-e5c8-42a5-b748-930880d1848a",
        "id": "Vk7wllHsu9Xz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the flatten model\n",
        "flattenModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 54, 300)           1176300   \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 16200)             0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 81005     \n",
            "=================================================================\n",
            "Total params: 1,257,305\n",
            "Trainable params: 81,005\n",
            "Non-trainable params: 1,176,300\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 1.2195 - acc: 0.5463 - val_loss: 1.0536 - val_acc: 0.6146\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.6691 - acc: 0.7573 - val_loss: 0.9702 - val_acc: 0.6634\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.4512 - acc: 0.8854 - val_loss: 0.9618 - val_acc: 0.6634\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.3356 - acc: 0.9427 - val_loss: 0.9652 - val_acc: 0.6634\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.2620 - acc: 0.9634 - val_loss: 0.9583 - val_acc: 0.6585\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.2110 - acc: 0.9780 - val_loss: 0.9813 - val_acc: 0.6390\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.1749 - acc: 0.9915 - val_loss: 0.9833 - val_acc: 0.6439\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.1468 - acc: 0.9951 - val_loss: 0.9969 - val_acc: 0.6488\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.1259 - acc: 0.9988 - val_loss: 1.0036 - val_acc: 0.6439\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.1090 - acc: 0.9988 - val_loss: 1.0182 - val_acc: 0.6390\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0959 - acc: 0.9988 - val_loss: 1.0344 - val_acc: 0.6390\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0847 - acc: 0.9988 - val_loss: 1.0456 - val_acc: 0.6341\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0754 - acc: 1.0000 - val_loss: 1.0558 - val_acc: 0.6390\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 1.0678 - val_acc: 0.6390\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0613 - acc: 1.0000 - val_loss: 1.0768 - val_acc: 0.6341\n",
            "\n",
            "Test Accuracy: 63.414634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "1e1dd018-1b64-4d88-f76b-a3822503434a",
        "id": "BjsJ3CC-u9Yd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the average model\n",
        "averageModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 54, 300)           1176300   \n",
            "_________________________________________________________________\n",
            "lambda_5 (Lambda)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 1505      \n",
            "=================================================================\n",
            "Total params: 1,177,805\n",
            "Trainable params: 1,505\n",
            "Non-trainable params: 1,176,300\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 1.5816 - acc: 0.2476 - val_loss: 1.5281 - val_acc: 0.4049\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.4764 - acc: 0.5195 - val_loss: 1.4381 - val_acc: 0.5854\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.3941 - acc: 0.6012 - val_loss: 1.3668 - val_acc: 0.5951\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.3275 - acc: 0.6012 - val_loss: 1.3132 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.2775 - acc: 0.6000 - val_loss: 1.2701 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.2388 - acc: 0.6000 - val_loss: 1.2355 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.2088 - acc: 0.6000 - val_loss: 1.2092 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.1842 - acc: 0.6000 - val_loss: 1.1888 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.1650 - acc: 0.6000 - val_loss: 1.1714 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.1491 - acc: 0.6000 - val_loss: 1.1570 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.1360 - acc: 0.6000 - val_loss: 1.1444 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.1244 - acc: 0.6000 - val_loss: 1.1336 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.1147 - acc: 0.6000 - val_loss: 1.1242 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.1056 - acc: 0.6000 - val_loss: 1.1159 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.0980 - acc: 0.6012 - val_loss: 1.1089 - val_acc: 0.6049\n",
            "\n",
            "Test Accuracy: 60.487805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "382f1803-8d62-444d-8569-fdc905d5bcf5",
        "id": "pGWZ2vftu9Y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the sum model\n",
        "sumModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 54, 300)           1176300   \n",
            "_________________________________________________________________\n",
            "lambda_6 (Lambda)            (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 5)                 1505      \n",
            "=================================================================\n",
            "Total params: 1,177,805\n",
            "Trainable params: 1,505\n",
            "Non-trainable params: 1,176,300\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 1s - loss: 6.1789 - acc: 0.1561 - val_loss: 3.2226 - val_acc: 0.1902\n",
            "Epoch 2/15\n",
            " - 0s - loss: 2.2679 - acc: 0.4183 - val_loss: 2.1366 - val_acc: 0.4537\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.8081 - acc: 0.4841 - val_loss: 1.8247 - val_acc: 0.4390\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.5555 - acc: 0.5000 - val_loss: 1.6348 - val_acc: 0.4634\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.3707 - acc: 0.5280 - val_loss: 1.4991 - val_acc: 0.4976\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.2503 - acc: 0.5561 - val_loss: 1.4463 - val_acc: 0.4976\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.1324 - acc: 0.5695 - val_loss: 1.3688 - val_acc: 0.5220\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.0569 - acc: 0.6037 - val_loss: 1.3214 - val_acc: 0.5512\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.9957 - acc: 0.6341 - val_loss: 1.2818 - val_acc: 0.5463\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.9428 - acc: 0.6354 - val_loss: 1.2638 - val_acc: 0.5561\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.9051 - acc: 0.6500 - val_loss: 1.2676 - val_acc: 0.5415\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.8554 - acc: 0.6695 - val_loss: 1.2194 - val_acc: 0.5756\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.8196 - acc: 0.6890 - val_loss: 1.2159 - val_acc: 0.5805\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.7873 - acc: 0.6976 - val_loss: 1.1923 - val_acc: 0.5902\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.7725 - acc: 0.7037 - val_loss: 1.1799 - val_acc: 0.5805\n",
            "\n",
            "Test Accuracy: 58.048781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QsP4ClCODkK6"
      },
      "cell_type": "markdown",
      "source": [
        "###3.1.7 (OPTIONAL) Using the Wikipedia 2014 + Gigaword 5 corpus pre-trained word embedding model\n",
        "It was trained using GloVe algorithm with vector size 100 (glove.6b.100d.txt)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NqT9lXQIU11P"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.7.1 Download and Extract embedding"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d013ee7e-d31f-4926-dff7-b599c792098f",
        "id": "T5n2ccBN-4U9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading 6B ~0.8GB\n",
        "urllib.request.urlretrieve(\"https://nlp.stanford.edu/data/glove.6B.zip\", filename=\"6B.zip\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('6B.zip', <http.client.HTTPMessage at 0x7f1df0cfa9b0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "I_Drvi0F_uAF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "directory_6B = '/content/drive/My Drive/NLP Assignment 2/Part3/6B/'\n",
        "filename_6B = '/content/drive/My Drive/NLP Assignment 2/Part3/6B/glove.6B.100d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z8-bp8xf_tn5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract downloaded embeddind to specific Drive folder\n",
        "with zipfile.ZipFile('6B.zip', 'r') as z:\n",
        "  z.extractall(directory_6B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AUHRYnxmHev6"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.7.2 Prepare the embedding layer"
      ]
    },
    {
      "metadata": {
        "id": "yDSzRBxPAcv6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Vector dimensionality of downloaded embedding\n",
        "emb_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fH3Bi6fkAtVx",
        "colab_type": "code",
        "outputId": "91317213-ea2e-4d10-ae03-224faa151dae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Load embedding as a dict\n",
        "embeddings_index = load_embedding(filename_6B, emb_dim)\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-vM_08t8BzCD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a weight matrix for words in training docs\n",
        "embedding_matrix = get_weight_matrix(embeddings_index, tokenizer, emb_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AoT5ZOvnBmOY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create the embedding layer using embedding_matrix as weights\n",
        "# The dimensionality of layer should be equal to that of downloaded embedding (output_dim = emb_dim)\n",
        "embedding_layer = Embedding(vocab_size, output_dim = emb_dim, weights=[embedding_matrix],\n",
        "                            input_length=max_length, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "52G8d6H2BmPM"
      },
      "cell_type": "markdown",
      "source": [
        "####3.1.7.3 Execute the 3 NN Classifiers"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IHlDtLHMBmPV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model's parameters\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "epochs = 15\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "02199875-e262-43d4-92b7-a3fd72b5f869",
        "id": "o-8PZql7BmP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the flatten model\n",
        "flattenModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 27,005\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 1.2006 - acc: 0.5634 - val_loss: 1.0756 - val_acc: 0.5805\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.8831 - acc: 0.6537 - val_loss: 1.0439 - val_acc: 0.5659\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.7356 - acc: 0.7305 - val_loss: 1.0124 - val_acc: 0.5951\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.6206 - acc: 0.7866 - val_loss: 1.0049 - val_acc: 0.6146\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.5359 - acc: 0.8293 - val_loss: 1.0095 - val_acc: 0.6293\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.4704 - acc: 0.8622 - val_loss: 1.0106 - val_acc: 0.6244\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.4196 - acc: 0.8890 - val_loss: 1.0117 - val_acc: 0.6244\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.3773 - acc: 0.9085 - val_loss: 1.0130 - val_acc: 0.6146\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.3416 - acc: 0.9256 - val_loss: 1.0303 - val_acc: 0.6293\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.3120 - acc: 0.9354 - val_loss: 1.0257 - val_acc: 0.6195\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.2883 - acc: 0.9524 - val_loss: 1.0421 - val_acc: 0.6244\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.2647 - acc: 0.9537 - val_loss: 1.0516 - val_acc: 0.6244\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.2468 - acc: 0.9561 - val_loss: 1.0527 - val_acc: 0.6146\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.2295 - acc: 0.9646 - val_loss: 1.0712 - val_acc: 0.6244\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.2148 - acc: 0.9768 - val_loss: 1.0911 - val_acc: 0.6244\n",
            "\n",
            "Test Accuracy: 62.439025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "2ed0a345-d79a-4762-a6cf-8a50f57ca8ca",
        "id": "KlRCgJxQBmQY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the average model\n",
        "averageModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 392,605\n",
            "Trainable params: 505\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 1.5148 - acc: 0.5439 - val_loss: 1.4692 - val_acc: 0.5902\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.4264 - acc: 0.5902 - val_loss: 1.3918 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.3563 - acc: 0.5988 - val_loss: 1.3291 - val_acc: 0.6049\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.3021 - acc: 0.5988 - val_loss: 1.2800 - val_acc: 0.6049\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.2597 - acc: 0.5988 - val_loss: 1.2430 - val_acc: 0.6049\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.2268 - acc: 0.5988 - val_loss: 1.2145 - val_acc: 0.6049\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.2023 - acc: 0.5988 - val_loss: 1.1915 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.1831 - acc: 0.5988 - val_loss: 1.1733 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.1680 - acc: 0.5988 - val_loss: 1.1588 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.1563 - acc: 0.6000 - val_loss: 1.1468 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.1464 - acc: 0.6000 - val_loss: 1.1383 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.1388 - acc: 0.6000 - val_loss: 1.1301 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.1319 - acc: 0.6000 - val_loss: 1.1230 - val_acc: 0.6049\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.1260 - acc: 0.6000 - val_loss: 1.1171 - val_acc: 0.6049\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.1209 - acc: 0.6000 - val_loss: 1.1119 - val_acc: 0.6049\n",
            "\n",
            "Test Accuracy: 60.487805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "da28b702-7b2e-4031-9c68-b7008527b305",
        "id": "49wIiTekBmQ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        }
      },
      "cell_type": "code",
      "source": [
        "# Execute the sum model\n",
        "sumModel(embedding_layer,activation,losses,optimizer,epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 505       \n",
            "=================================================================\n",
            "Total params: 392,605\n",
            "Trainable params: 505\n",
            "Non-trainable params: 392,100\n",
            "_________________________________________________________________\n",
            "\n",
            "Model Summary:\n",
            " None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 0s - loss: 2.7684 - acc: 0.4463 - val_loss: 2.3073 - val_acc: 0.4878\n",
            "Epoch 2/15\n",
            " - 0s - loss: 2.3714 - acc: 0.4524 - val_loss: 2.0552 - val_acc: 0.4732\n",
            "Epoch 3/15\n",
            " - 0s - loss: 2.1695 - acc: 0.4573 - val_loss: 1.9055 - val_acc: 0.4780\n",
            "Epoch 4/15\n",
            " - 0s - loss: 2.0062 - acc: 0.4744 - val_loss: 1.8006 - val_acc: 0.5268\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.8717 - acc: 0.4744 - val_loss: 1.7155 - val_acc: 0.5122\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.7686 - acc: 0.4854 - val_loss: 1.6465 - val_acc: 0.4780\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.6733 - acc: 0.4854 - val_loss: 1.5795 - val_acc: 0.5220\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.5917 - acc: 0.4756 - val_loss: 1.5344 - val_acc: 0.5512\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.5261 - acc: 0.4878 - val_loss: 1.4807 - val_acc: 0.5268\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.4686 - acc: 0.5012 - val_loss: 1.4447 - val_acc: 0.5317\n",
            "Epoch 11/15\n",
            " - 0s - loss: 1.4285 - acc: 0.4927 - val_loss: 1.4059 - val_acc: 0.5415\n",
            "Epoch 12/15\n",
            " - 0s - loss: 1.3715 - acc: 0.5098 - val_loss: 1.3700 - val_acc: 0.5366\n",
            "Epoch 13/15\n",
            " - 0s - loss: 1.3133 - acc: 0.5159 - val_loss: 1.3366 - val_acc: 0.5415\n",
            "Epoch 14/15\n",
            " - 0s - loss: 1.2821 - acc: 0.5159 - val_loss: 1.3148 - val_acc: 0.5463\n",
            "Epoch 15/15\n",
            " - 0s - loss: 1.2415 - acc: 0.5232 - val_loss: 1.2843 - val_acc: 0.5415\n",
            "\n",
            "Test Accuracy: 54.146342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o8KzvKoxD8AT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3.1.8 Conclusions\n",
        "All 4+1 trained/pre-trained word embedding models used perform equally well with just minor deviations on reported Accuracy. This means that on Part 2 we had enough data to train well our model on given dataset.\n",
        "\n",
        "Among the different NN classifiers used I noticed only minor differentiation on performance:\n",
        "- Using Flatten layer performs slightly better than using Average layer and this in its turn is slightly better than using Sum layer"
      ]
    },
    {
      "metadata": {
        "id": "Not2mUxzD7jh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7pBuTo06AcGz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zs_nM3XzIiV9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Train an Embedding Layer on the DataSet\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D0ie8VHr53sT"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2.0 Prepare the data\n",
        "\n",
        "If not already done, use procedures at 3.0 to prepare the data for this part"
      ]
    },
    {
      "metadata": {
        "id": "G-G9i4ZrJi8C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2.0.1 (Optional) Load stored data from Drive\n",
        "\n",
        "But you still gonna need the **le (LabelEncoder)** variable from previous part\n",
        "\n",
        "So use it just to refresh your data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "wcNXnLgGLovy",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Recover vocabulary (as a list) from local file\n",
        "# with open(vocab_list_filename, \"r\") as f:\n",
        "#   vocab = f.read().split('\\n')\n",
        "import pickle\n",
        "with open(vocab_filename, 'rb') as inputfile:\n",
        "  vocab = list(pickle.load(inputfile))\n",
        "\n",
        "import numpy as np\n",
        "# Load X variable\n",
        "X = list(np.load(X_var_filename))\n",
        "# Load y variable (encoded)\n",
        "y = np.load(y_var_filename)  # y is used as an np.array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "QOThcCeXju7D"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 Training the Embedding Layer"
      ]
    },
    {
      "metadata": {
        "id": "pl_8E3iOlTJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1.1 Split data, encode and pad sequences"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "enqN8MxJIgEL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split training-testing data\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,\n",
        "                                               stratify=y, random_state=0)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# encode sequences\n",
        "encoded_X_train = tokenizer.texts_to_sequences(X_train)\n",
        "encoded_X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s) for s in X])\n",
        "X_train_pad = pad_sequences(encoded_X_train, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(encoded_X_test, maxlen=max_length, padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pbOvNvr3Q83",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.1.2 Define and train the model\n",
        "\n",
        "I define the model inside a function in order to be able to call it on the next part of the Assignment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "a68f22ef-e755-47e2-c4c9-1b438a74a68e",
        "id": "wHpjPLmM6pAy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Model's parameters\n",
        "output_dim = 100\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "batch = 64\n",
        "\n",
        "parameters = [output_dim, activation, losses, optimizer]\n",
        "\n",
        "def annDef(param):\n",
        "  # Define Model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=output_dim, input_length=max_length))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(5, activation=activation))\n",
        "  print(model.summary())\n",
        "\n",
        "  # Compile Network\n",
        "  model.compile(loss=losses, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  # Fit Network to training data\n",
        "  model.fit(X_train_pad,y_train,epochs=15,validation_data=\n",
        "            (X_test_pad, y_test))\n",
        "  \n",
        "  # Evaluate Network during Training\n",
        "  loss, acc = model.evaluate(X_test_pad, y_test, verbose=2)\n",
        "  print('Test Accuracy: %f' % (acc*100))\n",
        "  \n",
        "  return model\n",
        "  \n",
        "\n",
        "model = annDef(parameters)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            "820/820 [==============================] - 1s 921us/step - loss: 1.1683 - acc: 0.5878 - val_loss: 1.0632 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            "820/820 [==============================] - 0s 372us/step - loss: 0.9870 - acc: 0.6012 - val_loss: 1.0405 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            "820/820 [==============================] - 0s 352us/step - loss: 0.8663 - acc: 0.6305 - val_loss: 1.0208 - val_acc: 0.6098\n",
            "Epoch 4/15\n",
            "820/820 [==============================] - 0s 329us/step - loss: 0.6952 - acc: 0.8000 - val_loss: 0.9890 - val_acc: 0.6195\n",
            "Epoch 5/15\n",
            "820/820 [==============================] - 0s 345us/step - loss: 0.4939 - acc: 0.8951 - val_loss: 0.9564 - val_acc: 0.6341\n",
            "Epoch 6/15\n",
            "820/820 [==============================] - 0s 316us/step - loss: 0.3260 - acc: 0.9293 - val_loss: 0.9338 - val_acc: 0.6488\n",
            "Epoch 7/15\n",
            "820/820 [==============================] - 0s 308us/step - loss: 0.2108 - acc: 0.9683 - val_loss: 0.9132 - val_acc: 0.6439\n",
            "Epoch 8/15\n",
            "820/820 [==============================] - 0s 313us/step - loss: 0.1365 - acc: 0.9841 - val_loss: 0.9062 - val_acc: 0.6634\n",
            "Epoch 9/15\n",
            "820/820 [==============================] - 0s 304us/step - loss: 0.0900 - acc: 0.9963 - val_loss: 0.8990 - val_acc: 0.6585\n",
            "Epoch 10/15\n",
            "820/820 [==============================] - 0s 317us/step - loss: 0.0620 - acc: 0.9988 - val_loss: 0.8966 - val_acc: 0.6585\n",
            "Epoch 11/15\n",
            "820/820 [==============================] - 0s 304us/step - loss: 0.0447 - acc: 0.9988 - val_loss: 0.8953 - val_acc: 0.6585\n",
            "Epoch 12/15\n",
            "820/820 [==============================] - 0s 295us/step - loss: 0.0338 - acc: 0.9988 - val_loss: 0.8960 - val_acc: 0.6537\n",
            "Epoch 13/15\n",
            "820/820 [==============================] - 0s 299us/step - loss: 0.0263 - acc: 0.9988 - val_loss: 0.8968 - val_acc: 0.6537\n",
            "Epoch 14/15\n",
            "820/820 [==============================] - 0s 317us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.8992 - val_acc: 0.6537\n",
            "Epoch 15/15\n",
            "820/820 [==============================] - 0s 304us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.9042 - val_acc: 0.6537\n",
            "Test Accuracy: 65.365854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2RQv6CNgZwBN",
        "colab_type": "code",
        "outputId": "0f83f38c-59c9-4d4a-9d42-a2aedd3e3240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test_pad, y_test, verbose=2)\n",
        "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.904, accuracy: 0.654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BTQIKu_BqxFp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 Model evaluation"
      ]
    },
    {
      "metadata": {
        "id": "oIC1NpqPrQdm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2.1 Evaluation metrics\n",
        "\n",
        "I will use the well known metrics from sklearn library"
      ]
    },
    {
      "metadata": {
        "id": "QtkACVCRkw5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "goMFQhr9rkfl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2.2 Prediction on the test set\n",
        "\n",
        "Use trained model to extract the predictions for the test set of data"
      ]
    },
    {
      "metadata": {
        "id": "OBWKxT9lkwr2",
        "colab_type": "code",
        "outputId": "86fd35ea-a206-4c0f-ff04-877aad1d279c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_pad)\n",
        "y_pred[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00968105, 0.00510269, 0.01223819, 0.9161998 , 0.05677829],\n",
              "       [0.03495167, 0.01374921, 0.0318008 , 0.2897289 , 0.62976944],\n",
              "       [0.05981677, 0.02891751, 0.06560082, 0.5346728 , 0.31099203],\n",
              "       [0.03929907, 0.020247  , 0.15926659, 0.7121886 , 0.06899872],\n",
              "       [0.00630452, 0.00258863, 0.02620382, 0.95616907, 0.00873397],\n",
              "       [0.03840388, 0.00985509, 0.01523667, 0.4653909 , 0.47111353],\n",
              "       [0.04405275, 0.01691445, 0.06129391, 0.7286618 , 0.14907712],\n",
              "       [0.0364914 , 0.0433452 , 0.25970498, 0.6260432 , 0.03441518],\n",
              "       [0.01215471, 0.00286744, 0.0075188 , 0.9526722 , 0.02478686],\n",
              "       [0.01018422, 0.00499421, 0.00988333, 0.9224082 , 0.05253008]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "uX3xNC9VsPhK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's obvious that the prediction for each sample is in the SoftMax format. The output is composed by 5 float numbers denoting the probability the sample to belong to each one of the 5 categories."
      ]
    },
    {
      "metadata": {
        "id": "q2rSPdQotqap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2.3 Turning probabilities into Labels\n",
        "\n",
        "My task is to restore the original labels for testing data. I will follow the opposite procedure than the one used to encode the categorical y values:\n",
        "\n",
        "- Turn SoftMax format into Integer encoding\n",
        "- Turn Integer encoding into original labels, using the original fitted LabelEncoder() instance (inverse transformation)."
      ]
    },
    {
      "metadata": {
        "id": "OTFJJ78dk1n9",
        "colab_type": "code",
        "outputId": "c94b98da-bea5-4ac7-d517-d6032011f2db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Integer encoding\n",
        "# argmax() function will replace the set of probabilities with the index of\n",
        "# the higher probability \n",
        "y_p = np.argmax(y_pred, axis=1)\n",
        "y_p_t = np.argmax(y_test, axis=1)\n",
        "y_p[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 4, 3, 3, 3, 4, 3, 3, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "2Ya6dUiXlbzN",
        "colab_type": "code",
        "outputId": "9a40f8da-0030-4219-a64f-fb0a0fe8293e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Inverse transformation of Integer encoding to Labels\n",
        "y_predictions = le.inverse_transform(y_p)\n",
        "y_true = le.inverse_transform(y_p_t)\n",
        "y_predictions[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['misc', 'ownx', 'misc', 'misc', 'misc', 'ownx', 'misc', 'misc',\n",
              "       'misc', 'misc'], dtype='<U4')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "OAn9rJri3Nl_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.2.2.4 Calculate the metrics"
      ]
    },
    {
      "metadata": {
        "id": "nY-mC48wlbGF",
        "colab_type": "code",
        "outputId": "67d31ce6-16c4-4912-a4a0-cac9c6a79cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_true,y_predictions)\n",
        "recall = recall_score(y_true,y_predictions,average='macro')\n",
        "precision = precision_score(y_true,y_predictions,average='macro')\n",
        "f1 = f1_score(y_true,y_predictions,average='macro')\n",
        "\n",
        "print('Accuracy:\\t{:.3f}'.format(accuracy))\n",
        "print('Recall:\\t\\t{:.3f}'.format(recall))\n",
        "print('Precision:\\t{:.3f}'.format(precision))\n",
        "print('F1 score:\\t{:.3f}'.format(f1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:\t0.654\n",
            "Recall:\t\t0.289\n",
            "Precision:\t0.549\n",
            "F1 score:\t0.304\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "P8Fl5t9sLgfo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3.2.3 Conclusions\n",
        "\n",
        "Judging by reported accuracy this model performs better than models used on Part 3.1. Surprisingly performs much better than the word embedding obtained from Part 2 trained on the same Dataset. Probably the difference is due to the different training method used here."
      ]
    },
    {
      "metadata": {
        "id": "uCAmgPRALf7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XORTHfGVHh5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3.2.4 (Optional) Creating different models (classifiers) using different NN definitions\n",
        "Just for experimental reasons and to compare performance with the first one"
      ]
    },
    {
      "metadata": {
        "id": "Di093jemiYtm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.2.4.1 Using Convonutional and Pooling layers instead of Flatten layer\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "2ff9147a-a646-4c05-e678-f1681682ef02",
        "id": "gwqe1Rypf_ak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Model's parameters\n",
        "output_dim = 100\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "filters = 256\n",
        "batch = 64\n",
        "\n",
        "parameters = [output_dim,activation,losses,optimizer,filters,batch]\n",
        "\n",
        "\n",
        "def anotherAnnDef(param):\n",
        "  from keras.layers.core import SpatialDropout1D\n",
        "  from keras.layers.convolutional import Conv1D\n",
        "  from keras.layers.pooling import GlobalMaxPooling1D\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=output_dim, input_length=max_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  model.add(Conv1D(filters=filters,\n",
        "                   kernel_size=3,\n",
        "                   activation=\"relu\"))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dense(5, activation=activation))\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=losses,metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "  model.fit(X_train_pad,y_train,batch_size=batch,epochs=20,validation_data=(X_test_pad, y_test))\n",
        "\n",
        "  # evaluate model\n",
        "  score = model.evaluate(X_test_pad, y_test, verbose=1)\n",
        "  print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))\n",
        "\n",
        "  \n",
        "model_another = anotherAnnDef(parameters)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 1.4422 - acc: 0.4683 - val_loss: 1.2183 - val_acc: 0.6000\n",
            "Epoch 2/20\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 1.0872 - acc: 0.6000 - val_loss: 1.0748 - val_acc: 0.6000\n",
            "Epoch 3/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 1.0221 - acc: 0.6000 - val_loss: 1.0549 - val_acc: 0.6000\n",
            "Epoch 4/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.9733 - acc: 0.6000 - val_loss: 1.0386 - val_acc: 0.6000\n",
            "Epoch 5/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.9004 - acc: 0.6122 - val_loss: 1.0103 - val_acc: 0.6049\n",
            "Epoch 6/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.7779 - acc: 0.7805 - val_loss: 0.9701 - val_acc: 0.6537\n",
            "Epoch 7/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.6203 - acc: 0.8549 - val_loss: 0.9410 - val_acc: 0.6732\n",
            "Epoch 8/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.4691 - acc: 0.8659 - val_loss: 0.9411 - val_acc: 0.6829\n",
            "Epoch 9/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.3458 - acc: 0.8854 - val_loss: 0.9801 - val_acc: 0.6927\n",
            "Epoch 10/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.2597 - acc: 0.9159 - val_loss: 1.0449 - val_acc: 0.6878\n",
            "Epoch 11/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1885 - acc: 0.9463 - val_loss: 1.1055 - val_acc: 0.6878\n",
            "Epoch 12/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1389 - acc: 0.9732 - val_loss: 1.1755 - val_acc: 0.6878\n",
            "Epoch 13/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1013 - acc: 0.9829 - val_loss: 1.2463 - val_acc: 0.6829\n",
            "Epoch 14/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.0714 - acc: 0.9915 - val_loss: 1.3054 - val_acc: 0.6829\n",
            "Epoch 15/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.0517 - acc: 0.9988 - val_loss: 1.3608 - val_acc: 0.6634\n",
            "Epoch 16/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.0377 - acc: 1.0000 - val_loss: 1.4002 - val_acc: 0.6634\n",
            "Epoch 17/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.0270 - acc: 1.0000 - val_loss: 1.4359 - val_acc: 0.6537\n",
            "Epoch 18/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.0206 - acc: 1.0000 - val_loss: 1.4600 - val_acc: 0.6488\n",
            "Epoch 19/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 1.4801 - val_acc: 0.6488\n",
            "Epoch 20/20\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.0124 - acc: 1.0000 - val_loss: 1.5013 - val_acc: 0.6488\n",
            "205/205 [==============================] - 0s 260us/step\n",
            "Test score: 1.501, accuracy: 0.649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2IgLsZcDinY0"
      },
      "cell_type": "markdown",
      "source": [
        "####3.2.4.2 Using a Lambda layer to AVERAGE the embeddings\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "e987af87-971d-492b-db6b-ca6d0895df6a",
        "id": "i7MdifGyinZC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Model's parameters\n",
        "output_dim = 100\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "filters = 256\n",
        "batch = 64\n",
        "\n",
        "parameters = [output_dim,activation,losses,optimizer,filters,batch]\n",
        "\n",
        "\n",
        "def yetAnotherAnnDef(param):\n",
        "  import keras\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=output_dim, input_length=max_length))\n",
        "  model.add(keras.layers.Lambda(lambda x: keras.backend.mean(x, axis=1)))\n",
        "  model.add(Dense(5, activation=activation))\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=losses,metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "  model.fit(X_train_pad,y_train,batch_size=batch,epochs=20,validation_data=(X_test_pad, y_test))\n",
        "\n",
        "  # evaluate model\n",
        "  score = model.evaluate(X_test_pad, y_test, verbose=1)\n",
        "  print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))\n",
        "\n",
        "  \n",
        "model_yet_another = yetAnotherAnnDef(parameters)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/20\n",
            "820/820 [==============================] - 1s 715us/step - loss: 1.5584 - acc: 0.6000 - val_loss: 1.5089 - val_acc: 0.6000\n",
            "Epoch 2/20\n",
            "820/820 [==============================] - 0s 161us/step - loss: 1.4630 - acc: 0.6000 - val_loss: 1.4172 - val_acc: 0.6000\n",
            "Epoch 3/20\n",
            "820/820 [==============================] - 0s 173us/step - loss: 1.3672 - acc: 0.6000 - val_loss: 1.3207 - val_acc: 0.6000\n",
            "Epoch 4/20\n",
            "820/820 [==============================] - 0s 171us/step - loss: 1.2668 - acc: 0.6000 - val_loss: 1.2289 - val_acc: 0.6000\n",
            "Epoch 5/20\n",
            "820/820 [==============================] - 0s 169us/step - loss: 1.1759 - acc: 0.6000 - val_loss: 1.1516 - val_acc: 0.6000\n",
            "Epoch 6/20\n",
            "820/820 [==============================] - 0s 160us/step - loss: 1.1053 - acc: 0.6000 - val_loss: 1.0977 - val_acc: 0.6000\n",
            "Epoch 7/20\n",
            "820/820 [==============================] - 0s 172us/step - loss: 1.0564 - acc: 0.6000 - val_loss: 1.0682 - val_acc: 0.6000\n",
            "Epoch 8/20\n",
            "820/820 [==============================] - 0s 173us/step - loss: 1.0304 - acc: 0.6000 - val_loss: 1.0516 - val_acc: 0.6000\n",
            "Epoch 9/20\n",
            "820/820 [==============================] - 0s 173us/step - loss: 1.0129 - acc: 0.6000 - val_loss: 1.0433 - val_acc: 0.6000\n",
            "Epoch 10/20\n",
            "820/820 [==============================] - 0s 170us/step - loss: 1.0001 - acc: 0.6000 - val_loss: 1.0374 - val_acc: 0.6000\n",
            "Epoch 11/20\n",
            "820/820 [==============================] - 0s 169us/step - loss: 0.9894 - acc: 0.6000 - val_loss: 1.0330 - val_acc: 0.6000\n",
            "Epoch 12/20\n",
            "820/820 [==============================] - 0s 185us/step - loss: 0.9781 - acc: 0.6000 - val_loss: 1.0287 - val_acc: 0.6000\n",
            "Epoch 13/20\n",
            "820/820 [==============================] - 0s 179us/step - loss: 0.9672 - acc: 0.6000 - val_loss: 1.0246 - val_acc: 0.6000\n",
            "Epoch 14/20\n",
            "820/820 [==============================] - 0s 180us/step - loss: 0.9561 - acc: 0.6000 - val_loss: 1.0206 - val_acc: 0.6000\n",
            "Epoch 15/20\n",
            "820/820 [==============================] - 0s 166us/step - loss: 0.9444 - acc: 0.6012 - val_loss: 1.0167 - val_acc: 0.6049\n",
            "Epoch 16/20\n",
            "820/820 [==============================] - 0s 161us/step - loss: 0.9326 - acc: 0.6024 - val_loss: 1.0125 - val_acc: 0.6049\n",
            "Epoch 17/20\n",
            "820/820 [==============================] - 0s 166us/step - loss: 0.9198 - acc: 0.6061 - val_loss: 1.0082 - val_acc: 0.6098\n",
            "Epoch 18/20\n",
            "820/820 [==============================] - 0s 176us/step - loss: 0.9067 - acc: 0.6085 - val_loss: 1.0036 - val_acc: 0.6098\n",
            "Epoch 19/20\n",
            "820/820 [==============================] - 0s 159us/step - loss: 0.8926 - acc: 0.6146 - val_loss: 0.9988 - val_acc: 0.6098\n",
            "Epoch 20/20\n",
            "820/820 [==============================] - 0s 164us/step - loss: 0.8776 - acc: 0.6220 - val_loss: 0.9941 - val_acc: 0.6098\n",
            "205/205 [==============================] - 0s 66us/step\n",
            "Test score: 0.994, accuracy: 0.610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jtQl5cPulGdQ"
      },
      "cell_type": "markdown",
      "source": [
        "####3.2.4.3 Using a Lambda layer to SUM the embeddings\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b61997c2-362f-4f2a-804c-9f845e27224f",
        "id": "e_bX7QV4lGdh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Model's parameters\n",
        "output_dim = 100\n",
        "activation = 'softmax'\n",
        "losses = 'categorical_crossentropy'\n",
        "optimizer = 'adam'\n",
        "filters = 256\n",
        "batch = 64\n",
        "\n",
        "parameters = [output_dim,activation,losses,optimizer,filters,batch]\n",
        "\n",
        "\n",
        "def andAnotherAnnDef(param):\n",
        "  import keras\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=output_dim, input_length=max_length))\n",
        "  model.add(keras.layers.Lambda(lambda x: keras.backend.sum(x, axis=1)))\n",
        "  model.add(Dense(5, activation=activation))\n",
        "\n",
        "  model.compile(optimizer=optimizer, loss=losses,metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "  model.fit(X_train_pad,y_train,batch_size=batch,epochs=20,validation_data=(X_test_pad, y_test))\n",
        "\n",
        "  # evaluate model\n",
        "  score = model.evaluate(X_test_pad, y_test, verbose=1)\n",
        "  print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))\n",
        "\n",
        "  \n",
        "model_and_another = andAnotherAnnDef(parameters)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/20\n",
            "820/820 [==============================] - 1s 639us/step - loss: 1.0928 - acc: 0.5683 - val_loss: 1.0525 - val_acc: 0.6098\n",
            "Epoch 2/20\n",
            "820/820 [==============================] - 0s 152us/step - loss: 0.8176 - acc: 0.7341 - val_loss: 0.9640 - val_acc: 0.6146\n",
            "Epoch 3/20\n",
            "820/820 [==============================] - 0s 154us/step - loss: 0.6123 - acc: 0.8122 - val_loss: 0.9063 - val_acc: 0.6829\n",
            "Epoch 4/20\n",
            "820/820 [==============================] - 0s 155us/step - loss: 0.4839 - acc: 0.8524 - val_loss: 0.8801 - val_acc: 0.6976\n",
            "Epoch 5/20\n",
            "820/820 [==============================] - 0s 164us/step - loss: 0.3837 - acc: 0.9146 - val_loss: 0.8636 - val_acc: 0.7024\n",
            "Epoch 6/20\n",
            "820/820 [==============================] - 0s 159us/step - loss: 0.3042 - acc: 0.9451 - val_loss: 0.8524 - val_acc: 0.7073\n",
            "Epoch 7/20\n",
            "820/820 [==============================] - 0s 165us/step - loss: 0.2444 - acc: 0.9622 - val_loss: 0.8463 - val_acc: 0.7024\n",
            "Epoch 8/20\n",
            "820/820 [==============================] - 0s 152us/step - loss: 0.1982 - acc: 0.9841 - val_loss: 0.8475 - val_acc: 0.7024\n",
            "Epoch 9/20\n",
            "820/820 [==============================] - 0s 154us/step - loss: 0.1606 - acc: 0.9890 - val_loss: 0.8437 - val_acc: 0.6976\n",
            "Epoch 10/20\n",
            "820/820 [==============================] - 0s 162us/step - loss: 0.1334 - acc: 0.9939 - val_loss: 0.8473 - val_acc: 0.7024\n",
            "Epoch 11/20\n",
            "820/820 [==============================] - 0s 153us/step - loss: 0.1099 - acc: 0.9976 - val_loss: 0.8516 - val_acc: 0.6976\n",
            "Epoch 12/20\n",
            "820/820 [==============================] - 0s 175us/step - loss: 0.0937 - acc: 0.9963 - val_loss: 0.8550 - val_acc: 0.7024\n",
            "Epoch 13/20\n",
            "820/820 [==============================] - 0s 158us/step - loss: 0.0790 - acc: 0.9988 - val_loss: 0.8669 - val_acc: 0.6976\n",
            "Epoch 14/20\n",
            "820/820 [==============================] - 0s 171us/step - loss: 0.0667 - acc: 0.9988 - val_loss: 0.8749 - val_acc: 0.7024\n",
            "Epoch 15/20\n",
            "820/820 [==============================] - 0s 158us/step - loss: 0.0577 - acc: 0.9988 - val_loss: 0.8772 - val_acc: 0.7073\n",
            "Epoch 16/20\n",
            "820/820 [==============================] - 0s 152us/step - loss: 0.0504 - acc: 0.9988 - val_loss: 0.8855 - val_acc: 0.7073\n",
            "Epoch 17/20\n",
            "820/820 [==============================] - 0s 156us/step - loss: 0.0443 - acc: 0.9988 - val_loss: 0.8939 - val_acc: 0.7122\n",
            "Epoch 18/20\n",
            "820/820 [==============================] - 0s 159us/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.9077 - val_acc: 0.6927\n",
            "Epoch 19/20\n",
            "820/820 [==============================] - 0s 157us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 0.9071 - val_acc: 0.7220\n",
            "Epoch 20/20\n",
            "820/820 [==============================] - 0s 161us/step - loss: 0.0313 - acc: 1.0000 - val_loss: 0.9196 - val_acc: 0.6976\n",
            "205/205 [==============================] - 0s 75us/step\n",
            "Test score: 0.920, accuracy: 0.698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a5h9RrolNxd4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LjXODvnQOiu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZA4AGJklKz0I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3 Experimenting with various parameters of the Classifier"
      ]
    },
    {
      "metadata": {
        "id": "qXV1xbp-po7J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.0 Prepare the data\n",
        "\n",
        "If not already done, use procedures at 3.0 to prepare the data for this part"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eN1UsM9_323f"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 Split data, encode and pad sequences"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ffQLw3r43232",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split training-testing data\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,\n",
        "                                               stratify=y, random_state=0)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# encode sequences\n",
        "encoded_X_train = tokenizer.texts_to_sequences(X_train)\n",
        "encoded_X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# pad sequences\n",
        "max_length = max([len(s) for s in X])\n",
        "X_train_pad = pad_sequences(encoded_X_train, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(encoded_X_test, maxlen=max_length, padding='post')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XkeJpIDYOxNJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 Model definition function\n",
        "\n",
        "Same as previous part"
      ]
    },
    {
      "metadata": {
        "id": "HhCBZUKeOwgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def annDef(od, act, los, opt):\n",
        "  # Define Model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=od, input_length=max_length))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(5, activation=act))\n",
        "  print(model.summary())\n",
        "\n",
        "  # Compile Network\n",
        "  model.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  # Fit Network to training data\n",
        "  model.fit(X_train_pad, y_train, batch_size=128, epochs=15, verbose=2, \n",
        "            validation_data=(X_test_pad, y_test))\n",
        "\n",
        "  # Evaluate Network during Training\n",
        "  loss, acc = model.evaluate(X_test_pad, y_test, verbose=2)\n",
        "  print('Test Accuracy: %f' % (acc*100))\n",
        "  \n",
        "  return(model, acc)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cwTpybZ1R2SR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.3 Model evaluation function\n",
        "\n",
        "Currently the only built-in metric provided by Keras is Accuracy.  In order to use other evaluation metrics I have to import them from the sklearn library.\n",
        "\n",
        "I define a function that receives a trained model and evaluates it using sklearn's metrics"
      ]
    },
    {
      "metadata": {
        "id": "XPojGkU0R17N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
        "\n",
        "def evalModel(model):\n",
        "  y_pred = model.predict(X_test_pad)\n",
        "  y_p = np.argmax(y_pred, axis=1)\n",
        "  y_p_t = np.argmax(y_test, axis=1)\n",
        "  y_predictions = le.inverse_transform(y_p)\n",
        "  y_true = le.inverse_transform(y_p_t)\n",
        "  accuracy = accuracy_score(y_true,y_predictions)\n",
        "  recall = recall_score(y_true,y_predictions,average='macro')\n",
        "  precision = precision_score(y_true,y_predictions,average='macro')\n",
        "  f1 = f1_score(y_true,y_predictions,average='macro')\n",
        "  return((accuracy,recall,precision,f1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ZusO0fZPCi6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.4 Execute models\n",
        "\n",
        "Use a range of values to parameterize a set of models.\n",
        "\n",
        "Execute each model 3 times (180 models in total, execution time > 4h)\n",
        "\n",
        "I strore the DataFrame with final results as a .csv file on Drive to access them locally.\n",
        "\n",
        "You can download the final results from this link: [modelsResults.csv](https://)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "6732faf4-afdb-4ebe-9506-a07a7af7b825",
        "id": "MbsGkVP3LUYw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110558
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
        "\n",
        "# Dataframe to store total results\n",
        "headers=['Dimensions','Activation','Losses','Optimizer','K_Accurasy(+/-sd)',\n",
        "         'Accurasy(+/-sd)','Recall(+/-sd)','Precision(+/-sd)','F1_score(+/-sd)']\n",
        "headers2 = ['K_Accurasy','Accurasy','Recall','Precision','F1_score']\n",
        "\n",
        "modelsResults = pd.DataFrame(columns=headers)\n",
        "\n",
        "# Model's parameters' range\n",
        "output_dim = [100,500,1000]\n",
        "activation = ['softmax','sigmoid','relu','tanh','exponential']\n",
        "losses = ['categorical_crossentropy','binary_crossentropy','mean_squared_error','categorical_hinge']\n",
        "optimizer = ['adam','sgd','Adadelta']\n",
        "\n",
        "# Testing set\n",
        "# output_dim = [500]\n",
        "# activation = ['exponential',]\n",
        "# losses = ['binary_crossentropy',]\n",
        "# optimizer = ['adam',]\n",
        "\n",
        "count = 1\n",
        "\n",
        "for od in output_dim:\n",
        "  for act in activation:\n",
        "    for los in losses:\n",
        "      for opt in optimizer:\n",
        "        \n",
        "        total_rounds = len(output_dim)*len(activation)*len(losses)*len(optimizer)\n",
        "        print('Round %d of %d' %(count, total_rounds))\n",
        "        count += 1\n",
        "        \n",
        "        parameters = [od, act, los, opt]\n",
        "        # List to store the results of each set\n",
        "        results = []\n",
        "        # Dataframe to store the results of each run\n",
        "        temp = pd.DataFrame(columns=headers2)\n",
        "        \n",
        "        # Execute model 3 times for each set\n",
        "        for i in range(3):\n",
        "          model, kaccuracy = annDef(od, act, los, opt)\n",
        "          (accuracy,recall,precision,f1) = evalModel(model)\n",
        "          # Store run results as a new row to dataframe\n",
        "          new_row = pd.Series([kaccuracy,accuracy,recall,precision,f1], index=headers2)\n",
        "          temp = temp.append(new_row, ignore_index=1)\n",
        "          # Clear memory for next run\n",
        "          del model,new_row,kaccuracy,accuracy,recall,precision,f1\n",
        "        \n",
        "        # Get the mean statistics of the 3 executions\n",
        "        kacc = temp.K_Accurasy.mean()\n",
        "        kaccSd = temp.K_Accurasy.std()\n",
        "        acc = temp.Accurasy.mean()\n",
        "        accSd = temp.Accurasy.std()\n",
        "        rec = temp.Recall.mean()\n",
        "        recSd = temp.Recall.std()\n",
        "        pre = temp.Precision.mean()\n",
        "        preSd = temp.Precision.std()\n",
        "        f1 = temp.F1_score.mean()\n",
        "        f1Sd = temp.F1_score.std()\n",
        "        \n",
        "        # Statistics as stings\n",
        "        kaccStr = '%04.1f+/-%.1f' % (100*kacc, 100*kaccSd)\n",
        "        accStr = '%04.1f+/-%.1f' % (100*acc, 100*accSd)\n",
        "        recStr = '%04.1f+/-%.1f' % (100*rec, 100*recSd)\n",
        "        preStr = '%04.1f+/-%.1f' % (100*pre, 100*preSd)\n",
        "        f1Str = '%04.1f+/-%.1f' % (100*f1, 100*f1Sd)\n",
        "        \n",
        "        stats = [kaccStr,accStr,recStr,preStr,f1Str]\n",
        "        \n",
        "        # Store the final results of set\n",
        "        results.extend(parameters)\n",
        "        results.extend(stats)\n",
        "        \n",
        "        new_row = pd.Series(results, index=headers)\n",
        "        modelsResults = modelsResults.append(new_row, ignore_index=1)\n",
        "        \n",
        "        # Clear memory for next run\n",
        "        del temp,results,new_row,parameters,stats,accStr,recStr,preStr,\n",
        "        f1Str,acc,accSd,rec,recSd,pre,preSd,f1,f1Str,kacc,kaccSd\n",
        "\n",
        "  # Store results after each dimension\n",
        "  store_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/total_'+str(od)+'.csv'\n",
        "  total.to_csv(store_filename, index=False)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round 1 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.1647 - acc: 0.5683\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9762 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8766 - acc: 0.6671\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6929 - acc: 0.7793\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4886 - acc: 0.8854\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3237 - acc: 0.9293\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2077 - acc: 0.9659\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1344 - acc: 0.9841\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0901 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0619 - acc: 0.9976\n",
            "Test Accuracy: 67.317073\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 0s - loss: 1.1518 - acc: 0.5683\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9895 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8725 - acc: 0.6110\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7005 - acc: 0.7756\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4960 - acc: 0.8927\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3262 - acc: 0.9317\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2118 - acc: 0.9683\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1343 - acc: 0.9866\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0901 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0619 - acc: 0.9988\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.1933 - acc: 0.5744\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9854 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8804 - acc: 0.6049\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7081 - acc: 0.7951\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5020 - acc: 0.8805\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3297 - acc: 0.9268\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2128 - acc: 0.9695\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1367 - acc: 0.9866\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0907 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0623 - acc: 0.9976\n",
            "Test Accuracy: 67.804878\n",
            "Round 2 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.4772 - acc: 0.5427\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.2163 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0986 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0647 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0519 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0458 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0438 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0410 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0391 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0386 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.4688 - acc: 0.5024\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.2037 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0929 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0638 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0529 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0471 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0444 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0418 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0402 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0384 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 0s - loss: 1.4640 - acc: 0.5524\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.2089 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0969 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0631 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0502 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0450 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0418 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0396 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0381 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0359 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 3 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_7 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.1626 - acc: 0.5732\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0248 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9961 - acc: 0.5963\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9538 - acc: 0.6220\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9010 - acc: 0.6268\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8280 - acc: 0.6793\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7382 - acc: 0.7537\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6469 - acc: 0.8183\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5520 - acc: 0.8646\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4680 - acc: 0.8915\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.1221 - acc: 0.5793\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0417 - acc: 0.5890\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0077 - acc: 0.6024\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9632 - acc: 0.6122\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9190 - acc: 0.6220\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8454 - acc: 0.6573\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7601 - acc: 0.7244\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6663 - acc: 0.8024\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5713 - acc: 0.8683\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4854 - acc: 0.8890\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.1404 - acc: 0.5988\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0277 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9952 - acc: 0.6037\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9580 - acc: 0.6085\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9038 - acc: 0.6207\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8285 - acc: 0.6854\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7424 - acc: 0.7256\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6509 - acc: 0.8195\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5575 - acc: 0.8549\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4745 - acc: 0.8805\n",
            "Test Accuracy: 66.341464\n",
            "Round 4 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_10 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3838 - acc: 0.8268\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3368 - acc: 0.8549\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2962 - acc: 0.8832\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2326 - acc: 0.9371\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1656 - acc: 0.9710\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1142 - acc: 0.9737\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0783 - acc: 0.9829\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0527 - acc: 0.9900\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0354 - acc: 0.9944\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0236 - acc: 0.9978\n",
            "Test Accuracy: 87.609754\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3831 - acc: 0.8305\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3340 - acc: 0.8439\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2933 - acc: 0.8659\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2318 - acc: 0.9451\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1648 - acc: 0.9688\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1133 - acc: 0.9756\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0774 - acc: 0.9827\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0524 - acc: 0.9893\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0354 - acc: 0.9949\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0236 - acc: 0.9983\n",
            "Test Accuracy: 87.317071\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_12 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3791 - acc: 0.8300\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3328 - acc: 0.8454\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2938 - acc: 0.8724\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2299 - acc: 0.9417\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1633 - acc: 0.9702\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1124 - acc: 0.9749\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0768 - acc: 0.9859\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0514 - acc: 0.9895\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0344 - acc: 0.9956\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0231 - acc: 0.9976\n",
            "Test Accuracy: 87.609755\n",
            "Round 5 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_13 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4918 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4722 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4527 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4329 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4142 - acc: 0.8000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3978 - acc: 0.8000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3850 - acc: 0.8020\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3765 - acc: 0.8268\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3707 - acc: 0.8346\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3667 - acc: 0.8373\n",
            "Test Accuracy: 84.195119\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4928 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4727 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4530 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4332 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4142 - acc: 0.8000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3978 - acc: 0.8000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3854 - acc: 0.8022\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3767 - acc: 0.8268\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3710 - acc: 0.8329\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3671 - acc: 0.8390\n",
            "Test Accuracy: 83.999998\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_15 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.4976 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4780 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4586 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4390 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4197 - acc: 0.8000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4023 - acc: 0.8000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3882 - acc: 0.8000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3783 - acc: 0.8149\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3715 - acc: 0.8346\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3671 - acc: 0.8378\n",
            "Test Accuracy: 83.902436\n",
            "Round 6 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_16 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3821 - acc: 0.8276\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3516 - acc: 0.8415\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3486 - acc: 0.8393\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3434 - acc: 0.8446\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3402 - acc: 0.8420\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3320 - acc: 0.8488\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3246 - acc: 0.8517\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3128 - acc: 0.8629\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3023 - acc: 0.8702\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2899 - acc: 0.8829\n",
            "Test Accuracy: 84.682925\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_17 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_17 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3822 - acc: 0.8237\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3498 - acc: 0.8402\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3486 - acc: 0.8412\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3446 - acc: 0.8410\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3386 - acc: 0.8441\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3326 - acc: 0.8505\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3243 - acc: 0.8546\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3150 - acc: 0.8585\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3033 - acc: 0.8688\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2900 - acc: 0.8832\n",
            "Test Accuracy: 84.585364\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_18 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.3848 - acc: 0.8241\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3533 - acc: 0.8351\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3482 - acc: 0.8410\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3453 - acc: 0.8407\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3377 - acc: 0.8444\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3333 - acc: 0.8476\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3263 - acc: 0.8544\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3160 - acc: 0.8585\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3054 - acc: 0.8668\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2922 - acc: 0.8790\n",
            "Test Accuracy: 84.487803\n",
            "Round 7 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_19 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_19 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1224 - acc: 0.5573\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1038 - acc: 0.6085\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0885 - acc: 0.6878\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0635 - acc: 0.8610\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0445 - acc: 0.8756\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0325 - acc: 0.8951\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0239 - acc: 0.9293\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0175 - acc: 0.9463\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0127 - acc: 0.9707\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0088 - acc: 0.9841\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_20 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_20 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1236 - acc: 0.5695\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1041 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0861 - acc: 0.6610\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0615 - acc: 0.8695\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0428 - acc: 0.8768\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0320 - acc: 0.9024\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0238 - acc: 0.9280\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0175 - acc: 0.9524\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0125 - acc: 0.9683\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0087 - acc: 0.9841\n",
            "Test Accuracy: 68.780488\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_21 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_21 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1218 - acc: 0.5817\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1040 - acc: 0.6195\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0872 - acc: 0.6976\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0625 - acc: 0.8646\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0439 - acc: 0.8768\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0327 - acc: 0.8927\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0247 - acc: 0.9268\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0184 - acc: 0.9427\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0132 - acc: 0.9683\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0091 - acc: 0.9817\n",
            "Test Accuracy: 66.341464\n",
            "Round 8 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_22 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_22 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1610 - acc: 0.0963\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1587 - acc: 0.4805\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1563 - acc: 0.5963\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1540 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1515 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1490 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1465 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1438 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1411 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1383 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_23 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_23 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1568 - acc: 0.3549\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1547 - acc: 0.5098\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1525 - acc: 0.5866\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1502 - acc: 0.5988\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1478 - acc: 0.5988\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1453 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1428 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1401 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1375 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1348 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_24 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_24 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1582 - acc: 0.3841\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1559 - acc: 0.5634\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1536 - acc: 0.5963\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1512 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1488 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1462 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1436 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1409 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1382 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1354 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 9 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_25 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_25 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1238 - acc: 0.5866\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1123 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1109 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1100 - acc: 0.5976\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1095 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1093 - acc: 0.5976\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1081 - acc: 0.5988\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1069 - acc: 0.6037\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1061 - acc: 0.6073\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1051 - acc: 0.6012\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_26 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_26 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1257 - acc: 0.5707\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1118 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1110 - acc: 0.6049\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1100 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1101 - acc: 0.5988\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1095 - acc: 0.5976\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1079 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1080 - acc: 0.6037\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1062 - acc: 0.6037\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1055 - acc: 0.6098\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_27 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_27 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1250 - acc: 0.5756\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1121 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1109 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1099 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1100 - acc: 0.6024\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1086 - acc: 0.6037\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1091 - acc: 0.5939\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1080 - acc: 0.6024\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1070 - acc: 0.6073\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1060 - acc: 0.5988\n",
            "Test Accuracy: 59.024390\n",
            "Round 10 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_28 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_28 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.9056 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8006 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7988 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7981 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7971 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7941 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7786 - acc: 0.6037\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7245 - acc: 0.6354\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5217 - acc: 0.7573\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3115 - acc: 0.8707\n",
            "Test Accuracy: 66.341464\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_29 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_29 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.9060 - acc: 0.5707\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8002 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7991 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7986 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7980 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7973 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7954 - acc: 0.6012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7879 - acc: 0.6012\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7577 - acc: 0.6195\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.6494 - acc: 0.6695\n",
            "Test Accuracy: 62.926829\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_30 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_30 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.8958 - acc: 0.5817\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8000 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7990 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7985 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7979 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7970 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7939 - acc: 0.6012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7823 - acc: 0.6073\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7393 - acc: 0.6244\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.5637 - acc: 0.7317\n",
            "Test Accuracy: 64.878049\n",
            "Round 11 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_31 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_31 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 1.0008 - acc: 0.4744\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9964 - acc: 0.5963\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9926 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9885 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9836 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9777 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9702 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9610 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9489 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.9339 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_32 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_32 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.9982 - acc: 0.5951\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9945 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9907 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9861 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9806 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9737 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9650 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9540 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9392 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.9209 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_33 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_33 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.9996 - acc: 0.5341\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9959 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9921 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9877 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9825 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9761 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9684 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9584 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9457 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.9294 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 12 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_34 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_34 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.9080 - acc: 0.5878\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8034 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8009 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8004 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8001 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7999 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7998 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7997 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7997 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.7996 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_35 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_35 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.9188 - acc: 0.5756\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8051 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8012 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8004 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8001 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7999 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7998 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7997 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7996 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.7995 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_36 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_36 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.9043 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8046 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8013 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8005 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8002 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8000 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7999 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7998 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7997 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.7997 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "Round 13 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_37 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_37 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.2154 - acc: 0.5817\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0185 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9598 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8795 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7340 - acc: 0.6159\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4268 - acc: 0.9012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1928 - acc: 0.9671\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1037 - acc: 0.9927\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0613 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0399 - acc: 0.9988\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_38 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_38 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.2394 - acc: 0.5695\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0224 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9624 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8805 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7541 - acc: 0.6037\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4805 - acc: 0.8646\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2077 - acc: 0.9598\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1041 - acc: 0.9902\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0621 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0405 - acc: 0.9988\n",
            "Test Accuracy: 65.365854\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_39 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_39 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.2120 - acc: 0.5744\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0155 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9706 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8815 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7603 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.5467 - acc: 0.7720\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2338 - acc: 0.9573\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1090 - acc: 0.9817\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0629 - acc: 0.9963\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0415 - acc: 0.9976\n",
            "Test Accuracy: 66.341464\n",
            "Round 14 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_40 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_40 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.5779 - acc: 0.4780\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.5009 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.4180 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.3285 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.2404 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.1652 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.1134 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0830 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0655 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0553 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_41 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_41 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.5763 - acc: 0.5037\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4972 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.4115 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.3176 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.2266 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.1514 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.1018 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0738 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0592 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0512 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_42 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_42 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.5816 - acc: 0.4805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.5032 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.4201 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.3298 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.2397 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.1635 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.1110 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0798 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0630 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0535 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 15 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_43 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_43 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1939 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0414 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0260 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0128 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9998 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9754 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9490 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8952 - acc: 0.6220\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8232 - acc: 0.6476\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.7175 - acc: 0.7463\n",
            "Test Accuracy: 62.926829\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_44 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_44 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1738 - acc: 0.5902\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0381 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0344 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0151 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0024 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9808 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9548 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9223 - acc: 0.6049\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8707 - acc: 0.6183\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.7972 - acc: 0.6841\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_45 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_45 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 1.1892 - acc: 0.5915\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0359 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0262 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0124 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9956 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9714 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9423 - acc: 0.6061\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8917 - acc: 0.6183\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8121 - acc: 0.6598\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.7188 - acc: 0.7366\n",
            "Test Accuracy: 53.658537\n",
            "Round 16 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_46 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_46 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4550 - acc: 0.8220\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3457 - acc: 0.8400\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3230 - acc: 0.8429\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2850 - acc: 0.8880\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2318 - acc: 0.9410\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1748 - acc: 0.9690\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1284 - acc: 0.9761\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0944 - acc: 0.9807\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0703 - acc: 0.9883\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0519 - acc: 0.9905\n",
            "Test Accuracy: 86.731707\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_47 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_47 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4570 - acc: 0.8295\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3439 - acc: 0.8417\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3209 - acc: 0.8451\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2836 - acc: 0.8795\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2321 - acc: 0.9366\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1757 - acc: 0.9705\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1290 - acc: 0.9761\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0952 - acc: 0.9807\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0707 - acc: 0.9854\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0528 - acc: 0.9922\n",
            "Test Accuracy: 86.829266\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_48 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_48 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4491 - acc: 0.8268\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3442 - acc: 0.8400\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3221 - acc: 0.8456\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2859 - acc: 0.8949\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2336 - acc: 0.9327\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1785 - acc: 0.9676\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1305 - acc: 0.9761\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0958 - acc: 0.9817\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0710 - acc: 0.9873\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0528 - acc: 0.9910\n",
            "Test Accuracy: 85.951218\n",
            "Round 17 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_49 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_49 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6821 - acc: 0.6132\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6467 - acc: 0.8117\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6094 - acc: 0.8378\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5684 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5253 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4842 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4489 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4213 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4013 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3873 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_50 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_50 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6763 - acc: 0.7454\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6372 - acc: 0.8385\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5964 - acc: 0.8402\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5528 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5090 - acc: 0.8402\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4691 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4366 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4124 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3951 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3832 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_51 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_51 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.6751 - acc: 0.7446\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6359 - acc: 0.8041\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5946 - acc: 0.8295\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5507 - acc: 0.8412\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5069 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4672 - acc: 0.8400\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4351 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4112 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3943 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3826 - acc: 0.8400\n",
            "Test Accuracy: 83.999997\n",
            "Round 18 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_52 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_52 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4377 - acc: 0.8154\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3530 - acc: 0.8405\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3502 - acc: 0.8410\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3498 - acc: 0.8405\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3461 - acc: 0.8405\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3444 - acc: 0.8385\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3426 - acc: 0.8383\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3397 - acc: 0.8422\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3355 - acc: 0.8468\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3319 - acc: 0.8485\n",
            "Test Accuracy: 83.804877\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_53 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_53 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.4345 - acc: 0.8232\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3532 - acc: 0.8400\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3540 - acc: 0.8385\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3484 - acc: 0.8405\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3475 - acc: 0.8405\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3442 - acc: 0.8422\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3428 - acc: 0.8410\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3399 - acc: 0.8432\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3369 - acc: 0.8446\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3334 - acc: 0.8456\n",
            "Test Accuracy: 83.512194\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_54 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_54 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.4306 - acc: 0.8356\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3537 - acc: 0.8398\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3496 - acc: 0.8393\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3504 - acc: 0.8385\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3457 - acc: 0.8415\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3457 - acc: 0.8390\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3422 - acc: 0.8420\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3416 - acc: 0.8380\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3360 - acc: 0.8449\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3333 - acc: 0.8449\n",
            "Test Accuracy: 83.999998\n",
            "Round 19 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_55 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_55 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.1429 - acc: 0.5707\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1078 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1000 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0846 - acc: 0.7049\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0641 - acc: 0.8476\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0461 - acc: 0.8768\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0343 - acc: 0.8976\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0263 - acc: 0.9244\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0203 - acc: 0.9451\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0158 - acc: 0.9622\n",
            "Test Accuracy: 65.365854\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_56 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_56 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.1467 - acc: 0.5793\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1082 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0997 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0843 - acc: 0.6817\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0636 - acc: 0.8683\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0459 - acc: 0.8768\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0340 - acc: 0.9012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0260 - acc: 0.9280\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0201 - acc: 0.9524\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0156 - acc: 0.9622\n",
            "Test Accuracy: 66.341464\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_57 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_57 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.1428 - acc: 0.5963\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1079 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0990 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0842 - acc: 0.7073\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0636 - acc: 0.8524\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0456 - acc: 0.8756\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0341 - acc: 0.9037\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0262 - acc: 0.9268\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0204 - acc: 0.9463\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0159 - acc: 0.9646\n",
            "Test Accuracy: 67.804878\n",
            "Round 20 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_58 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_58 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.2423 - acc: 0.5841\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.2320 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2218 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2115 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2011 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1907 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1806 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1710 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1622 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1543 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_59 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_59 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.2491 - acc: 0.4732\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.2401 - acc: 0.5951\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2313 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2224 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2132 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.2038 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1941 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1845 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1751 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1663 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_60 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_60 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 2s - loss: 0.2495 - acc: 0.2610\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.2396 - acc: 0.4720\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.2300 - acc: 0.5854\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2204 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2107 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.2008 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1908 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1811 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1717 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1631 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "Round 21 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_61 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_61 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.1501 - acc: 0.5841\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1128 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1113 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1107 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1106 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1103 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1097 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1096 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1089 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1088 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_62 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_62 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.1468 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1129 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1115 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1109 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1108 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1104 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1099 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1098 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1090 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1091 - acc: 0.6012\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_63 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_63 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.1473 - acc: 0.5829\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1124 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1116 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1110 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1105 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1102 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1102 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1093 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1095 - acc: 0.6024\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1092 - acc: 0.6024\n",
            "Test Accuracy: 60.000000\n",
            "Round 22 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_64 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_64 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.9381 - acc: 0.5756\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8210 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8023 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7999 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7989 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7981 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7973 - acc: 0.6012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7961 - acc: 0.6012\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7933 - acc: 0.6012\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.7824 - acc: 0.6037\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_65 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_65 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.9579 - acc: 0.5695\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8423 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8066 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8002 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7983 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7965 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7930 - acc: 0.6012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7786 - acc: 0.6024\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7131 - acc: 0.6293\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.5404 - acc: 0.8073\n",
            "Test Accuracy: 65.365854\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_66 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_66 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.9577 - acc: 0.5622\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8380 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8046 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7997 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7981 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.7965 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7937 - acc: 0.6012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.7826 - acc: 0.6024\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.7332 - acc: 0.6171\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.5798 - acc: 0.7451\n",
            "Test Accuracy: 66.341464\n",
            "Round 23 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_67 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_67 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.0003 - acc: 0.5512\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9946 - acc: 0.5976\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9897 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9848 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9794 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9736 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9674 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9608 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9538 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.9464 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_68 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_68 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.0001 - acc: 0.5195\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9938 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9887 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9836 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9781 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9721 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9658 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9590 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9517 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.9444 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_69 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_69 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.0012 - acc: 0.5183\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9941 - acc: 0.5988\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9893 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9841 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9787 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9729 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9665 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9596 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9524 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.9446 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 24 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_70 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_70 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.9530 - acc: 0.5902\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8709 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8240 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8089 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8040 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8022 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8013 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8007 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8004 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8002 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_71 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_71 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.9513 - acc: 0.5878\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8527 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8144 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8055 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8028 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8016 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8010 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8006 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8003 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8002 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_72 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_72 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.9496 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8535 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8134 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8054 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8029 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8016 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8010 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8006 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8003 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8002 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 25 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_73 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_73 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 2.7524 - acc: 0.5695\n",
            "Epoch 2/10\n",
            " - 0s - loss: 2.5015 - acc: 0.6037\n",
            "Epoch 3/10\n",
            " - 0s - loss: 2.4101 - acc: 0.6183\n",
            "Epoch 4/10\n",
            " - 0s - loss: 2.2948 - acc: 0.7061\n",
            "Epoch 5/10\n",
            " - 0s - loss: 2.1891 - acc: 0.8646\n",
            "Epoch 6/10\n",
            " - 0s - loss: nan - acc: 0.8707\n",
            "Epoch 7/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 8/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 9/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 10/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_74 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_74 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: nan - acc: 0.0720\n",
            "Epoch 2/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 3/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 4/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 5/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 6/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 7/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 8/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 9/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 10/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_75 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_75 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 2.4207 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.7951 - acc: 0.6024\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.7020 - acc: 0.6037\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.5780 - acc: 0.6085\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.3873 - acc: 0.6720\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.1426 - acc: 0.8829\n",
            "Epoch 7/10\n",
            " - 0s - loss: nan - acc: 0.2390\n",
            "Epoch 8/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 9/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 10/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "Round 26 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_76 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_76 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 3.0461 - acc: 0.5341\n",
            "Epoch 2/10\n",
            " - 0s - loss: 2.6107 - acc: 0.6012\n",
            "Epoch 3/10\n",
            " - 0s - loss: 2.6066 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 2.6021 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 2.5980 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 2.5928 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 2.5937 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 2.5876 - acc: 0.6012\n",
            "Epoch 9/10\n",
            " - 0s - loss: 2.5833 - acc: 0.6012\n",
            "Epoch 10/10\n",
            " - 0s - loss: 2.5771 - acc: 0.6012\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_77 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_77 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 2.6736 - acc: 0.5829\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.7072 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.5897 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.7374 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.7352 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.7340 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.7329 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.7321 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.7315 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.7311 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_78 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_78 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 5.1670 - acc: 0.5902\n",
            "Epoch 2/10\n",
            " - 0s - loss: 5.0446 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 5.0412 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 5.0453 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 5.0763 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 5.0074 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 5.0721 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 4.9802 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: nan - acc: 0.3695\n",
            "Epoch 10/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "Round 27 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_79 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_79 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: nan - acc: 0.0671\n",
            "Epoch 2/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 3/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 4/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 5/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 6/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 7/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 8/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 9/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 10/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_80 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_80 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 2.8883 - acc: 0.5232\n",
            "Epoch 2/10\n",
            " - 0s - loss: 2.4245 - acc: 0.6402\n",
            "Epoch 3/10\n",
            " - 0s - loss: nan - acc: 0.3049\n",
            "Epoch 4/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 5/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 6/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 7/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 8/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 9/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 10/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_81 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_81 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_81 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 6.3925 - acc: 0.5549\n",
            "Epoch 2/10\n",
            " - 0s - loss: 6.0962 - acc: 0.6012\n",
            "Epoch 3/10\n",
            " - 0s - loss: 5.6963 - acc: 0.6073\n",
            "Epoch 4/10\n",
            " - 0s - loss: 5.4946 - acc: 0.6037\n",
            "Epoch 5/10\n",
            " - 0s - loss: nan - acc: 0.3585\n",
            "Epoch 6/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 7/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 8/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 9/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Epoch 10/10\n",
            " - 0s - loss: nan - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "Round 28 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_82 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_82 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_82 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.6072 - acc: 0.7998\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4588 - acc: 0.8507\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4100 - acc: 0.8744\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3578 - acc: 0.9020\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3047 - acc: 0.9561\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.2577 - acc: 0.9761\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2288 - acc: 0.9820\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.2144 - acc: 0.9841\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.2070 - acc: 0.9856\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2030 - acc: 0.9868\n",
            "Test Accuracy: 84.682926\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_83 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_83 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_83 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 1.0579 - acc: 0.8027\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6378 - acc: 0.8461\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5962 - acc: 0.8788\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5551 - acc: 0.8961\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5106 - acc: 0.9541\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4713 - acc: 0.9673\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4439 - acc: 0.9737\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4286 - acc: 0.9734\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4211 - acc: 0.9739\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4173 - acc: 0.9737\n",
            "Test Accuracy: 85.658535\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_84 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_84 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 1.4529 - acc: 0.8234\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4035 - acc: 0.8495\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.3831 - acc: 0.8624\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.3657 - acc: 0.8880\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.3494 - acc: 0.9063\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.3347 - acc: 0.9151\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.3227 - acc: 0.9193\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.3123 - acc: 0.9200\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.3046 - acc: 0.9200\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.2991 - acc: 0.9200\n",
            "Test Accuracy: 85.658535\n",
            "Round 29 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_85 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_85 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_85 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.5251 - acc: 0.8176\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4943 - acc: 0.8376\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4959 - acc: 0.8390\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5141 - acc: 0.8388\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4915 - acc: 0.8393\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4899 - acc: 0.8407\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4890 - acc: 0.8407\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4880 - acc: 0.8410\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4867 - acc: 0.8412\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4858 - acc: 0.8405\n",
            "Test Accuracy: 83.804877\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_86 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_86 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_86 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.4914 - acc: 0.8012\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3724 - acc: 0.8329\n",
            "Epoch 3/10\n",
            " - 0s - loss: 4.5505 - acc: 0.5771\n",
            "Epoch 4/10\n",
            " - 0s - loss: 3.8102 - acc: 0.6205\n",
            "Epoch 5/10\n",
            " - 0s - loss: 6.8546 - acc: 0.4310\n",
            "Epoch 6/10\n",
            " - 0s - loss: 6.8894 - acc: 0.4298\n",
            "Epoch 7/10\n",
            " - 0s - loss: 9.3955 - acc: 0.2685\n",
            "Epoch 8/10\n",
            " - 0s - loss: 9.3963 - acc: 0.2690\n",
            "Epoch 9/10\n",
            " - 0s - loss: 9.3961 - acc: 0.2683\n",
            "Epoch 10/10\n",
            " - 0s - loss: 9.3960 - acc: 0.2676\n",
            "Test Accuracy: 26.926829\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_87 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_87 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 3s - loss: 0.7397 - acc: 0.7395\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4163 - acc: 0.8185\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4003 - acc: 0.8324\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3832 - acc: 0.8378\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3637 - acc: 0.8385\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3561 - acc: 0.8388\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3543 - acc: 0.8398\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3478 - acc: 0.8388\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3464 - acc: 0.8395\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3447 - acc: 0.8402\n",
            "Test Accuracy: 83.804875\n",
            "Round 30 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_88 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_88 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 2.4775 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 2.4595 - acc: 0.8000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 2.4553 - acc: 0.8000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 2.4518 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 2.4478 - acc: 0.8000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 2.4427 - acc: 0.8000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 2.4368 - acc: 0.8002\n",
            "Epoch 8/10\n",
            " - 0s - loss: 2.4059 - acc: 0.8010\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.6487 - acc: 0.8134\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.6076 - acc: 0.8534\n",
            "Test Accuracy: 84.195119\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_89 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_89 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_89 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.5755 - acc: 0.8005\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5027 - acc: 0.8288\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4879 - acc: 0.8437\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4595 - acc: 0.8449\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4316 - acc: 0.8524\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4174 - acc: 0.8600\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3775 - acc: 0.8717\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3500 - acc: 0.8956\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3149 - acc: 0.9154\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2876 - acc: 0.9480\n",
            "Test Accuracy: 86.146339\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_90 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_90 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.5707 - acc: 0.8073\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5013 - acc: 0.8385\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4795 - acc: 0.8424\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4511 - acc: 0.8456\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4178 - acc: 0.8524\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3818 - acc: 0.8724\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3516 - acc: 0.8898\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3206 - acc: 0.9205\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.2889 - acc: 0.9476\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2612 - acc: 0.9629\n",
            "Test Accuracy: 87.024389\n",
            "Round 31 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_91 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_91 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1860 - acc: 0.2671\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1741 - acc: 0.2732\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1606 - acc: 0.2756\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1502 - acc: 0.2951\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1468 - acc: 0.3207\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1460 - acc: 0.3293\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1458 - acc: 0.3317\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1457 - acc: 0.3317\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1457 - acc: 0.3366\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1457 - acc: 0.3366\n",
            "Test Accuracy: 26.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_92 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_92 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1228 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.0935 - acc: 0.6085\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0671 - acc: 0.8378\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0413 - acc: 0.8695\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0308 - acc: 0.8744\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0275 - acc: 0.8854\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0264 - acc: 0.8988\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0260 - acc: 0.9073\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0259 - acc: 0.9171\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0258 - acc: 0.9232\n",
            "Test Accuracy: 64.878049\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_93 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_93 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1285 - acc: 0.5451\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.0941 - acc: 0.6049\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0681 - acc: 0.7720\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0411 - acc: 0.8805\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0271 - acc: 0.8988\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0219 - acc: 0.9122\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0192 - acc: 0.9195\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0179 - acc: 0.9317\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0174 - acc: 0.9329\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0172 - acc: 0.9500\n",
            "Test Accuracy: 64.390244\n",
            "Round 32 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_94 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_94 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1868 - acc: 0.3366\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1412 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1205 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1146 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1131 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1127 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1125 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1124 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1123 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1121 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_95 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_95 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1775 - acc: 0.4256\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1437 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1265 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1160 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1131 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1123 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1121 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1120 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1119 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1118 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_96 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_96 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_96 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1601 - acc: 0.5220\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1260 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1163 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1142 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1137 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1134 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1130 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1126 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1123 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1122 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "Round 33 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_97 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_97 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_97 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1209 - acc: 0.5585\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1097 - acc: 0.6049\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1043 - acc: 0.6061\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0993 - acc: 0.6134\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0926 - acc: 0.6305\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0853 - acc: 0.6659\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0778 - acc: 0.6927\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0677 - acc: 0.7683\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0589 - acc: 0.8268\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0500 - acc: 0.8585\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_98 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_98 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_98 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1207 - acc: 0.5976\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1103 - acc: 0.6049\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1044 - acc: 0.6061\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1006 - acc: 0.6024\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0928 - acc: 0.6476\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0858 - acc: 0.6488\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0780 - acc: 0.6890\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0692 - acc: 0.7427\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0593 - acc: 0.8085\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0502 - acc: 0.8707\n",
            "Test Accuracy: 63.902439\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_99 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_99 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_99 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.1191 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1101 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1043 - acc: 0.6037\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0986 - acc: 0.6073\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0926 - acc: 0.6317\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0853 - acc: 0.6537\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0775 - acc: 0.6902\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0687 - acc: 0.7573\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0591 - acc: 0.8085\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0500 - acc: 0.8768\n",
            "Test Accuracy: 67.317073\n",
            "Round 34 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_100 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_100 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_100 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.8748 - acc: 0.5780\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7523 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6604 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5106 - acc: 0.6232\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4207 - acc: 0.6598\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4061 - acc: 0.6646\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4013 - acc: 0.6671\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4007 - acc: 0.6671\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4006 - acc: 0.6671\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4005 - acc: 0.6671\n",
            "Test Accuracy: 60.975610\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_101 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_101 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_101 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.8734 - acc: 0.5780\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7585 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6572 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4994 - acc: 0.6256\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4184 - acc: 0.6598\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4039 - acc: 0.6646\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4012 - acc: 0.6671\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4007 - acc: 0.6671\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4006 - acc: 0.6671\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4004 - acc: 0.6671\n",
            "Test Accuracy: 60.487805\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_102 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_102 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_102 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.8473 - acc: 0.5793\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7611 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6616 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5056 - acc: 0.6232\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4173 - acc: 0.6622\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4030 - acc: 0.6659\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4010 - acc: 0.6671\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4007 - acc: 0.6671\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4006 - acc: 0.6671\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4004 - acc: 0.6671\n",
            "Test Accuracy: 60.975610\n",
            "Round 35 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_103 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_103 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_103 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.9689 - acc: 0.5634\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8938 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8200 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8091 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8076 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8064 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8056 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8049 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8041 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8040 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_104 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_104 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_104 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 1.0017 - acc: 0.1061\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0001 - acc: 0.0768\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0000 - acc: 0.0720\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0000 - acc: 0.0732\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0000 - acc: 0.0756\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9999 - acc: 0.0768\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9999 - acc: 0.0841\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9994 - acc: 0.1805\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9641 - acc: 0.5951\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8905 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_105 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_105 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_105 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.9648 - acc: 0.5841\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8879 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8156 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8106 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8088 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8073 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8055 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8041 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8032 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8028 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 36 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_106 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_106 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_106 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.8605 - acc: 0.5878\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8068 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7725 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7302 - acc: 0.6024\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.6624 - acc: 0.6085\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.5806 - acc: 0.6451\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4981 - acc: 0.7134\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3913 - acc: 0.8244\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3082 - acc: 0.8683\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2479 - acc: 0.8732\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_107 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_107 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_107 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 0.8553 - acc: 0.5878\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8180 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7806 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7367 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.6706 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.5853 - acc: 0.6220\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.5076 - acc: 0.6293\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4523 - acc: 0.6488\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4225 - acc: 0.6622\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4106 - acc: 0.6646\n",
            "Test Accuracy: 60.487805\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_108 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_108 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_108 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 1.0081 - acc: 0.0659\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9999 - acc: 0.0695\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9996 - acc: 0.0695\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9993 - acc: 0.0805\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9991 - acc: 0.0707\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.9992 - acc: 0.0890\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.9975 - acc: 0.0902\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.9952 - acc: 0.0939\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.9927 - acc: 0.0951\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.9903 - acc: 0.0915\n",
            "Test Accuracy: 7.804878\n",
            "Round 37 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_109 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_109 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_109 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 4s - loss: 5.7430 - acc: 0.2524\n",
            "Epoch 2/10\n",
            " - 0s - loss: 10.3195 - acc: 0.2720\n",
            "Epoch 3/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Epoch 4/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Epoch 5/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Epoch 6/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Epoch 7/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Epoch 8/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Epoch 9/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Epoch 10/10\n",
            " - 0s - loss: 11.2040 - acc: 0.2720\n",
            "Test Accuracy: 26.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_110 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_110 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_110 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 13.0909 - acc: 0.5585\n",
            "Epoch 2/10\n",
            " - 0s - loss: 14.1328 - acc: 0.5951\n",
            "Epoch 3/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5915\n",
            "Epoch 4/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5890\n",
            "Epoch 5/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5890\n",
            "Epoch 6/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5890\n",
            "Epoch 7/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5890\n",
            "Epoch 8/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5890\n",
            "Epoch 9/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5890\n",
            "Epoch 10/10\n",
            " - 0s - loss: 14.2508 - acc: 0.5890\n",
            "Test Accuracy: 58.536586\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_111 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_111 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_111 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 10.0809 - acc: 0.5512\n",
            "Epoch 2/10\n",
            " - 0s - loss: 10.3681 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 10.4390 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 10.4573 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 10.4768 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 10.4768 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 10.4768 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 10.4768 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 10.4768 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 10.4768 - acc: 0.6000\n",
            "Test Accuracy: 59.512195\n",
            "Round 38 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_112 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_112 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_112 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 2.4801 - acc: 0.0756\n",
            "Epoch 2/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 3/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 4/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 5/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 6/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 7/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 8/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 9/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Epoch 10/10\n",
            " - 0s - loss: 2.5553 - acc: 0.0634\n",
            "Test Accuracy: 6.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_113 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_113 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_113 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 8.6445 - acc: 0.5976\n",
            "Epoch 2/10\n",
            " - 0s - loss: 6.2507 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 6.2900 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_114 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_114 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 6.2543 - acc: 0.0793\n",
            "Epoch 2/10\n",
            " - 0s - loss: 5.5690 - acc: 0.0915\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.7887 - acc: 0.0646\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.7740 - acc: 0.0671\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.7712 - acc: 0.0671\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.7706 - acc: 0.0671\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.7702 - acc: 0.0671\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.7700 - acc: 0.0671\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.7697 - acc: 0.0671\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.7695 - acc: 0.0671\n",
            "Test Accuracy: 6.829268\n",
            "Round 39 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_115 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_115 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 9.7907 - acc: 0.5573\n",
            "Epoch 2/10\n",
            " - 0s - loss: 9.6666 - acc: 0.4585\n",
            "Epoch 3/10\n",
            " - 0s - loss: 10.6246 - acc: 0.3366\n",
            "Epoch 4/10\n",
            " - 0s - loss: 9.4082 - acc: 0.0915\n",
            "Epoch 5/10\n",
            " - 0s - loss: 9.9209 - acc: 0.3415\n",
            "Epoch 6/10\n",
            " - 0s - loss: 10.7427 - acc: 0.3476\n",
            "Epoch 7/10\n",
            " - 0s - loss: 9.7356 - acc: 0.1512\n",
            "Epoch 8/10\n",
            " - 0s - loss: 9.3377 - acc: 0.1146\n",
            "Epoch 9/10\n",
            " - 0s - loss: 10.5361 - acc: 0.1976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 10.0544 - acc: 0.4341\n",
            "Test Accuracy: 9.268293\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_116 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_116 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 7.2695 - acc: 0.4878\n",
            "Epoch 2/10\n",
            " - 0s - loss: 8.0208 - acc: 0.5146\n",
            "Epoch 3/10\n",
            " - 0s - loss: 6.6847 - acc: 0.4244\n",
            "Epoch 4/10\n",
            " - 0s - loss: 6.7028 - acc: 0.4171\n",
            "Epoch 5/10\n",
            " - 0s - loss: 6.7028 - acc: 0.4171\n",
            "Epoch 6/10\n",
            " - 0s - loss: 6.7028 - acc: 0.4171\n",
            "Epoch 7/10\n",
            " - 0s - loss: 6.7028 - acc: 0.4171\n",
            "Epoch 8/10\n",
            " - 0s - loss: 6.7028 - acc: 0.4171\n",
            "Epoch 9/10\n",
            " - 0s - loss: 6.7028 - acc: 0.4171\n",
            "Epoch 10/10\n",
            " - 0s - loss: 6.7028 - acc: 0.4171\n",
            "Test Accuracy: 44.878049\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_117 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_117 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 12.4081 - acc: 0.1561\n",
            "Epoch 2/10\n",
            " - 0s - loss: 14.9208 - acc: 0.0988\n",
            "Epoch 3/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 15.0173 - acc: 0.1000\n",
            "Test Accuracy: 10.731707\n",
            "Round 40 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_118 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_118 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.7690 - acc: 0.6759\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.6458 - acc: 0.4824\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6143 - acc: 0.5880\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5827 - acc: 0.6888\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5502 - acc: 0.7146\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.5184 - acc: 0.8185\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4894 - acc: 0.8595\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4673 - acc: 0.7883\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4524 - acc: 0.6946\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4423 - acc: 0.6312\n",
            "Test Accuracy: 50.243903\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_119 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_119 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 1.4583 - acc: 0.6827\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.4101 - acc: 0.6466\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.3896 - acc: 0.8954\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.3616 - acc: 0.9127\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.3318 - acc: 0.9100\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.3127 - acc: 0.7956\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.3027 - acc: 0.5763\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.2978 - acc: 0.4951\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.2953 - acc: 0.4529\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.2938 - acc: 0.4315\n",
            "Test Accuracy: 36.390245\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_120 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_120 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_120 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.6239 - acc: 0.7861\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5392 - acc: 0.8290\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4777 - acc: 0.8885\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4248 - acc: 0.9300\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3728 - acc: 0.9690\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3357 - acc: 0.9744\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3158 - acc: 0.9746\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3036 - acc: 0.9705\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.2960 - acc: 0.9685\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.2907 - acc: 0.9593\n",
            "Test Accuracy: 85.268292\n",
            "Round 41 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_121 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_121 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_121 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.4552 - acc: 0.8320\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3665 - acc: 0.8383\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3803 - acc: 0.8378\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3852 - acc: 0.8385\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3543 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3575 - acc: 0.8390\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3480 - acc: 0.8398\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3461 - acc: 0.8407\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3776 - acc: 0.8390\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3657 - acc: 0.8385\n",
            "Test Accuracy: 84.195120\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_122 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_122 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_122 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.5014 - acc: 0.8063\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3935 - acc: 0.8310\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3894 - acc: 0.8378\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3877 - acc: 0.8380\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3845 - acc: 0.8395\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3810 - acc: 0.8390\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3803 - acc: 0.8405\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3881 - acc: 0.8407\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3806 - acc: 0.8405\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4068 - acc: 0.8398\n",
            "Test Accuracy: 83.999999\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_123 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_123 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_123 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.7061 - acc: 0.8054\n",
            "Epoch 2/10\n",
            " - 0s - loss: 4.5114 - acc: 0.3278\n",
            "Epoch 3/10\n",
            " - 0s - loss: 7.1081 - acc: 0.1324\n",
            "Epoch 4/10\n",
            " - 0s - loss: 7.7449 - acc: 0.1368\n",
            "Epoch 5/10\n",
            " - 0s - loss: 7.7449 - acc: 0.1368\n",
            "Epoch 6/10\n",
            " - 0s - loss: 7.7448 - acc: 0.1368\n",
            "Epoch 7/10\n",
            " - 0s - loss: 7.7447 - acc: 0.1368\n",
            "Epoch 8/10\n",
            " - 0s - loss: 7.7447 - acc: 0.1368\n",
            "Epoch 9/10\n",
            " - 0s - loss: 7.7446 - acc: 0.1368\n",
            "Epoch 10/10\n",
            " - 0s - loss: 7.7446 - acc: 0.1368\n",
            "Test Accuracy: 13.951220\n",
            "Round 42 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_124 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_124 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_124 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.6507 - acc: 0.8000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5094 - acc: 0.8029\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4685 - acc: 0.8290\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4560 - acc: 0.8412\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4364 - acc: 0.8493\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4231 - acc: 0.8517\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4015 - acc: 0.8637\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3826 - acc: 0.8754\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3627 - acc: 0.9105\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3446 - acc: 0.9320\n",
            "Test Accuracy: 86.146341\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_125 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_125 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_125 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.5846 - acc: 0.8078\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4959 - acc: 0.8380\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4732 - acc: 0.8432\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4589 - acc: 0.8459\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4413 - acc: 0.8527\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4232 - acc: 0.8612\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4041 - acc: 0.8800\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3822 - acc: 0.8998\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3573 - acc: 0.9190\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3328 - acc: 0.9454\n",
            "Test Accuracy: 85.365852\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_126 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_126 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_126 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.7499 - acc: 0.8049\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5711 - acc: 0.8361\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5525 - acc: 0.8444\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5491 - acc: 0.8454\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5230 - acc: 0.8456\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.5090 - acc: 0.8532\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4908 - acc: 0.8732\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.4708 - acc: 0.8866\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.4469 - acc: 0.9100\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4252 - acc: 0.9295\n",
            "Test Accuracy: 85.560974\n",
            "Round 43 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_127 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_127 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_127 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.1295 - acc: 0.5305\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.0956 - acc: 0.6061\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0709 - acc: 0.7427\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0421 - acc: 0.9439\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0224 - acc: 0.9866\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0128 - acc: 0.9963\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0078 - acc: 0.9988\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0029 - acc: 1.0000\n",
            "Test Accuracy: 66.341464\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_128 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_128 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_128 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.1248 - acc: 0.5634\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.0935 - acc: 0.6110\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0678 - acc: 0.7610\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0392 - acc: 0.9524\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0211 - acc: 0.9878\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0124 - acc: 0.9976\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0078 - acc: 0.9976\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0030 - acc: 1.0000\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_129 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_129 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_129 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.1255 - acc: 0.5329\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.0942 - acc: 0.6085\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0681 - acc: 0.7720\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0396 - acc: 0.9537\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0216 - acc: 0.9829\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0128 - acc: 0.9976\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0079 - acc: 0.9976\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0052 - acc: 0.9988\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0032 - acc: 1.0000\n",
            "Test Accuracy: 64.390244\n",
            "Round 44 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_130 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_130 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_130 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.1718 - acc: 0.4659\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1313 - acc: 0.5988\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1184 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1143 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1129 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1124 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1121 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1120 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1119 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1118 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_131 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_131 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_131 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.1705 - acc: 0.5537\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1303 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1179 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1141 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1128 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1123 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1120 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1119 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1118 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1117 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_132 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_132 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_132 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 5s - loss: 0.1641 - acc: 0.5402\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1293 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1180 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1144 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1131 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1126 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1124 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1122 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1121 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1120 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 45 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_133 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_133 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_133 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.1188 - acc: 0.5780\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1100 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1079 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1024 - acc: 0.6024\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0982 - acc: 0.6037\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0934 - acc: 0.6110\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0888 - acc: 0.6305\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0831 - acc: 0.6659\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0759 - acc: 0.7244\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0691 - acc: 0.7744\n",
            "Test Accuracy: 61.463415\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_134 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_134 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.1215 - acc: 0.5768\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1097 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1076 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1028 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0986 - acc: 0.6073\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0946 - acc: 0.6061\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0889 - acc: 0.6244\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0833 - acc: 0.6598\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0765 - acc: 0.7098\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0699 - acc: 0.7463\n",
            "Test Accuracy: 64.390244\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_135 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_135 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.1210 - acc: 0.5744\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1100 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1064 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1027 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0985 - acc: 0.6049\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0938 - acc: 0.6134\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0882 - acc: 0.6280\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0833 - acc: 0.6646\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0761 - acc: 0.6976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0697 - acc: 0.7744\n",
            "Test Accuracy: 62.439025\n",
            "Round 46 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_136 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_136 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_136 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.8915 - acc: 0.5622\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7817 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7239 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6413 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5507 - acc: 0.6037\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4743 - acc: 0.6793\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3215 - acc: 0.9683\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1660 - acc: 0.9841\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1071 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0719 - acc: 1.0000\n",
            "Test Accuracy: 67.317073\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_137 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_137 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_137 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.8902 - acc: 0.5817\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7784 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7193 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6386 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5476 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4831 - acc: 0.6390\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3714 - acc: 0.9305\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1923 - acc: 0.9805\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1154 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0732 - acc: 1.0000\n",
            "Test Accuracy: 65.365854\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_138 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_138 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_138 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.8748 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7781 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7256 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6455 - acc: 0.6012\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.5565 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.4882 - acc: 0.6256\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.4002 - acc: 0.8549\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.2168 - acc: 0.9720\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1280 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0813 - acc: 0.9988\n",
            "Test Accuracy: 65.853659\n",
            "Round 47 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_139 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_139 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_139 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.9818 - acc: 0.5451\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9210 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8769 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8517 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8370 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8279 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8229 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8192 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8173 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8154 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_140 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_140 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_140 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.9817 - acc: 0.5585\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9273 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8839 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8558 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8397 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8301 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8240 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8204 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8190 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8160 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_141 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_141 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_141 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.9797 - acc: 0.5756\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9261 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8828 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8560 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8382 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8298 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8235 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8209 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8183 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8164 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 48 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_142 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_142 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_142 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.9168 - acc: 0.5683\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8348 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8070 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7808 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7429 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.6968 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.6487 - acc: 0.6012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6029 - acc: 0.6061\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5482 - acc: 0.6951\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4481 - acc: 0.8817\n",
            "Test Accuracy: 64.390244\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_143 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_143 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_143 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.9276 - acc: 0.5720\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8437 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8200 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7834 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7422 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.6994 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.6496 - acc: 0.6012\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6020 - acc: 0.6122\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5277 - acc: 0.7537\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4107 - acc: 0.8890\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_144 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_144 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_144 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 0.9179 - acc: 0.5805\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8320 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8058 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7768 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.7399 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.6936 - acc: 0.6012\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.6425 - acc: 0.6037\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.5956 - acc: 0.6110\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5346 - acc: 0.7037\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4375 - acc: 0.8988\n",
            "Test Accuracy: 64.390244\n",
            "Round 49 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_145 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_145 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_145 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.1599 - acc: 0.5866\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9880 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8671 - acc: 0.6207\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6959 - acc: 0.8317\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4934 - acc: 0.8927\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3239 - acc: 0.9268\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2100 - acc: 0.9659\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1351 - acc: 0.9927\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0908 - acc: 0.9976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0627 - acc: 0.9976\n",
            "Test Accuracy: 66.341464\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_146 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_146 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_146 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.1695 - acc: 0.5732\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9828 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8736 - acc: 0.6744\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.7012 - acc: 0.7646\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4927 - acc: 0.8902\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3240 - acc: 0.9354\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2089 - acc: 0.9683\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1352 - acc: 0.9890\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0890 - acc: 0.9963\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0617 - acc: 0.9976\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_147 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_147 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_147 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.1618 - acc: 0.5695\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.9887 - acc: 0.6195\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8757 - acc: 0.6329\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.6997 - acc: 0.8000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.4943 - acc: 0.8878\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3246 - acc: 0.9329\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2069 - acc: 0.9671\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1362 - acc: 0.9915\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0903 - acc: 0.9927\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0617 - acc: 0.9988\n",
            "Test Accuracy: 67.317073\n",
            "Round 50 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_148 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_148 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_148 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.4598 - acc: 0.5512\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.1975 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0920 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0622 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0510 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0462 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0427 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0408 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0391 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0373 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_149 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_149 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_149 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.4488 - acc: 0.5963\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.2063 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.1013 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0680 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0546 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0484 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0459 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0435 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0416 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0408 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_150 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_150 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_150 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.4892 - acc: 0.5244\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.2293 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.1011 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 1.0645 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 1.0513 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 1.0467 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 1.0442 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 1.0417 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 1.0398 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.0392 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 51 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_151 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_151 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_151 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.1478 - acc: 0.5768\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0303 - acc: 0.6012\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9960 - acc: 0.6024\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9494 - acc: 0.5976\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8983 - acc: 0.6402\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8234 - acc: 0.6720\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7407 - acc: 0.7439\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6361 - acc: 0.8293\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5481 - acc: 0.8683\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4624 - acc: 0.8902\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_152 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_152 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_152 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 6s - loss: 1.1506 - acc: 0.5707\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0324 - acc: 0.5976\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.9946 - acc: 0.6024\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9519 - acc: 0.6195\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8956 - acc: 0.6439\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8204 - acc: 0.6915\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7326 - acc: 0.7524\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6381 - acc: 0.8183\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5520 - acc: 0.8744\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4671 - acc: 0.8866\n",
            "Test Accuracy: 66.341464\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_153 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_153 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_153 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 1.1536 - acc: 0.5585\n",
            "Epoch 2/10\n",
            " - 0s - loss: 1.0260 - acc: 0.5988\n",
            "Epoch 3/10\n",
            " - 0s - loss: 1.0036 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.9662 - acc: 0.6037\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.9056 - acc: 0.6268\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8338 - acc: 0.6744\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.7476 - acc: 0.7378\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.6538 - acc: 0.8317\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.5611 - acc: 0.8671\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.4769 - acc: 0.8927\n",
            "Test Accuracy: 65.853659\n",
            "Round 52 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_154 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_154 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_154 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 1.2681 - acc: 0.4671\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4696 - acc: 0.8371\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3938 - acc: 0.8407\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3444 - acc: 0.8407\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3018 - acc: 0.8639\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.2621 - acc: 0.9039\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2276 - acc: 0.9390\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1945 - acc: 0.9580\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1654 - acc: 0.9573\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1439 - acc: 0.9534\n",
            "Test Accuracy: 85.170730\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_155 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_155 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.9313 - acc: 0.5632\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.3958 - acc: 0.8346\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3444 - acc: 0.8405\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3108 - acc: 0.8417\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.2748 - acc: 0.8683\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.2396 - acc: 0.9395\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2036 - acc: 0.9590\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1709 - acc: 0.9651\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1461 - acc: 0.9641\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1292 - acc: 0.9544\n",
            "Test Accuracy: 84.585364\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_156 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_156 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 1.0823 - acc: 0.4934\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4760 - acc: 0.8183\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4110 - acc: 0.8466\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3652 - acc: 0.8539\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3309 - acc: 0.8412\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3013 - acc: 0.8437\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.2730 - acc: 0.8715\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.2456 - acc: 0.9371\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.2152 - acc: 0.9517\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1866 - acc: 0.9698\n",
            "Test Accuracy: 85.170730\n",
            "Round 53 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_157 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_157 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_157 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 1.1157 - acc: 0.6839\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4548 - acc: 0.8390\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4089 - acc: 0.8390\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3883 - acc: 0.8393\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3773 - acc: 0.8395\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3705 - acc: 0.8395\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3661 - acc: 0.8395\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3630 - acc: 0.8395\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3607 - acc: 0.8398\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3590 - acc: 0.8398\n",
            "Test Accuracy: 83.609753\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_158 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_158 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_158 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.7560 - acc: 0.7683\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4078 - acc: 0.8402\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3881 - acc: 0.8398\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3787 - acc: 0.8398\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3726 - acc: 0.8398\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3685 - acc: 0.8398\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3653 - acc: 0.8398\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3628 - acc: 0.8398\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3605 - acc: 0.8398\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3587 - acc: 0.8398\n",
            "Test Accuracy: 83.609753\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_159 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_159 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_159 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.8550 - acc: 0.7871\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.4546 - acc: 0.8400\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.3923 - acc: 0.8398\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3729 - acc: 0.8400\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3649 - acc: 0.8400\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3602 - acc: 0.8395\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3584 - acc: 0.8400\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3569 - acc: 0.8400\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3553 - acc: 0.8400\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3542 - acc: 0.8400\n",
            "Test Accuracy: 83.707315\n",
            "Round 54 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_160 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_160 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_160 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 1.3473 - acc: 0.2400\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.5479 - acc: 0.7690\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.4053 - acc: 0.8393\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.3682 - acc: 0.8429\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3513 - acc: 0.8398\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3455 - acc: 0.8432\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3355 - acc: 0.8478\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3276 - acc: 0.8488\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3172 - acc: 0.8546\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3052 - acc: 0.8549\n",
            "Test Accuracy: 83.707316\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_161 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_161 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_161 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 5.1455 - acc: 0.2656\n",
            "Epoch 2/10\n",
            " - 0s - loss: 4.6954 - acc: 0.6344\n",
            "Epoch 3/10\n",
            " - 0s - loss: 4.6273 - acc: 0.6463\n",
            "Epoch 4/10\n",
            " - 0s - loss: 4.6143 - acc: 0.6466\n",
            "Epoch 5/10\n",
            " - 0s - loss: 4.6097 - acc: 0.6463\n",
            "Epoch 6/10\n",
            " - 0s - loss: 4.6080 - acc: 0.6463\n",
            "Epoch 7/10\n",
            " - 0s - loss: 4.6063 - acc: 0.6468\n",
            "Epoch 8/10\n",
            " - 0s - loss: 4.6054 - acc: 0.6468\n",
            "Epoch 9/10\n",
            " - 0s - loss: 4.6032 - acc: 0.6473\n",
            "Epoch 10/10\n",
            " - 0s - loss: 4.6017 - acc: 0.6471\n",
            "Test Accuracy: 64.000002\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_162 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_162 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_162 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 3.6664 - acc: 0.2000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7686 - acc: 0.4868\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5681 - acc: 0.7437\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.4479 - acc: 0.7939\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3829 - acc: 0.8417\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.3524 - acc: 0.8461\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.3394 - acc: 0.8488\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.3286 - acc: 0.8493\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.3206 - acc: 0.8549\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.3058 - acc: 0.8573\n",
            "Test Accuracy: 84.097560\n",
            "Round 55 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_163 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_163 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_163 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 8s - loss: 0.2380 - acc: 0.5037\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1114 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0964 - acc: 0.6024\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0794 - acc: 0.6537\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0604 - acc: 0.8610\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0441 - acc: 0.8720\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0350 - acc: 0.8732\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0304 - acc: 0.8780\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0281 - acc: 0.8890\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0269 - acc: 0.9024\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_164 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_164 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_164 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.2312 - acc: 0.5622\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1112 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0965 - acc: 0.6012\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0798 - acc: 0.6585\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0604 - acc: 0.8634\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0437 - acc: 0.8720\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0341 - acc: 0.8744\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0299 - acc: 0.8780\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0277 - acc: 0.8915\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0264 - acc: 0.9098\n",
            "Test Accuracy: 64.878049\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_165 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_165 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_165 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.2325 - acc: 0.5110\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1109 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0967 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0802 - acc: 0.6695\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0614 - acc: 0.8659\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0450 - acc: 0.8720\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0358 - acc: 0.8732\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0311 - acc: 0.8780\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0282 - acc: 0.8817\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0263 - acc: 0.9049\n",
            "Test Accuracy: 65.853659\n",
            "Round 56 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_166 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_166 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_166 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.5395 - acc: 0.4878\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.2721 - acc: 0.6024\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1844 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1527 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1385 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1309 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1263 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1233 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1213 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1197 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_167 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_167 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_167 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.5643 - acc: 0.4768\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.2846 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1892 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1548 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1395 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1314 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1266 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1235 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1213 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1198 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_168 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_168 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_168 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.5609 - acc: 0.4244\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.2864 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1901 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1554 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1400 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1320 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1272 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1240 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1219 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1203 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 57 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_169 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_169 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_169 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.2317 - acc: 0.5268\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1170 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1133 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1108 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1093 - acc: 0.6012\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1079 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1056 - acc: 0.6024\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1038 - acc: 0.6098\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1018 - acc: 0.6024\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1006 - acc: 0.6073\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_170 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_170 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_170 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.2344 - acc: 0.5488\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1183 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1129 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1111 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1086 - acc: 0.5988\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1082 - acc: 0.6024\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1061 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1036 - acc: 0.6037\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1029 - acc: 0.5976\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1000 - acc: 0.6085\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_171 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_171 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_171 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.2349 - acc: 0.4976\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1199 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.1138 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1108 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.1101 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1073 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1065 - acc: 0.6049\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.1035 - acc: 0.6049\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.1028 - acc: 0.6098\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.1003 - acc: 0.6061\n",
            "Test Accuracy: 58.536586\n",
            "Round 58 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_172 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_172 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_172 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.8617 - acc: 0.5963\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7182 - acc: 0.6012\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5272 - acc: 0.6878\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2365 - acc: 0.9037\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0523 - acc: 0.9829\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 3.0115e-04 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0000e+00 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0000e+00 - acc: 1.0000\n",
            "Test Accuracy: 63.902439\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_173 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_173 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_173 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.8778 - acc: 0.5744\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7247 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5357 - acc: 0.6634\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.2195 - acc: 0.9171\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0253 - acc: 0.9951\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0053 - acc: 0.9988\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0052 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0024 - acc: 0.9988\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_174 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_174 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_174 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 8s - loss: 0.8689 - acc: 0.5793\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7199 - acc: 0.6012\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.5266 - acc: 0.6866\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.1889 - acc: 0.9220\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0258 - acc: 0.9915\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0103 - acc: 0.9976\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0087 - acc: 0.9976\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0055 - acc: 0.9976\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0044 - acc: 0.9988\n",
            "Epoch 10/10\n",
            " - 0s - loss: 1.4168e-04 - acc: 1.0000\n",
            "Test Accuracy: 65.853659\n",
            "Round 59 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_175 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_175 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_175 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.9666 - acc: 0.5598\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8370 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8227 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8157 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8150 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8143 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8092 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8072 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8039 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8048 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_176 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_176 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_176 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.9547 - acc: 0.5768\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8317 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8180 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8187 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8118 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8121 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8092 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8050 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8037 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8081 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_177 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_177 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_177 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 7s - loss: 0.9651 - acc: 0.5610\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8408 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.8190 - acc: 0.6000\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.8157 - acc: 0.6000\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.8125 - acc: 0.6000\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.8106 - acc: 0.6000\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.8072 - acc: 0.6000\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.8051 - acc: 0.6000\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.8063 - acc: 0.6000\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.8044 - acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "Round 60 of 60\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_178 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_178 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_178 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 8s - loss: 0.8860 - acc: 0.5598\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8019 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6977 - acc: 0.6085\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5333 - acc: 0.7232\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3004 - acc: 0.8732\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1690 - acc: 0.9476\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0804 - acc: 0.9720\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0249 - acc: 0.9951\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0149 - acc: 0.9939\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0038 - acc: 0.9988\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_179 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_179 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_179 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 8s - loss: 0.9026 - acc: 0.5902\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.8110 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.6991 - acc: 0.6085\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5329 - acc: 0.7073\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3249 - acc: 0.8720\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1556 - acc: 0.9524\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0690 - acc: 0.9732\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0373 - acc: 0.9866\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0120 - acc: 0.9963\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0062 - acc: 0.9988\n",
            "Test Accuracy: 68.780488\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_180 (Embedding)    (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_180 (Flatten)        (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_180 (Dense)            (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            " - 8s - loss: 0.9138 - acc: 0.5902\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.7969 - acc: 0.6000\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.7143 - acc: 0.6037\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.5677 - acc: 0.6793\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.3445 - acc: 0.8646\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.1728 - acc: 0.9341\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.1000 - acc: 0.9659\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0332 - acc: 0.9915\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0134 - acc: 0.9951\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0124 - acc: 0.9963\n",
            "Test Accuracy: 63.902439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ie-H1KZ-ZNmQ",
        "colab_type": "code",
        "outputId": "ec2870a6-32ba-4d57-f90b-3aa94611dc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2012
        }
      },
      "cell_type": "code",
      "source": [
        "modelsResults"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimensions</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Losses</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>K_Accurasy(+/-sd)</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.3+/-0.5</td>\n",
              "      <td>67.3+/-0.5</td>\n",
              "      <td>27.2+/-1.0</td>\n",
              "      <td>40.3+/-11.2</td>\n",
              "      <td>26.8+/-1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.0+/-0.3</td>\n",
              "      <td>66.0+/-0.3</td>\n",
              "      <td>25.8+/-0.5</td>\n",
              "      <td>47.2+/-0.7</td>\n",
              "      <td>25.3+/-0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.5+/-0.2</td>\n",
              "      <td>68.5+/-1.0</td>\n",
              "      <td>28.6+/-1.0</td>\n",
              "      <td>41.1+/-6.9</td>\n",
              "      <td>28.9+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.0+/-0.1</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>84.6+/-0.1</td>\n",
              "      <td>60.7+/-0.7</td>\n",
              "      <td>20.5+/-0.6</td>\n",
              "      <td>25.4+/-11.6</td>\n",
              "      <td>16.0+/-1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.3+/-1.3</td>\n",
              "      <td>67.3+/-1.3</td>\n",
              "      <td>28.8+/-1.4</td>\n",
              "      <td>43.9+/-4.5</td>\n",
              "      <td>29.5+/-1.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>59.7+/-0.6</td>\n",
              "      <td>59.7+/-0.6</td>\n",
              "      <td>20.1+/-0.2</td>\n",
              "      <td>14.5+/-4.3</td>\n",
              "      <td>15.6+/-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>64.7+/-1.7</td>\n",
              "      <td>64.7+/-1.7</td>\n",
              "      <td>23.9+/-1.4</td>\n",
              "      <td>25.3+/-0.8</td>\n",
              "      <td>21.8+/-2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>59.5+/-0.0</td>\n",
              "      <td>59.5+/-0.0</td>\n",
              "      <td>19.8+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>14.9+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.2+/-0.7</td>\n",
              "      <td>66.2+/-0.7</td>\n",
              "      <td>27.9+/-2.3</td>\n",
              "      <td>41.1+/-5.2</td>\n",
              "      <td>28.3+/-3.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>58.9+/-4.7</td>\n",
              "      <td>58.9+/-4.7</td>\n",
              "      <td>23.5+/-2.8</td>\n",
              "      <td>32.6+/-11.6</td>\n",
              "      <td>20.9+/-4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>86.5+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>26.9+/-0.6</td>\n",
              "      <td>40.7+/-12.3</td>\n",
              "      <td>26.5+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>83.8+/-0.2</td>\n",
              "      <td>59.2+/-0.6</td>\n",
              "      <td>20.3+/-1.0</td>\n",
              "      <td>15.1+/-5.6</td>\n",
              "      <td>16.3+/-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.5+/-1.2</td>\n",
              "      <td>66.5+/-1.2</td>\n",
              "      <td>27.1+/-1.5</td>\n",
              "      <td>46.7+/-1.3</td>\n",
              "      <td>27.3+/-2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>sgd</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>63.9+/-3.4</td>\n",
              "      <td>63.9+/-3.4</td>\n",
              "      <td>23.0+/-2.6</td>\n",
              "      <td>21.8+/-8.5</td>\n",
              "      <td>20.1+/-4.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>59.7+/-0.3</td>\n",
              "      <td>59.7+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>14.9+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>01.4+/-0.0</td>\n",
              "      <td>02.6+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>42.3+/-30.7</td>\n",
              "      <td>42.3+/-30.7</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>08.5+/-6.1</td>\n",
              "      <td>10.9+/-7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>01.4+/-0.0</td>\n",
              "      <td>02.6+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>85.3+/-0.6</td>\n",
              "      <td>62.4+/-1.8</td>\n",
              "      <td>23.5+/-1.9</td>\n",
              "      <td>26.7+/-6.1</td>\n",
              "      <td>21.6+/-3.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>64.8+/-32.8</td>\n",
              "      <td>40.7+/-33.5</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>08.1+/-6.7</td>\n",
              "      <td>10.3+/-8.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>85.8+/-1.4</td>\n",
              "      <td>62.4+/-2.1</td>\n",
              "      <td>22.0+/-1.5</td>\n",
              "      <td>27.0+/-5.2</td>\n",
              "      <td>18.8+/-2.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>60.3+/-0.3</td>\n",
              "      <td>60.3+/-0.3</td>\n",
              "      <td>21.0+/-0.8</td>\n",
              "      <td>16.6+/-3.9</td>\n",
              "      <td>16.7+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>sgd</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>25.7+/-31.8</td>\n",
              "      <td>25.7+/-31.8</td>\n",
              "      <td>22.3+/-2.5</td>\n",
              "      <td>25.0+/-6.3</td>\n",
              "      <td>10.7+/-7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>63.1+/-3.7</td>\n",
              "      <td>63.1+/-3.7</td>\n",
              "      <td>24.2+/-2.3</td>\n",
              "      <td>20.3+/-5.4</td>\n",
              "      <td>21.2+/-3.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>sgd</td>\n",
              "      <td>59.7+/-0.3</td>\n",
              "      <td>59.7+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>14.9+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>27.2+/-34.8</td>\n",
              "      <td>27.2+/-34.8</td>\n",
              "      <td>22.8+/-4.7</td>\n",
              "      <td>16.0+/-12.9</td>\n",
              "      <td>10.7+/-13.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>36.7+/-24.2</td>\n",
              "      <td>36.7+/-24.2</td>\n",
              "      <td>20.7+/-1.5</td>\n",
              "      <td>13.8+/-4.4</td>\n",
              "      <td>13.4+/-8.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>22.8+/-31.8</td>\n",
              "      <td>22.8+/-31.8</td>\n",
              "      <td>20.9+/-1.7</td>\n",
              "      <td>04.8+/-6.2</td>\n",
              "      <td>06.5+/-7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>15.9+/-11.8</td>\n",
              "      <td>15.9+/-11.8</td>\n",
              "      <td>22.1+/-2.0</td>\n",
              "      <td>06.9+/-5.9</td>\n",
              "      <td>06.8+/-3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>22.3+/-7.9</td>\n",
              "      <td>62.8+/-2.4</td>\n",
              "      <td>23.1+/-2.7</td>\n",
              "      <td>20.7+/-7.5</td>\n",
              "      <td>20.5+/-4.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>63.9+/-21.3</td>\n",
              "      <td>41.3+/-32.0</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>08.3+/-6.4</td>\n",
              "      <td>10.5+/-7.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.2+/-5.6</td>\n",
              "      <td>63.3+/-1.2</td>\n",
              "      <td>22.8+/-1.5</td>\n",
              "      <td>38.3+/-12.4</td>\n",
              "      <td>20.3+/-2.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.5+/-2.0</td>\n",
              "      <td>66.5+/-2.0</td>\n",
              "      <td>27.7+/-1.2</td>\n",
              "      <td>47.4+/-0.8</td>\n",
              "      <td>28.6+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>sgd</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.3+/-0.8</td>\n",
              "      <td>66.3+/-0.8</td>\n",
              "      <td>26.8+/-0.5</td>\n",
              "      <td>47.6+/-1.5</td>\n",
              "      <td>26.9+/-0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>30.5+/-0.4</td>\n",
              "      <td>44.7+/-3.3</td>\n",
              "      <td>32.0+/-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.7+/-1.2</td>\n",
              "      <td>66.7+/-1.2</td>\n",
              "      <td>27.6+/-2.9</td>\n",
              "      <td>45.4+/-4.0</td>\n",
              "      <td>27.9+/-3.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.6+/-0.3</td>\n",
              "      <td>67.6+/-0.3</td>\n",
              "      <td>28.0+/-0.1</td>\n",
              "      <td>46.7+/-0.1</td>\n",
              "      <td>28.1+/-0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>67.2+/-0.6</td>\n",
              "      <td>67.2+/-0.6</td>\n",
              "      <td>27.5+/-0.9</td>\n",
              "      <td>44.9+/-6.0</td>\n",
              "      <td>27.9+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>81.7+/-0.1</td>\n",
              "      <td>63.9+/-2.0</td>\n",
              "      <td>25.4+/-2.3</td>\n",
              "      <td>42.8+/-6.3</td>\n",
              "      <td>24.7+/-3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>79.6+/-6.7</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>84.1+/-0.5</td>\n",
              "      <td>60.8+/-1.2</td>\n",
              "      <td>20.7+/-0.8</td>\n",
              "      <td>24.1+/-10.7</td>\n",
              "      <td>16.4+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>62.6+/-2.3</td>\n",
              "      <td>62.6+/-2.3</td>\n",
              "      <td>22.3+/-2.1</td>\n",
              "      <td>21.6+/-8.4</td>\n",
              "      <td>19.4+/-3.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>61.1+/-2.9</td>\n",
              "      <td>61.1+/-2.9</td>\n",
              "      <td>22.6+/-1.4</td>\n",
              "      <td>27.9+/-5.9</td>\n",
              "      <td>20.0+/-2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>64.9+/-3.0</td>\n",
              "      <td>64.9+/-3.0</td>\n",
              "      <td>31.8+/-2.9</td>\n",
              "      <td>41.3+/-5.5</td>\n",
              "      <td>33.2+/-3.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>sgd</td>\n",
              "      <td>59.7+/-0.3</td>\n",
              "      <td>59.7+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>14.9+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>29.3+/-2.1</td>\n",
              "      <td>47.2+/-1.5</td>\n",
              "      <td>30.3+/-2.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Dimensions   Activation                    Losses Optimizer  \\\n",
              "0           100      softmax  categorical_crossentropy      adam   \n",
              "1           100      softmax  categorical_crossentropy       sgd   \n",
              "2           100      softmax  categorical_crossentropy  Adadelta   \n",
              "3           100      softmax       binary_crossentropy      adam   \n",
              "4           100      softmax       binary_crossentropy       sgd   \n",
              "5           100      softmax       binary_crossentropy  Adadelta   \n",
              "6           100      softmax        mean_squared_error      adam   \n",
              "7           100      softmax        mean_squared_error       sgd   \n",
              "8           100      softmax        mean_squared_error  Adadelta   \n",
              "9           100      softmax         categorical_hinge      adam   \n",
              "10          100      softmax         categorical_hinge       sgd   \n",
              "11          100      softmax         categorical_hinge  Adadelta   \n",
              "12          100      sigmoid  categorical_crossentropy      adam   \n",
              "13          100      sigmoid  categorical_crossentropy       sgd   \n",
              "14          100      sigmoid  categorical_crossentropy  Adadelta   \n",
              "15          100      sigmoid       binary_crossentropy      adam   \n",
              "16          100      sigmoid       binary_crossentropy       sgd   \n",
              "17          100      sigmoid       binary_crossentropy  Adadelta   \n",
              "18          100      sigmoid        mean_squared_error      adam   \n",
              "19          100      sigmoid        mean_squared_error       sgd   \n",
              "20          100      sigmoid        mean_squared_error  Adadelta   \n",
              "21          100      sigmoid         categorical_hinge      adam   \n",
              "22          100      sigmoid         categorical_hinge       sgd   \n",
              "23          100      sigmoid         categorical_hinge  Adadelta   \n",
              "24          100         relu  categorical_crossentropy      adam   \n",
              "25          100         relu  categorical_crossentropy       sgd   \n",
              "26          100         relu  categorical_crossentropy  Adadelta   \n",
              "27          100         relu       binary_crossentropy      adam   \n",
              "28          100         relu       binary_crossentropy       sgd   \n",
              "29          100         relu       binary_crossentropy  Adadelta   \n",
              "..          ...          ...                       ...       ...   \n",
              "150        1000         relu        mean_squared_error      adam   \n",
              "151        1000         relu        mean_squared_error       sgd   \n",
              "152        1000         relu        mean_squared_error  Adadelta   \n",
              "153        1000         relu         categorical_hinge      adam   \n",
              "154        1000         relu         categorical_hinge       sgd   \n",
              "155        1000         relu         categorical_hinge  Adadelta   \n",
              "156        1000         tanh  categorical_crossentropy      adam   \n",
              "157        1000         tanh  categorical_crossentropy       sgd   \n",
              "158        1000         tanh  categorical_crossentropy  Adadelta   \n",
              "159        1000         tanh       binary_crossentropy      adam   \n",
              "160        1000         tanh       binary_crossentropy       sgd   \n",
              "161        1000         tanh       binary_crossentropy  Adadelta   \n",
              "162        1000         tanh        mean_squared_error      adam   \n",
              "163        1000         tanh        mean_squared_error       sgd   \n",
              "164        1000         tanh        mean_squared_error  Adadelta   \n",
              "165        1000         tanh         categorical_hinge      adam   \n",
              "166        1000         tanh         categorical_hinge       sgd   \n",
              "167        1000         tanh         categorical_hinge  Adadelta   \n",
              "168        1000  exponential  categorical_crossentropy      adam   \n",
              "169        1000  exponential  categorical_crossentropy       sgd   \n",
              "170        1000  exponential  categorical_crossentropy  Adadelta   \n",
              "171        1000  exponential       binary_crossentropy      adam   \n",
              "172        1000  exponential       binary_crossentropy       sgd   \n",
              "173        1000  exponential       binary_crossentropy  Adadelta   \n",
              "174        1000  exponential        mean_squared_error      adam   \n",
              "175        1000  exponential        mean_squared_error       sgd   \n",
              "176        1000  exponential        mean_squared_error  Adadelta   \n",
              "177        1000  exponential         categorical_hinge      adam   \n",
              "178        1000  exponential         categorical_hinge       sgd   \n",
              "179        1000  exponential         categorical_hinge  Adadelta   \n",
              "\n",
              "    K_Accurasy(+/-sd) Accurasy(+/-sd) Recall(+/-sd) Precision(+/-sd)  \\\n",
              "0          67.3+/-0.5      67.3+/-0.5    27.2+/-1.0      40.3+/-11.2   \n",
              "1          60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "2          66.0+/-0.3      66.0+/-0.3    25.8+/-0.5       47.2+/-0.7   \n",
              "3          87.5+/-0.2      68.5+/-1.0    28.6+/-1.0       41.1+/-6.9   \n",
              "4          84.0+/-0.1      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "5          84.6+/-0.1      60.7+/-0.7    20.5+/-0.6      25.4+/-11.6   \n",
              "6          67.3+/-1.3      67.3+/-1.3    28.8+/-1.4       43.9+/-4.5   \n",
              "7          60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "8          59.7+/-0.6      59.7+/-0.6    20.1+/-0.2       14.5+/-4.3   \n",
              "9          64.7+/-1.7      64.7+/-1.7    23.9+/-1.4       25.3+/-0.8   \n",
              "10         60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "11         59.5+/-0.0      59.5+/-0.0    19.8+/-0.0       12.0+/-0.0   \n",
              "12         66.2+/-0.7      66.2+/-0.7    27.9+/-2.3       41.1+/-5.2   \n",
              "13         60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "14         58.9+/-4.7      58.9+/-4.7    23.5+/-2.8      32.6+/-11.6   \n",
              "15         86.5+/-0.5      66.8+/-0.5    26.9+/-0.6      40.7+/-12.3   \n",
              "16         84.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "17         83.8+/-0.2      59.2+/-0.6    20.3+/-1.0       15.1+/-5.6   \n",
              "18         66.5+/-1.2      66.5+/-1.2    27.1+/-1.5       46.7+/-1.3   \n",
              "19         59.8+/-0.3      59.8+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "20         60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "21         63.9+/-3.4      63.9+/-3.4    23.0+/-2.6       21.8+/-8.5   \n",
              "22         60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "23         59.7+/-0.3      59.7+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "24         06.8+/-0.0      06.8+/-0.0    20.0+/-0.0       01.4+/-0.0   \n",
              "25        42.3+/-30.7     42.3+/-30.7    20.0+/-0.0       08.5+/-6.1   \n",
              "26         06.8+/-0.0      06.8+/-0.0    20.0+/-0.0       01.4+/-0.0   \n",
              "27         85.3+/-0.6      62.4+/-1.8    23.5+/-1.9       26.7+/-6.1   \n",
              "28        64.8+/-32.8     40.7+/-33.5    20.0+/-0.0       08.1+/-6.7   \n",
              "29         85.8+/-1.4      62.4+/-2.1    22.0+/-1.5       27.0+/-5.2   \n",
              "..                ...             ...           ...              ...   \n",
              "150        60.3+/-0.3      60.3+/-0.3    21.0+/-0.8       16.6+/-3.9   \n",
              "151        59.8+/-0.3      59.8+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "152       25.7+/-31.8     25.7+/-31.8    22.3+/-2.5       25.0+/-6.3   \n",
              "153        63.1+/-3.7      63.1+/-3.7    24.2+/-2.3       20.3+/-5.4   \n",
              "154        59.7+/-0.3      59.7+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "155       27.2+/-34.8     27.2+/-34.8    22.8+/-4.7      16.0+/-12.9   \n",
              "156       36.7+/-24.2     36.7+/-24.2    20.7+/-1.5       13.8+/-4.4   \n",
              "157       22.8+/-31.8     22.8+/-31.8    20.9+/-1.7       04.8+/-6.2   \n",
              "158       15.9+/-11.8     15.9+/-11.8    22.1+/-2.0       06.9+/-5.9   \n",
              "159        22.3+/-7.9      62.8+/-2.4    23.1+/-2.7       20.7+/-7.5   \n",
              "160       63.9+/-21.3     41.3+/-32.0    19.9+/-0.1       08.3+/-6.4   \n",
              "161        66.2+/-5.6      63.3+/-1.2    22.8+/-1.5      38.3+/-12.4   \n",
              "162        66.5+/-2.0      66.5+/-2.0    27.7+/-1.2       47.4+/-0.8   \n",
              "163        59.8+/-0.3      59.8+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "164        66.3+/-0.8      66.3+/-0.8    26.8+/-0.5       47.6+/-1.5   \n",
              "165        68.9+/-0.6      68.9+/-0.6    30.5+/-0.4       44.7+/-3.3   \n",
              "166        60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "167        66.7+/-1.2      66.7+/-1.2    27.6+/-2.9       45.4+/-4.0   \n",
              "168        67.6+/-0.3      67.6+/-0.3    28.0+/-0.1       46.7+/-0.1   \n",
              "169        59.8+/-0.3      59.8+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "170        67.2+/-0.6      67.2+/-0.6    27.5+/-0.9       44.9+/-6.0   \n",
              "171        81.7+/-0.1      63.9+/-2.0    25.4+/-2.3       42.8+/-6.3   \n",
              "172        79.6+/-6.7      59.8+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "173        84.1+/-0.5      60.8+/-1.2    20.7+/-0.8      24.1+/-10.7   \n",
              "174        62.6+/-2.3      62.6+/-2.3    22.3+/-2.1       21.6+/-8.4   \n",
              "175        60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "176        61.1+/-2.9      61.1+/-2.9    22.6+/-1.4       27.9+/-5.9   \n",
              "177        64.9+/-3.0      64.9+/-3.0    31.8+/-2.9       41.3+/-5.5   \n",
              "178        59.7+/-0.3      59.7+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "179        68.1+/-0.3      68.1+/-0.3    29.3+/-2.1       47.2+/-1.5   \n",
              "\n",
              "    F1_score(+/-sd)  \n",
              "0        26.8+/-1.6  \n",
              "1        15.0+/-0.0  \n",
              "2        25.3+/-0.8  \n",
              "3        28.9+/-1.4  \n",
              "4        15.0+/-0.0  \n",
              "5        16.0+/-1.1  \n",
              "6        29.5+/-1.8  \n",
              "7        15.0+/-0.0  \n",
              "8        15.6+/-1.0  \n",
              "9        21.8+/-2.0  \n",
              "10       15.0+/-0.0  \n",
              "11       14.9+/-0.0  \n",
              "12       28.3+/-3.2  \n",
              "13       15.0+/-0.0  \n",
              "14       20.9+/-4.6  \n",
              "15       26.5+/-1.3  \n",
              "16       15.0+/-0.0  \n",
              "17       16.3+/-2.5  \n",
              "18       27.3+/-2.3  \n",
              "19       15.0+/-0.0  \n",
              "20       15.0+/-0.0  \n",
              "21       20.1+/-4.4  \n",
              "22       15.0+/-0.0  \n",
              "23       14.9+/-0.0  \n",
              "24       02.6+/-0.0  \n",
              "25       10.9+/-7.2  \n",
              "26       02.6+/-0.0  \n",
              "27       21.6+/-3.6  \n",
              "28       10.3+/-8.2  \n",
              "29       18.8+/-2.7  \n",
              "..              ...  \n",
              "150      16.7+/-1.4  \n",
              "151      15.0+/-0.0  \n",
              "152      10.7+/-7.8  \n",
              "153      21.2+/-3.9  \n",
              "154      14.9+/-0.0  \n",
              "155     10.7+/-13.8  \n",
              "156      13.4+/-8.2  \n",
              "157      06.5+/-7.3  \n",
              "158      06.8+/-3.0  \n",
              "159      20.5+/-4.7  \n",
              "160      10.5+/-7.7  \n",
              "161      20.3+/-2.7  \n",
              "162      28.6+/-1.3  \n",
              "163      15.0+/-0.0  \n",
              "164      26.9+/-0.9  \n",
              "165      32.0+/-0.3  \n",
              "166      15.0+/-0.0  \n",
              "167      27.9+/-3.8  \n",
              "168      28.1+/-0.1  \n",
              "169      15.0+/-0.0  \n",
              "170      27.9+/-1.4  \n",
              "171      24.7+/-3.3  \n",
              "172      15.0+/-0.0  \n",
              "173      16.4+/-1.5  \n",
              "174      19.4+/-3.9  \n",
              "175      15.0+/-0.0  \n",
              "176      20.0+/-2.6  \n",
              "177      33.2+/-3.7  \n",
              "178      14.9+/-0.0  \n",
              "179      30.3+/-2.8  \n",
              "\n",
              "[180 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "9RMaNDoZOC0F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.5 Save results to Google Drive\n",
        "\n",
        "The evaluation of all these models is an extremely long procedure (> 4h).\n",
        "\n",
        "I strore the DataFrame with final results as a .csv file on Drive to access them locally.\n",
        "\n",
        "You can download the DataFrame from this link: [modelsResults.csv](https://)"
      ]
    },
    {
      "metadata": {
        "id": "BujbttTKXDWY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.4.5.1 Save DataFrame to Drive as a .csv file"
      ]
    },
    {
      "metadata": {
        "id": "dzYL8sl1OD82",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "total_results_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/modelsResults.csv'\n",
        "total.to_csv(total_results_filename, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PgPjg2CLXTJ2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.4.5.2 Recover results from .csv file to DataFrame format"
      ]
    },
    {
      "metadata": {
        "id": "0ZT4uj-2XSvQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "total_results_filename = '/content/drive/My Drive/NLP Assignment 2/Part3/modelsResults.csv'\n",
        "\n",
        "modelsResults = pd.read_csv(total_results_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-HpVxQrZYBW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.6 Evaluate models"
      ]
    },
    {
      "metadata": {
        "id": "-nHMl-6aZXts",
        "colab_type": "code",
        "outputId": "306a4332-2ec4-4d17-9aff-b1ed6f3e96f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1638
        }
      },
      "cell_type": "code",
      "source": [
        "# Sort results according to test set Accuracy reported from Keras\n",
        "\n",
        "modelsResults.sort_values('K_Accurasy(+/-sd)',ascending=False)[:50]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimensions</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Losses</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>K_Accurasy(+/-sd)</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.7+/-0.2</td>\n",
              "      <td>69.1+/-0.3</td>\n",
              "      <td>28.9+/-0.1</td>\n",
              "      <td>48.0+/-0.6</td>\n",
              "      <td>29.1+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.5+/-0.2</td>\n",
              "      <td>68.5+/-1.0</td>\n",
              "      <td>28.6+/-1.0</td>\n",
              "      <td>41.1+/-6.9</td>\n",
              "      <td>28.9+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.3+/-0.3</td>\n",
              "      <td>68.3+/-0.5</td>\n",
              "      <td>28.6+/-0.4</td>\n",
              "      <td>47.3+/-0.1</td>\n",
              "      <td>28.8+/-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.1+/-0.4</td>\n",
              "      <td>67.6+/-0.6</td>\n",
              "      <td>27.8+/-0.4</td>\n",
              "      <td>47.0+/-0.8</td>\n",
              "      <td>27.9+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>86.9+/-0.2</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>27.9+/-0.2</td>\n",
              "      <td>47.7+/-0.8</td>\n",
              "      <td>28.1+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>86.5+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>26.9+/-0.6</td>\n",
              "      <td>40.7+/-12.3</td>\n",
              "      <td>26.5+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>86.1+/-0.6</td>\n",
              "      <td>64.2+/-1.5</td>\n",
              "      <td>24.6+/-1.5</td>\n",
              "      <td>48.1+/-1.3</td>\n",
              "      <td>23.4+/-2.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>86.0+/-0.7</td>\n",
              "      <td>63.4+/-1.3</td>\n",
              "      <td>23.3+/-1.8</td>\n",
              "      <td>26.0+/-2.1</td>\n",
              "      <td>20.8+/-2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>86.0+/-0.2</td>\n",
              "      <td>61.6+/-2.8</td>\n",
              "      <td>23.0+/-5.2</td>\n",
              "      <td>22.9+/-18.5</td>\n",
              "      <td>19.6+/-7.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>85.8+/-1.4</td>\n",
              "      <td>62.4+/-2.1</td>\n",
              "      <td>22.0+/-1.5</td>\n",
              "      <td>27.0+/-5.2</td>\n",
              "      <td>18.8+/-2.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>85.8+/-0.6</td>\n",
              "      <td>63.6+/-0.7</td>\n",
              "      <td>22.7+/-0.7</td>\n",
              "      <td>29.6+/-0.3</td>\n",
              "      <td>20.1+/-1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>100</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>85.7+/-0.4</td>\n",
              "      <td>63.3+/-1.7</td>\n",
              "      <td>22.6+/-1.5</td>\n",
              "      <td>27.8+/-1.9</td>\n",
              "      <td>19.7+/-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>85.6+/-2.0</td>\n",
              "      <td>64.1+/-3.1</td>\n",
              "      <td>24.4+/-2.9</td>\n",
              "      <td>28.7+/-10.5</td>\n",
              "      <td>22.7+/-5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>85.6+/-0.3</td>\n",
              "      <td>64.7+/-2.3</td>\n",
              "      <td>27.4+/-2.0</td>\n",
              "      <td>28.5+/-8.8</td>\n",
              "      <td>26.6+/-3.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>85.3+/-0.6</td>\n",
              "      <td>62.4+/-1.8</td>\n",
              "      <td>23.5+/-1.9</td>\n",
              "      <td>26.7+/-6.1</td>\n",
              "      <td>21.6+/-3.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>85.0+/-0.3</td>\n",
              "      <td>63.9+/-1.0</td>\n",
              "      <td>23.4+/-1.2</td>\n",
              "      <td>27.3+/-2.1</td>\n",
              "      <td>21.3+/-1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>84.9+/-0.3</td>\n",
              "      <td>61.6+/-1.1</td>\n",
              "      <td>21.3+/-0.9</td>\n",
              "      <td>30.8+/-2.4</td>\n",
              "      <td>17.8+/-1.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>84.6+/-0.1</td>\n",
              "      <td>60.7+/-0.7</td>\n",
              "      <td>20.5+/-0.6</td>\n",
              "      <td>25.4+/-11.6</td>\n",
              "      <td>16.0+/-1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>84.4+/-0.4</td>\n",
              "      <td>60.2+/-0.7</td>\n",
              "      <td>20.2+/-0.5</td>\n",
              "      <td>18.7+/-11.6</td>\n",
              "      <td>15.5+/-0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>84.1+/-0.5</td>\n",
              "      <td>60.8+/-1.2</td>\n",
              "      <td>20.7+/-0.8</td>\n",
              "      <td>24.1+/-10.7</td>\n",
              "      <td>16.4+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.1+/-0.1</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.1+/-0.1</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.0+/-0.1</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.0+/-0.1</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>84.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>83.9+/-0.1</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>83.8+/-0.2</td>\n",
              "      <td>59.2+/-0.6</td>\n",
              "      <td>20.3+/-1.0</td>\n",
              "      <td>15.1+/-5.6</td>\n",
              "      <td>16.3+/-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>83.7+/-0.1</td>\n",
              "      <td>61.0+/-1.3</td>\n",
              "      <td>20.7+/-1.0</td>\n",
              "      <td>20.3+/-7.5</td>\n",
              "      <td>16.4+/-1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>83.6+/-0.1</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>83.6+/-0.1</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>82.5+/-0.3</td>\n",
              "      <td>65.2+/-1.2</td>\n",
              "      <td>25.9+/-2.3</td>\n",
              "      <td>37.4+/-10.8</td>\n",
              "      <td>25.3+/-3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>81.7+/-0.1</td>\n",
              "      <td>63.9+/-2.0</td>\n",
              "      <td>25.4+/-2.3</td>\n",
              "      <td>42.8+/-6.3</td>\n",
              "      <td>24.7+/-3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>79.6+/-6.7</td>\n",
              "      <td>59.8+/-0.3</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>77.3+/-11.5</td>\n",
              "      <td>54.5+/-9.6</td>\n",
              "      <td>18.2+/-3.2</td>\n",
              "      <td>11.9+/-0.1</td>\n",
              "      <td>14.3+/-1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>71.7+/-10.9</td>\n",
              "      <td>31.2+/-26.9</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>06.2+/-5.4</td>\n",
              "      <td>08.7+/-6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>71.2+/-13.3</td>\n",
              "      <td>49.8+/-19.9</td>\n",
              "      <td>22.1+/-3.9</td>\n",
              "      <td>13.6+/-9.1</td>\n",
              "      <td>16.1+/-8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>70.7+/-1.0</td>\n",
              "      <td>70.7+/-1.0</td>\n",
              "      <td>32.6+/-2.0</td>\n",
              "      <td>45.6+/-3.1</td>\n",
              "      <td>34.3+/-2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>30.5+/-0.4</td>\n",
              "      <td>44.7+/-3.3</td>\n",
              "      <td>32.0+/-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.6+/-1.2</td>\n",
              "      <td>68.6+/-1.2</td>\n",
              "      <td>29.1+/-1.1</td>\n",
              "      <td>47.1+/-0.5</td>\n",
              "      <td>29.6+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.6+/-0.7</td>\n",
              "      <td>68.6+/-0.7</td>\n",
              "      <td>29.2+/-0.9</td>\n",
              "      <td>45.2+/-4.2</td>\n",
              "      <td>29.6+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>28.9+/-1.5</td>\n",
              "      <td>47.2+/-0.5</td>\n",
              "      <td>29.3+/-2.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-1.7</td>\n",
              "      <td>68.3+/-1.7</td>\n",
              "      <td>29.4+/-2.1</td>\n",
              "      <td>47.6+/-1.0</td>\n",
              "      <td>30.2+/-2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-1.3</td>\n",
              "      <td>68.3+/-1.3</td>\n",
              "      <td>28.3+/-0.9</td>\n",
              "      <td>47.7+/-0.5</td>\n",
              "      <td>28.4+/-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-0.8</td>\n",
              "      <td>68.3+/-0.8</td>\n",
              "      <td>28.8+/-0.7</td>\n",
              "      <td>43.8+/-4.0</td>\n",
              "      <td>29.7+/-1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.1+/-2.0</td>\n",
              "      <td>68.1+/-2.0</td>\n",
              "      <td>29.5+/-1.8</td>\n",
              "      <td>42.9+/-4.2</td>\n",
              "      <td>30.2+/-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>29.3+/-2.1</td>\n",
              "      <td>47.2+/-1.5</td>\n",
              "      <td>30.3+/-2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.0+/-1.1</td>\n",
              "      <td>68.0+/-1.1</td>\n",
              "      <td>28.5+/-1.7</td>\n",
              "      <td>47.9+/-0.5</td>\n",
              "      <td>29.1+/-2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.0+/-1.0</td>\n",
              "      <td>68.0+/-1.0</td>\n",
              "      <td>28.9+/-0.4</td>\n",
              "      <td>47.9+/-2.0</td>\n",
              "      <td>30.0+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>27.9+/-0.4</td>\n",
              "      <td>47.8+/-0.7</td>\n",
              "      <td>28.1+/-0.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Dimensions   Activation                    Losses Optimizer  \\\n",
              "63          500      softmax       binary_crossentropy      adam   \n",
              "3           100      softmax       binary_crossentropy      adam   \n",
              "123        1000      softmax       binary_crossentropy      adam   \n",
              "135        1000      sigmoid       binary_crossentropy      adam   \n",
              "75          500      sigmoid       binary_crossentropy      adam   \n",
              "15          100      sigmoid       binary_crossentropy      adam   \n",
              "125        1000      softmax       binary_crossentropy  Adadelta   \n",
              "65          500      softmax       binary_crossentropy  Adadelta   \n",
              "149        1000         relu       binary_crossentropy  Adadelta   \n",
              "29          100         relu       binary_crossentropy  Adadelta   \n",
              "101         500         tanh       binary_crossentropy  Adadelta   \n",
              "41          100         tanh       binary_crossentropy  Adadelta   \n",
              "89          500         relu       binary_crossentropy  Adadelta   \n",
              "87          500         relu       binary_crossentropy      adam   \n",
              "27          100         relu       binary_crossentropy      adam   \n",
              "51          100  exponential       binary_crossentropy      adam   \n",
              "137        1000      sigmoid       binary_crossentropy  Adadelta   \n",
              "5           100      softmax       binary_crossentropy  Adadelta   \n",
              "77          500      sigmoid       binary_crossentropy  Adadelta   \n",
              "173        1000  exponential       binary_crossentropy  Adadelta   \n",
              "100         500         tanh       binary_crossentropy       sgd   \n",
              "124        1000      softmax       binary_crossentropy       sgd   \n",
              "4           100      softmax       binary_crossentropy       sgd   \n",
              "64          500      softmax       binary_crossentropy       sgd   \n",
              "16          100      sigmoid       binary_crossentropy       sgd   \n",
              "76          500      sigmoid       binary_crossentropy       sgd   \n",
              "136        1000      sigmoid       binary_crossentropy       sgd   \n",
              "17          100      sigmoid       binary_crossentropy  Adadelta   \n",
              "113         500  exponential       binary_crossentropy  Adadelta   \n",
              "52          100  exponential       binary_crossentropy       sgd   \n",
              "112         500  exponential       binary_crossentropy       sgd   \n",
              "111         500  exponential       binary_crossentropy      adam   \n",
              "171        1000  exponential       binary_crossentropy      adam   \n",
              "172        1000  exponential       binary_crossentropy       sgd   \n",
              "53          100  exponential       binary_crossentropy  Adadelta   \n",
              "88          500         relu       binary_crossentropy       sgd   \n",
              "147        1000         relu       binary_crossentropy      adam   \n",
              "141        1000      sigmoid         categorical_hinge      adam   \n",
              "165        1000         tanh         categorical_hinge      adam   \n",
              "66          500      softmax        mean_squared_error      adam   \n",
              "126        1000      softmax        mean_squared_error      adam   \n",
              "60          500      softmax  categorical_crossentropy      adam   \n",
              "132        1000      sigmoid  categorical_crossentropy      adam   \n",
              "78          500      sigmoid        mean_squared_error      adam   \n",
              "105         500         tanh         categorical_hinge      adam   \n",
              "81          500      sigmoid         categorical_hinge      adam   \n",
              "179        1000  exponential         categorical_hinge  Adadelta   \n",
              "110         500  exponential  categorical_crossentropy  Adadelta   \n",
              "62          500      softmax  categorical_crossentropy  Adadelta   \n",
              "138        1000      sigmoid        mean_squared_error      adam   \n",
              "\n",
              "    K_Accurasy(+/-sd) Accurasy(+/-sd) Recall(+/-sd) Precision(+/-sd)  \\\n",
              "63         87.7+/-0.2      69.1+/-0.3    28.9+/-0.1       48.0+/-0.6   \n",
              "3          87.5+/-0.2      68.5+/-1.0    28.6+/-1.0       41.1+/-6.9   \n",
              "123        87.3+/-0.3      68.3+/-0.5    28.6+/-0.4       47.3+/-0.1   \n",
              "135        87.1+/-0.4      67.6+/-0.6    27.8+/-0.4       47.0+/-0.8   \n",
              "75         86.9+/-0.2      67.8+/-0.5    27.9+/-0.2       47.7+/-0.8   \n",
              "15         86.5+/-0.5      66.8+/-0.5    26.9+/-0.6      40.7+/-12.3   \n",
              "125        86.1+/-0.6      64.2+/-1.5    24.6+/-1.5       48.1+/-1.3   \n",
              "65         86.0+/-0.7      63.4+/-1.3    23.3+/-1.8       26.0+/-2.1   \n",
              "149        86.0+/-0.2      61.6+/-2.8    23.0+/-5.2      22.9+/-18.5   \n",
              "29         85.8+/-1.4      62.4+/-2.1    22.0+/-1.5       27.0+/-5.2   \n",
              "101        85.8+/-0.6      63.6+/-0.7    22.7+/-0.7       29.6+/-0.3   \n",
              "41         85.7+/-0.4      63.3+/-1.7    22.6+/-1.5       27.8+/-1.9   \n",
              "89         85.6+/-2.0      64.1+/-3.1    24.4+/-2.9      28.7+/-10.5   \n",
              "87         85.6+/-0.3      64.7+/-2.3    27.4+/-2.0       28.5+/-8.8   \n",
              "27         85.3+/-0.6      62.4+/-1.8    23.5+/-1.9       26.7+/-6.1   \n",
              "51         85.0+/-0.3      63.9+/-1.0    23.4+/-1.2       27.3+/-2.1   \n",
              "137        84.9+/-0.3      61.6+/-1.1    21.3+/-0.9       30.8+/-2.4   \n",
              "5          84.6+/-0.1      60.7+/-0.7    20.5+/-0.6      25.4+/-11.6   \n",
              "77         84.4+/-0.4      60.2+/-0.7    20.2+/-0.5      18.7+/-11.6   \n",
              "173        84.1+/-0.5      60.8+/-1.2    20.7+/-0.8      24.1+/-10.7   \n",
              "100        84.1+/-0.1      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "124        84.1+/-0.1      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "4          84.0+/-0.1      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "64         84.0+/-0.1      59.8+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "16         84.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "76         84.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "136        83.9+/-0.1      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "17         83.8+/-0.2      59.2+/-0.6    20.3+/-1.0       15.1+/-5.6   \n",
              "113        83.7+/-0.1      61.0+/-1.3    20.7+/-1.0       20.3+/-7.5   \n",
              "52         83.6+/-0.1      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "112        83.6+/-0.1      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "111        82.5+/-0.3      65.2+/-1.2    25.9+/-2.3      37.4+/-10.8   \n",
              "171        81.7+/-0.1      63.9+/-2.0    25.4+/-2.3       42.8+/-6.3   \n",
              "172        79.6+/-6.7      59.8+/-0.3    19.9+/-0.1       12.0+/-0.0   \n",
              "53        77.3+/-11.5      54.5+/-9.6    18.2+/-3.2       11.9+/-0.1   \n",
              "88        71.7+/-10.9     31.2+/-26.9    20.0+/-0.0       06.2+/-5.4   \n",
              "147       71.2+/-13.3     49.8+/-19.9    22.1+/-3.9       13.6+/-9.1   \n",
              "141        70.7+/-1.0      70.7+/-1.0    32.6+/-2.0       45.6+/-3.1   \n",
              "165        68.9+/-0.6      68.9+/-0.6    30.5+/-0.4       44.7+/-3.3   \n",
              "66         68.6+/-1.2      68.6+/-1.2    29.1+/-1.1       47.1+/-0.5   \n",
              "126        68.6+/-0.7      68.6+/-0.7    29.2+/-0.9       45.2+/-4.2   \n",
              "60         68.5+/-1.2      68.5+/-1.2    28.9+/-1.5       47.2+/-0.5   \n",
              "132        68.3+/-1.7      68.3+/-1.7    29.4+/-2.1       47.6+/-1.0   \n",
              "78         68.3+/-1.3      68.3+/-1.3    28.3+/-0.9       47.7+/-0.5   \n",
              "105        68.3+/-0.8      68.3+/-0.8    28.8+/-0.7       43.8+/-4.0   \n",
              "81         68.1+/-2.0      68.1+/-2.0    29.5+/-1.8       42.9+/-4.2   \n",
              "179        68.1+/-0.3      68.1+/-0.3    29.3+/-2.1       47.2+/-1.5   \n",
              "110        68.0+/-1.1      68.0+/-1.1    28.5+/-1.7       47.9+/-0.5   \n",
              "62         68.0+/-1.0      68.0+/-1.0    28.9+/-0.4       47.9+/-2.0   \n",
              "138        67.8+/-0.5      67.8+/-0.5    27.9+/-0.4       47.8+/-0.7   \n",
              "\n",
              "    F1_score(+/-sd)  \n",
              "63       29.1+/-0.2  \n",
              "3        28.9+/-1.4  \n",
              "123      28.8+/-0.3  \n",
              "135      27.9+/-0.5  \n",
              "75       28.1+/-0.2  \n",
              "15       26.5+/-1.3  \n",
              "125      23.4+/-2.4  \n",
              "65       20.8+/-2.9  \n",
              "149      19.6+/-7.7  \n",
              "29       18.8+/-2.7  \n",
              "101      20.1+/-1.1  \n",
              "41       19.7+/-2.5  \n",
              "89       22.7+/-5.0  \n",
              "87       26.6+/-3.4  \n",
              "27       21.6+/-3.6  \n",
              "51       21.3+/-1.9  \n",
              "137      17.8+/-1.7  \n",
              "5        16.0+/-1.1  \n",
              "77       15.5+/-0.9  \n",
              "173      16.4+/-1.5  \n",
              "100      15.0+/-0.0  \n",
              "124      15.0+/-0.0  \n",
              "4        15.0+/-0.0  \n",
              "64       15.0+/-0.0  \n",
              "16       15.0+/-0.0  \n",
              "76       15.0+/-0.0  \n",
              "136      15.0+/-0.0  \n",
              "17       16.3+/-2.5  \n",
              "113      16.4+/-1.9  \n",
              "52       15.0+/-0.0  \n",
              "112      15.0+/-0.0  \n",
              "111      25.3+/-3.3  \n",
              "171      24.7+/-3.3  \n",
              "172      15.0+/-0.0  \n",
              "53       14.3+/-1.2  \n",
              "88       08.7+/-6.2  \n",
              "147      16.1+/-8.3  \n",
              "141      34.3+/-2.0  \n",
              "165      32.0+/-0.3  \n",
              "66       29.6+/-1.5  \n",
              "126      29.6+/-1.3  \n",
              "60       29.3+/-2.1  \n",
              "132      30.2+/-2.9  \n",
              "78       28.4+/-1.0  \n",
              "105      29.7+/-1.1  \n",
              "81       30.2+/-2.5  \n",
              "179      30.3+/-2.8  \n",
              "110      29.1+/-2.2  \n",
              "62       30.0+/-0.5  \n",
              "138      28.1+/-0.4  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "ucSekUWTe3Nb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Important Observation**\n",
        "\n",
        "I notice a great difference on reported Accuracy from Keras and Sklearn metrics when parameter losses='binary_crossentropy'.\n",
        "That's not the case with other examined losses' values. So I will rely on Sklearn metrics exclusively."
      ]
    },
    {
      "metadata": {
        "id": "uOMsup2MZXds",
        "colab_type": "code",
        "outputId": "2a551bfe-fd01-4740-e60c-28517a2b8486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2012
        }
      },
      "cell_type": "code",
      "source": [
        "# Sort results according to test set Accuracy reported from Sklearn\n",
        "\n",
        "modelsResults.sort_values('Accurasy(+/-sd)',ascending=False)[:50]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimensions</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Losses</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>K_Accurasy(+/-sd)</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>70.7+/-1.0</td>\n",
              "      <td>70.7+/-1.0</td>\n",
              "      <td>32.6+/-2.0</td>\n",
              "      <td>45.6+/-3.1</td>\n",
              "      <td>34.3+/-2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.7+/-0.2</td>\n",
              "      <td>69.1+/-0.3</td>\n",
              "      <td>28.9+/-0.1</td>\n",
              "      <td>48.0+/-0.6</td>\n",
              "      <td>29.1+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>30.5+/-0.4</td>\n",
              "      <td>44.7+/-3.3</td>\n",
              "      <td>32.0+/-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.6+/-1.2</td>\n",
              "      <td>68.6+/-1.2</td>\n",
              "      <td>29.1+/-1.1</td>\n",
              "      <td>47.1+/-0.5</td>\n",
              "      <td>29.6+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.6+/-0.7</td>\n",
              "      <td>68.6+/-0.7</td>\n",
              "      <td>29.2+/-0.9</td>\n",
              "      <td>45.2+/-4.2</td>\n",
              "      <td>29.6+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>28.9+/-1.5</td>\n",
              "      <td>47.2+/-0.5</td>\n",
              "      <td>29.3+/-2.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.5+/-0.2</td>\n",
              "      <td>68.5+/-1.0</td>\n",
              "      <td>28.6+/-1.0</td>\n",
              "      <td>41.1+/-6.9</td>\n",
              "      <td>28.9+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-1.7</td>\n",
              "      <td>68.3+/-1.7</td>\n",
              "      <td>29.4+/-2.1</td>\n",
              "      <td>47.6+/-1.0</td>\n",
              "      <td>30.2+/-2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-1.3</td>\n",
              "      <td>68.3+/-1.3</td>\n",
              "      <td>28.3+/-0.9</td>\n",
              "      <td>47.7+/-0.5</td>\n",
              "      <td>28.4+/-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-0.8</td>\n",
              "      <td>68.3+/-0.8</td>\n",
              "      <td>28.8+/-0.7</td>\n",
              "      <td>43.8+/-4.0</td>\n",
              "      <td>29.7+/-1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.3+/-0.3</td>\n",
              "      <td>68.3+/-0.5</td>\n",
              "      <td>28.6+/-0.4</td>\n",
              "      <td>47.3+/-0.1</td>\n",
              "      <td>28.8+/-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.1+/-2.0</td>\n",
              "      <td>68.1+/-2.0</td>\n",
              "      <td>29.5+/-1.8</td>\n",
              "      <td>42.9+/-4.2</td>\n",
              "      <td>30.2+/-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>29.3+/-2.1</td>\n",
              "      <td>47.2+/-1.5</td>\n",
              "      <td>30.3+/-2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.0+/-1.1</td>\n",
              "      <td>68.0+/-1.1</td>\n",
              "      <td>28.5+/-1.7</td>\n",
              "      <td>47.9+/-0.5</td>\n",
              "      <td>29.1+/-2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.0+/-1.0</td>\n",
              "      <td>68.0+/-1.0</td>\n",
              "      <td>28.9+/-0.4</td>\n",
              "      <td>47.9+/-2.0</td>\n",
              "      <td>30.0+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>27.9+/-0.4</td>\n",
              "      <td>47.8+/-0.7</td>\n",
              "      <td>28.1+/-0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>86.9+/-0.2</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>27.9+/-0.2</td>\n",
              "      <td>47.7+/-0.8</td>\n",
              "      <td>28.1+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.6+/-1.2</td>\n",
              "      <td>67.6+/-1.2</td>\n",
              "      <td>28.6+/-1.2</td>\n",
              "      <td>46.9+/-0.8</td>\n",
              "      <td>29.1+/-1.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.6+/-1.0</td>\n",
              "      <td>67.6+/-1.0</td>\n",
              "      <td>28.2+/-0.4</td>\n",
              "      <td>43.7+/-6.2</td>\n",
              "      <td>28.3+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.1+/-0.4</td>\n",
              "      <td>67.6+/-0.6</td>\n",
              "      <td>27.8+/-0.4</td>\n",
              "      <td>47.0+/-0.8</td>\n",
              "      <td>27.9+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.6+/-0.3</td>\n",
              "      <td>67.6+/-0.3</td>\n",
              "      <td>28.0+/-0.1</td>\n",
              "      <td>46.7+/-0.1</td>\n",
              "      <td>28.1+/-0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>67.3+/-2.1</td>\n",
              "      <td>67.3+/-2.1</td>\n",
              "      <td>27.5+/-1.6</td>\n",
              "      <td>42.2+/-5.8</td>\n",
              "      <td>27.7+/-2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.3+/-1.3</td>\n",
              "      <td>67.3+/-1.3</td>\n",
              "      <td>28.8+/-1.4</td>\n",
              "      <td>43.9+/-4.5</td>\n",
              "      <td>29.5+/-1.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.3+/-0.5</td>\n",
              "      <td>67.3+/-0.5</td>\n",
              "      <td>27.2+/-1.0</td>\n",
              "      <td>40.3+/-11.2</td>\n",
              "      <td>26.8+/-1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>67.2+/-2.0</td>\n",
              "      <td>67.2+/-2.0</td>\n",
              "      <td>32.7+/-6.3</td>\n",
              "      <td>46.5+/-16.1</td>\n",
              "      <td>34.7+/-8.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>67.2+/-0.6</td>\n",
              "      <td>67.2+/-0.6</td>\n",
              "      <td>27.5+/-0.9</td>\n",
              "      <td>44.9+/-6.0</td>\n",
              "      <td>27.9+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.8+/-0.8</td>\n",
              "      <td>66.8+/-0.8</td>\n",
              "      <td>27.8+/-1.6</td>\n",
              "      <td>47.4+/-1.9</td>\n",
              "      <td>28.4+/-2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>27.4+/-0.3</td>\n",
              "      <td>43.0+/-5.6</td>\n",
              "      <td>27.4+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>86.5+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>26.9+/-0.6</td>\n",
              "      <td>40.7+/-12.3</td>\n",
              "      <td>26.5+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.7+/-1.6</td>\n",
              "      <td>66.7+/-1.6</td>\n",
              "      <td>27.4+/-0.8</td>\n",
              "      <td>35.6+/-2.8</td>\n",
              "      <td>27.4+/-0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>59.2+/-0.6</td>\n",
              "      <td>59.2+/-0.6</td>\n",
              "      <td>19.9+/-0.2</td>\n",
              "      <td>14.5+/-4.3</td>\n",
              "      <td>15.5+/-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>58.9+/-4.7</td>\n",
              "      <td>58.9+/-4.7</td>\n",
              "      <td>23.5+/-2.8</td>\n",
              "      <td>32.6+/-11.6</td>\n",
              "      <td>20.9+/-4.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>77.3+/-11.5</td>\n",
              "      <td>54.5+/-9.6</td>\n",
              "      <td>18.2+/-3.2</td>\n",
              "      <td>11.9+/-0.1</td>\n",
              "      <td>14.3+/-1.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>52.0+/-21.8</td>\n",
              "      <td>52.0+/-21.8</td>\n",
              "      <td>23.9+/-3.4</td>\n",
              "      <td>18.2+/-10.9</td>\n",
              "      <td>19.3+/-9.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>71.2+/-13.3</td>\n",
              "      <td>49.8+/-19.9</td>\n",
              "      <td>22.1+/-3.9</td>\n",
              "      <td>13.6+/-9.1</td>\n",
              "      <td>16.1+/-8.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>100</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>48.3+/-18.6</td>\n",
              "      <td>48.3+/-18.6</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>10.9+/-5.0</td>\n",
              "      <td>13.0+/-3.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>44.7+/-32.1</td>\n",
              "      <td>44.7+/-32.1</td>\n",
              "      <td>21.9+/-3.6</td>\n",
              "      <td>15.7+/-8.7</td>\n",
              "      <td>15.4+/-9.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>42.8+/-31.1</td>\n",
              "      <td>42.8+/-31.1</td>\n",
              "      <td>21.9+/-3.3</td>\n",
              "      <td>11.9+/-10.6</td>\n",
              "      <td>13.4+/-10.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>42.3+/-30.7</td>\n",
              "      <td>42.3+/-30.7</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>08.5+/-6.1</td>\n",
              "      <td>10.9+/-7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>42.3+/-30.7</td>\n",
              "      <td>42.3+/-30.7</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>08.5+/-6.1</td>\n",
              "      <td>10.9+/-7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>42.1+/-30.6</td>\n",
              "      <td>42.1+/-30.6</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>08.4+/-6.1</td>\n",
              "      <td>10.8+/-7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>63.9+/-21.3</td>\n",
              "      <td>41.3+/-32.0</td>\n",
              "      <td>19.9+/-0.1</td>\n",
              "      <td>08.3+/-6.4</td>\n",
              "      <td>10.5+/-7.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>64.8+/-32.8</td>\n",
              "      <td>40.7+/-33.5</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>08.1+/-6.7</td>\n",
              "      <td>10.3+/-8.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>100</td>\n",
              "      <td>tanh</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>60.7+/-40.5</td>\n",
              "      <td>40.7+/-33.5</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>08.1+/-6.7</td>\n",
              "      <td>10.3+/-8.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>36.7+/-24.2</td>\n",
              "      <td>36.7+/-24.2</td>\n",
              "      <td>20.7+/-1.5</td>\n",
              "      <td>13.8+/-4.4</td>\n",
              "      <td>13.4+/-8.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>31.2+/-26.9</td>\n",
              "      <td>31.2+/-26.9</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>06.2+/-5.4</td>\n",
              "      <td>08.7+/-6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>71.7+/-10.9</td>\n",
              "      <td>31.2+/-26.9</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>06.2+/-5.4</td>\n",
              "      <td>08.7+/-6.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>27.5+/-21.5</td>\n",
              "      <td>27.5+/-21.5</td>\n",
              "      <td>20.6+/-1.4</td>\n",
              "      <td>08.9+/-7.2</td>\n",
              "      <td>10.9+/-6.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>27.2+/-34.8</td>\n",
              "      <td>27.2+/-34.8</td>\n",
              "      <td>22.8+/-4.7</td>\n",
              "      <td>16.0+/-12.9</td>\n",
              "      <td>10.7+/-13.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>25.7+/-31.8</td>\n",
              "      <td>25.7+/-31.8</td>\n",
              "      <td>22.3+/-2.5</td>\n",
              "      <td>25.0+/-6.3</td>\n",
              "      <td>10.7+/-7.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>24.6+/-30.7</td>\n",
              "      <td>24.6+/-30.7</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>04.9+/-6.1</td>\n",
              "      <td>06.7+/-7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>1000</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>56.3+/-15.9</td>\n",
              "      <td>24.6+/-30.7</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>04.9+/-6.1</td>\n",
              "      <td>06.7+/-7.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>100</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>24.6+/-30.7</td>\n",
              "      <td>24.6+/-30.7</td>\n",
              "      <td>22.0+/-3.4</td>\n",
              "      <td>07.8+/-5.7</td>\n",
              "      <td>09.6+/-6.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>22.8+/-31.9</td>\n",
              "      <td>22.8+/-31.9</td>\n",
              "      <td>16.2+/-6.4</td>\n",
              "      <td>04.7+/-6.5</td>\n",
              "      <td>06.3+/-7.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>sgd</td>\n",
              "      <td>22.8+/-31.8</td>\n",
              "      <td>22.8+/-31.8</td>\n",
              "      <td>20.9+/-1.7</td>\n",
              "      <td>04.8+/-6.2</td>\n",
              "      <td>06.5+/-7.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>100</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>21.6+/-20.2</td>\n",
              "      <td>21.6+/-20.2</td>\n",
              "      <td>20.5+/-1.0</td>\n",
              "      <td>14.1+/-3.2</td>\n",
              "      <td>07.9+/-5.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>15.9+/-11.8</td>\n",
              "      <td>15.9+/-11.8</td>\n",
              "      <td>22.1+/-2.0</td>\n",
              "      <td>06.9+/-5.9</td>\n",
              "      <td>06.8+/-3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>01.4+/-0.0</td>\n",
              "      <td>02.6+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>01.4+/-0.0</td>\n",
              "      <td>02.6+/-0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>06.8+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>01.4+/-0.0</td>\n",
              "      <td>02.6+/-0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>180 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Dimensions   Activation                    Losses Optimizer  \\\n",
              "141        1000      sigmoid         categorical_hinge      adam   \n",
              "63          500      softmax       binary_crossentropy      adam   \n",
              "165        1000         tanh         categorical_hinge      adam   \n",
              "66          500      softmax        mean_squared_error      adam   \n",
              "126        1000      softmax        mean_squared_error      adam   \n",
              "60          500      softmax  categorical_crossentropy      adam   \n",
              "3           100      softmax       binary_crossentropy      adam   \n",
              "132        1000      sigmoid  categorical_crossentropy      adam   \n",
              "78          500      sigmoid        mean_squared_error      adam   \n",
              "105         500         tanh         categorical_hinge      adam   \n",
              "123        1000      softmax       binary_crossentropy      adam   \n",
              "81          500      sigmoid         categorical_hinge      adam   \n",
              "179        1000  exponential         categorical_hinge  Adadelta   \n",
              "110         500  exponential  categorical_crossentropy  Adadelta   \n",
              "62          500      softmax  categorical_crossentropy  Adadelta   \n",
              "138        1000      sigmoid        mean_squared_error      adam   \n",
              "75          500      sigmoid       binary_crossentropy      adam   \n",
              "120        1000      softmax  categorical_crossentropy      adam   \n",
              "108         500  exponential  categorical_crossentropy      adam   \n",
              "135        1000      sigmoid       binary_crossentropy      adam   \n",
              "168        1000  exponential  categorical_crossentropy      adam   \n",
              "107         500         tanh         categorical_hinge  Adadelta   \n",
              "6           100      softmax        mean_squared_error      adam   \n",
              "0           100      softmax  categorical_crossentropy      adam   \n",
              "119         500  exponential         categorical_hinge  Adadelta   \n",
              "170        1000  exponential  categorical_crossentropy  Adadelta   \n",
              "102         500         tanh        mean_squared_error      adam   \n",
              "48          100  exponential  categorical_crossentropy      adam   \n",
              "15          100      sigmoid       binary_crossentropy      adam   \n",
              "72          500      sigmoid  categorical_crossentropy      adam   \n",
              "..          ...          ...                       ...       ...   \n",
              "56          100  exponential        mean_squared_error  Adadelta   \n",
              "14          100      sigmoid  categorical_crossentropy  Adadelta   \n",
              "53          100  exponential       binary_crossentropy  Adadelta   \n",
              "30          100         relu        mean_squared_error      adam   \n",
              "147        1000         relu       binary_crossentropy      adam   \n",
              "36          100         tanh  categorical_crossentropy      adam   \n",
              "35          100         relu         categorical_hinge  Adadelta   \n",
              "144        1000         relu  categorical_crossentropy      adam   \n",
              "145        1000         relu  categorical_crossentropy       sgd   \n",
              "25          100         relu  categorical_crossentropy       sgd   \n",
              "146        1000         relu  categorical_crossentropy  Adadelta   \n",
              "160        1000         tanh       binary_crossentropy       sgd   \n",
              "28          100         relu       binary_crossentropy       sgd   \n",
              "40          100         tanh       binary_crossentropy       sgd   \n",
              "156        1000         tanh  categorical_crossentropy      adam   \n",
              "97          500         tanh  categorical_crossentropy       sgd   \n",
              "88          500         relu       binary_crossentropy       sgd   \n",
              "98          500         tanh  categorical_crossentropy  Adadelta   \n",
              "155        1000         relu         categorical_hinge  Adadelta   \n",
              "152        1000         relu        mean_squared_error  Adadelta   \n",
              "84          500         relu  categorical_crossentropy      adam   \n",
              "148        1000         relu       binary_crossentropy       sgd   \n",
              "37          100         tanh  categorical_crossentropy       sgd   \n",
              "96          500         tanh  categorical_crossentropy      adam   \n",
              "157        1000         tanh  categorical_crossentropy       sgd   \n",
              "38          100         tanh  categorical_crossentropy  Adadelta   \n",
              "158        1000         tanh  categorical_crossentropy  Adadelta   \n",
              "86          500         relu  categorical_crossentropy  Adadelta   \n",
              "26          100         relu  categorical_crossentropy  Adadelta   \n",
              "24          100         relu  categorical_crossentropy      adam   \n",
              "\n",
              "    K_Accurasy(+/-sd) Accurasy(+/-sd) Recall(+/-sd) Precision(+/-sd)  \\\n",
              "141        70.7+/-1.0      70.7+/-1.0    32.6+/-2.0       45.6+/-3.1   \n",
              "63         87.7+/-0.2      69.1+/-0.3    28.9+/-0.1       48.0+/-0.6   \n",
              "165        68.9+/-0.6      68.9+/-0.6    30.5+/-0.4       44.7+/-3.3   \n",
              "66         68.6+/-1.2      68.6+/-1.2    29.1+/-1.1       47.1+/-0.5   \n",
              "126        68.6+/-0.7      68.6+/-0.7    29.2+/-0.9       45.2+/-4.2   \n",
              "60         68.5+/-1.2      68.5+/-1.2    28.9+/-1.5       47.2+/-0.5   \n",
              "3          87.5+/-0.2      68.5+/-1.0    28.6+/-1.0       41.1+/-6.9   \n",
              "132        68.3+/-1.7      68.3+/-1.7    29.4+/-2.1       47.6+/-1.0   \n",
              "78         68.3+/-1.3      68.3+/-1.3    28.3+/-0.9       47.7+/-0.5   \n",
              "105        68.3+/-0.8      68.3+/-0.8    28.8+/-0.7       43.8+/-4.0   \n",
              "123        87.3+/-0.3      68.3+/-0.5    28.6+/-0.4       47.3+/-0.1   \n",
              "81         68.1+/-2.0      68.1+/-2.0    29.5+/-1.8       42.9+/-4.2   \n",
              "179        68.1+/-0.3      68.1+/-0.3    29.3+/-2.1       47.2+/-1.5   \n",
              "110        68.0+/-1.1      68.0+/-1.1    28.5+/-1.7       47.9+/-0.5   \n",
              "62         68.0+/-1.0      68.0+/-1.0    28.9+/-0.4       47.9+/-2.0   \n",
              "138        67.8+/-0.5      67.8+/-0.5    27.9+/-0.4       47.8+/-0.7   \n",
              "75         86.9+/-0.2      67.8+/-0.5    27.9+/-0.2       47.7+/-0.8   \n",
              "120        67.6+/-1.2      67.6+/-1.2    28.6+/-1.2       46.9+/-0.8   \n",
              "108        67.6+/-1.0      67.6+/-1.0    28.2+/-0.4       43.7+/-6.2   \n",
              "135        87.1+/-0.4      67.6+/-0.6    27.8+/-0.4       47.0+/-0.8   \n",
              "168        67.6+/-0.3      67.6+/-0.3    28.0+/-0.1       46.7+/-0.1   \n",
              "107        67.3+/-2.1      67.3+/-2.1    27.5+/-1.6       42.2+/-5.8   \n",
              "6          67.3+/-1.3      67.3+/-1.3    28.8+/-1.4       43.9+/-4.5   \n",
              "0          67.3+/-0.5      67.3+/-0.5    27.2+/-1.0      40.3+/-11.2   \n",
              "119        67.2+/-2.0      67.2+/-2.0    32.7+/-6.3      46.5+/-16.1   \n",
              "170        67.2+/-0.6      67.2+/-0.6    27.5+/-0.9       44.9+/-6.0   \n",
              "102        66.8+/-0.8      66.8+/-0.8    27.8+/-1.6       47.4+/-1.9   \n",
              "48         66.8+/-0.5      66.8+/-0.5    27.4+/-0.3       43.0+/-5.6   \n",
              "15         86.5+/-0.5      66.8+/-0.5    26.9+/-0.6      40.7+/-12.3   \n",
              "72         66.7+/-1.6      66.7+/-1.6    27.4+/-0.8       35.6+/-2.8   \n",
              "..                ...             ...           ...              ...   \n",
              "56         59.2+/-0.6      59.2+/-0.6    19.9+/-0.2       14.5+/-4.3   \n",
              "14         58.9+/-4.7      58.9+/-4.7    23.5+/-2.8      32.6+/-11.6   \n",
              "53        77.3+/-11.5      54.5+/-9.6    18.2+/-3.2       11.9+/-0.1   \n",
              "30        52.0+/-21.8     52.0+/-21.8    23.9+/-3.4      18.2+/-10.9   \n",
              "147       71.2+/-13.3     49.8+/-19.9    22.1+/-3.9       13.6+/-9.1   \n",
              "36        48.3+/-18.6     48.3+/-18.6    19.9+/-0.1       10.9+/-5.0   \n",
              "35        44.7+/-32.1     44.7+/-32.1    21.9+/-3.6       15.7+/-8.7   \n",
              "144       42.8+/-31.1     42.8+/-31.1    21.9+/-3.3      11.9+/-10.6   \n",
              "145       42.3+/-30.7     42.3+/-30.7    20.0+/-0.0       08.5+/-6.1   \n",
              "25        42.3+/-30.7     42.3+/-30.7    20.0+/-0.0       08.5+/-6.1   \n",
              "146       42.1+/-30.6     42.1+/-30.6    19.9+/-0.1       08.4+/-6.1   \n",
              "160       63.9+/-21.3     41.3+/-32.0    19.9+/-0.1       08.3+/-6.4   \n",
              "28        64.8+/-32.8     40.7+/-33.5    20.0+/-0.0       08.1+/-6.7   \n",
              "40        60.7+/-40.5     40.7+/-33.5    20.0+/-0.0       08.1+/-6.7   \n",
              "156       36.7+/-24.2     36.7+/-24.2    20.7+/-1.5       13.8+/-4.4   \n",
              "97        31.2+/-26.9     31.2+/-26.9    20.0+/-0.0       06.2+/-5.4   \n",
              "88        71.7+/-10.9     31.2+/-26.9    20.0+/-0.0       06.2+/-5.4   \n",
              "98        27.5+/-21.5     27.5+/-21.5    20.6+/-1.4       08.9+/-7.2   \n",
              "155       27.2+/-34.8     27.2+/-34.8    22.8+/-4.7      16.0+/-12.9   \n",
              "152       25.7+/-31.8     25.7+/-31.8    22.3+/-2.5       25.0+/-6.3   \n",
              "84        24.6+/-30.7     24.6+/-30.7    20.0+/-0.0       04.9+/-6.1   \n",
              "148       56.3+/-15.9     24.6+/-30.7    20.0+/-0.0       04.9+/-6.1   \n",
              "37        24.6+/-30.7     24.6+/-30.7    22.0+/-3.4       07.8+/-5.7   \n",
              "96        22.8+/-31.9     22.8+/-31.9    16.2+/-6.4       04.7+/-6.5   \n",
              "157       22.8+/-31.8     22.8+/-31.8    20.9+/-1.7       04.8+/-6.2   \n",
              "38        21.6+/-20.2     21.6+/-20.2    20.5+/-1.0       14.1+/-3.2   \n",
              "158       15.9+/-11.8     15.9+/-11.8    22.1+/-2.0       06.9+/-5.9   \n",
              "86         06.8+/-0.0      06.8+/-0.0    20.0+/-0.0       01.4+/-0.0   \n",
              "26         06.8+/-0.0      06.8+/-0.0    20.0+/-0.0       01.4+/-0.0   \n",
              "24         06.8+/-0.0      06.8+/-0.0    20.0+/-0.0       01.4+/-0.0   \n",
              "\n",
              "    F1_score(+/-sd)  \n",
              "141      34.3+/-2.0  \n",
              "63       29.1+/-0.2  \n",
              "165      32.0+/-0.3  \n",
              "66       29.6+/-1.5  \n",
              "126      29.6+/-1.3  \n",
              "60       29.3+/-2.1  \n",
              "3        28.9+/-1.4  \n",
              "132      30.2+/-2.9  \n",
              "78       28.4+/-1.0  \n",
              "105      29.7+/-1.1  \n",
              "123      28.8+/-0.3  \n",
              "81       30.2+/-2.5  \n",
              "179      30.3+/-2.8  \n",
              "110      29.1+/-2.2  \n",
              "62       30.0+/-0.5  \n",
              "138      28.1+/-0.4  \n",
              "75       28.1+/-0.2  \n",
              "120      29.1+/-1.8  \n",
              "108      28.3+/-0.5  \n",
              "135      27.9+/-0.5  \n",
              "168      28.1+/-0.1  \n",
              "107      27.7+/-2.3  \n",
              "6        29.5+/-1.8  \n",
              "0        26.8+/-1.6  \n",
              "119      34.7+/-8.9  \n",
              "170      27.9+/-1.4  \n",
              "102      28.4+/-2.2  \n",
              "48       27.4+/-0.2  \n",
              "15       26.5+/-1.3  \n",
              "72       27.4+/-0.9  \n",
              "..              ...  \n",
              "56       15.5+/-1.0  \n",
              "14       20.9+/-4.6  \n",
              "53       14.3+/-1.2  \n",
              "30       19.3+/-9.1  \n",
              "147      16.1+/-8.3  \n",
              "36       13.0+/-3.9  \n",
              "35       15.4+/-9.9  \n",
              "144     13.4+/-10.1  \n",
              "145      10.9+/-7.2  \n",
              "25       10.9+/-7.2  \n",
              "146      10.8+/-7.2  \n",
              "160      10.5+/-7.7  \n",
              "28       10.3+/-8.2  \n",
              "40       10.3+/-8.2  \n",
              "156      13.4+/-8.2  \n",
              "97       08.7+/-6.2  \n",
              "88       08.7+/-6.2  \n",
              "98       10.9+/-6.5  \n",
              "155     10.7+/-13.8  \n",
              "152      10.7+/-7.8  \n",
              "84       06.7+/-7.2  \n",
              "148      06.7+/-7.2  \n",
              "37       09.6+/-6.4  \n",
              "96       06.3+/-7.7  \n",
              "157      06.5+/-7.3  \n",
              "38       07.9+/-5.2  \n",
              "158      06.8+/-3.0  \n",
              "86       02.6+/-0.0  \n",
              "26       02.6+/-0.0  \n",
              "24       02.6+/-0.0  \n",
              "\n",
              "[180 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "XCeJYTDbZXNv",
        "colab_type": "code",
        "outputId": "b3da2e25-2ce9-431b-9b4f-956c0d1742a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1638
        }
      },
      "cell_type": "code",
      "source": [
        "# Sort results according to test set F1 score reported from Sklearn\n",
        "\n",
        "modelsResults.sort_values('F1_score(+/-sd)',ascending=False)[:50]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dimensions</th>\n",
              "      <th>Activation</th>\n",
              "      <th>Losses</th>\n",
              "      <th>Optimizer</th>\n",
              "      <th>K_Accurasy(+/-sd)</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>67.2+/-2.0</td>\n",
              "      <td>67.2+/-2.0</td>\n",
              "      <td>32.7+/-6.3</td>\n",
              "      <td>46.5+/-16.1</td>\n",
              "      <td>34.7+/-8.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>70.7+/-1.0</td>\n",
              "      <td>70.7+/-1.0</td>\n",
              "      <td>32.6+/-2.0</td>\n",
              "      <td>45.6+/-3.1</td>\n",
              "      <td>34.3+/-2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>64.9+/-3.0</td>\n",
              "      <td>64.9+/-3.0</td>\n",
              "      <td>31.8+/-2.9</td>\n",
              "      <td>41.3+/-5.5</td>\n",
              "      <td>33.2+/-3.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>65.9+/-2.0</td>\n",
              "      <td>65.9+/-2.0</td>\n",
              "      <td>31.8+/-1.5</td>\n",
              "      <td>38.7+/-6.1</td>\n",
              "      <td>33.1+/-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.2+/-0.7</td>\n",
              "      <td>66.2+/-0.7</td>\n",
              "      <td>31.3+/-0.8</td>\n",
              "      <td>38.9+/-4.3</td>\n",
              "      <td>32.4+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>68.9+/-0.6</td>\n",
              "      <td>30.5+/-0.4</td>\n",
              "      <td>44.7+/-3.3</td>\n",
              "      <td>32.0+/-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.2+/-2.5</td>\n",
              "      <td>66.2+/-2.5</td>\n",
              "      <td>31.0+/-0.7</td>\n",
              "      <td>36.9+/-5.4</td>\n",
              "      <td>31.2+/-1.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>29.3+/-2.1</td>\n",
              "      <td>47.2+/-1.5</td>\n",
              "      <td>30.3+/-2.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-1.7</td>\n",
              "      <td>68.3+/-1.7</td>\n",
              "      <td>29.4+/-2.1</td>\n",
              "      <td>47.6+/-1.0</td>\n",
              "      <td>30.2+/-2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.1+/-2.0</td>\n",
              "      <td>68.1+/-2.0</td>\n",
              "      <td>29.5+/-1.8</td>\n",
              "      <td>42.9+/-4.2</td>\n",
              "      <td>30.2+/-2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.0+/-1.0</td>\n",
              "      <td>68.0+/-1.0</td>\n",
              "      <td>28.9+/-0.4</td>\n",
              "      <td>47.9+/-2.0</td>\n",
              "      <td>30.0+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-0.8</td>\n",
              "      <td>68.3+/-0.8</td>\n",
              "      <td>28.8+/-0.7</td>\n",
              "      <td>43.8+/-4.0</td>\n",
              "      <td>29.7+/-1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.6+/-1.2</td>\n",
              "      <td>68.6+/-1.2</td>\n",
              "      <td>29.1+/-1.1</td>\n",
              "      <td>47.1+/-0.5</td>\n",
              "      <td>29.6+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.6+/-0.7</td>\n",
              "      <td>68.6+/-0.7</td>\n",
              "      <td>29.2+/-0.9</td>\n",
              "      <td>45.2+/-4.2</td>\n",
              "      <td>29.6+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.3+/-1.3</td>\n",
              "      <td>67.3+/-1.3</td>\n",
              "      <td>28.8+/-1.4</td>\n",
              "      <td>43.9+/-4.5</td>\n",
              "      <td>29.5+/-1.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>28.9+/-1.5</td>\n",
              "      <td>47.2+/-0.5</td>\n",
              "      <td>29.3+/-2.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>68.0+/-1.1</td>\n",
              "      <td>68.0+/-1.1</td>\n",
              "      <td>28.5+/-1.7</td>\n",
              "      <td>47.9+/-0.5</td>\n",
              "      <td>29.1+/-2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.6+/-1.2</td>\n",
              "      <td>67.6+/-1.2</td>\n",
              "      <td>28.6+/-1.2</td>\n",
              "      <td>46.9+/-0.8</td>\n",
              "      <td>29.1+/-1.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>500</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.7+/-0.2</td>\n",
              "      <td>69.1+/-0.3</td>\n",
              "      <td>28.9+/-0.1</td>\n",
              "      <td>48.0+/-0.6</td>\n",
              "      <td>29.1+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>100</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.2+/-1.7</td>\n",
              "      <td>66.2+/-1.7</td>\n",
              "      <td>28.1+/-2.0</td>\n",
              "      <td>44.3+/-1.9</td>\n",
              "      <td>28.9+/-2.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.5+/-0.2</td>\n",
              "      <td>68.5+/-1.0</td>\n",
              "      <td>28.6+/-1.0</td>\n",
              "      <td>41.1+/-6.9</td>\n",
              "      <td>28.9+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.3+/-0.3</td>\n",
              "      <td>68.3+/-0.5</td>\n",
              "      <td>28.6+/-0.4</td>\n",
              "      <td>47.3+/-0.1</td>\n",
              "      <td>28.8+/-0.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.5+/-2.0</td>\n",
              "      <td>66.5+/-2.0</td>\n",
              "      <td>27.7+/-1.2</td>\n",
              "      <td>47.4+/-0.8</td>\n",
              "      <td>28.6+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.8+/-0.8</td>\n",
              "      <td>66.8+/-0.8</td>\n",
              "      <td>27.8+/-1.6</td>\n",
              "      <td>47.4+/-1.9</td>\n",
              "      <td>28.4+/-2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>100</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.2+/-1.0</td>\n",
              "      <td>66.2+/-1.0</td>\n",
              "      <td>27.8+/-1.3</td>\n",
              "      <td>44.7+/-4.3</td>\n",
              "      <td>28.4+/-1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>68.3+/-1.3</td>\n",
              "      <td>68.3+/-1.3</td>\n",
              "      <td>28.3+/-0.9</td>\n",
              "      <td>47.7+/-0.5</td>\n",
              "      <td>28.4+/-1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.2+/-0.7</td>\n",
              "      <td>66.2+/-0.7</td>\n",
              "      <td>27.9+/-2.3</td>\n",
              "      <td>41.1+/-5.2</td>\n",
              "      <td>28.3+/-3.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.6+/-1.0</td>\n",
              "      <td>67.6+/-1.0</td>\n",
              "      <td>28.2+/-0.4</td>\n",
              "      <td>43.7+/-6.2</td>\n",
              "      <td>28.3+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>64.7+/-1.0</td>\n",
              "      <td>64.7+/-1.0</td>\n",
              "      <td>28.2+/-1.9</td>\n",
              "      <td>45.0+/-1.8</td>\n",
              "      <td>28.1+/-2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>27.9+/-0.4</td>\n",
              "      <td>47.8+/-0.7</td>\n",
              "      <td>28.1+/-0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>86.9+/-0.2</td>\n",
              "      <td>67.8+/-0.5</td>\n",
              "      <td>27.9+/-0.2</td>\n",
              "      <td>47.7+/-0.8</td>\n",
              "      <td>28.1+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.6+/-0.3</td>\n",
              "      <td>67.6+/-0.3</td>\n",
              "      <td>28.0+/-0.1</td>\n",
              "      <td>46.7+/-0.1</td>\n",
              "      <td>28.1+/-0.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.7+/-1.2</td>\n",
              "      <td>66.7+/-1.2</td>\n",
              "      <td>27.6+/-2.9</td>\n",
              "      <td>45.4+/-4.0</td>\n",
              "      <td>27.9+/-3.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>1000</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>67.2+/-0.6</td>\n",
              "      <td>67.2+/-0.6</td>\n",
              "      <td>27.5+/-0.9</td>\n",
              "      <td>44.9+/-6.0</td>\n",
              "      <td>27.9+/-1.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>1000</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>87.1+/-0.4</td>\n",
              "      <td>67.6+/-0.6</td>\n",
              "      <td>27.8+/-0.4</td>\n",
              "      <td>47.0+/-0.8</td>\n",
              "      <td>27.9+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>500</td>\n",
              "      <td>tanh</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>67.3+/-2.1</td>\n",
              "      <td>67.3+/-2.1</td>\n",
              "      <td>27.5+/-1.6</td>\n",
              "      <td>42.2+/-5.8</td>\n",
              "      <td>27.7+/-2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.7+/-1.6</td>\n",
              "      <td>66.7+/-1.6</td>\n",
              "      <td>27.4+/-0.8</td>\n",
              "      <td>35.6+/-2.8</td>\n",
              "      <td>27.4+/-0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>27.4+/-0.3</td>\n",
              "      <td>43.0+/-5.6</td>\n",
              "      <td>27.4+/-0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>adam</td>\n",
              "      <td>66.5+/-1.2</td>\n",
              "      <td>66.5+/-1.2</td>\n",
              "      <td>27.1+/-1.5</td>\n",
              "      <td>46.7+/-1.3</td>\n",
              "      <td>27.3+/-2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>1000</td>\n",
              "      <td>tanh</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.3+/-0.8</td>\n",
              "      <td>66.3+/-0.8</td>\n",
              "      <td>26.8+/-0.5</td>\n",
              "      <td>47.6+/-1.5</td>\n",
              "      <td>26.9+/-0.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>67.3+/-0.5</td>\n",
              "      <td>67.3+/-0.5</td>\n",
              "      <td>27.2+/-1.0</td>\n",
              "      <td>40.3+/-11.2</td>\n",
              "      <td>26.8+/-1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>85.6+/-0.3</td>\n",
              "      <td>64.7+/-2.3</td>\n",
              "      <td>27.4+/-2.0</td>\n",
              "      <td>28.5+/-8.8</td>\n",
              "      <td>26.6+/-3.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>100</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>86.5+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>26.9+/-0.6</td>\n",
              "      <td>40.7+/-12.3</td>\n",
              "      <td>26.5+/-1.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>1000</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>65.7+/-0.7</td>\n",
              "      <td>65.7+/-0.7</td>\n",
              "      <td>26.3+/-1.1</td>\n",
              "      <td>43.2+/-6.7</td>\n",
              "      <td>26.1+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>100</td>\n",
              "      <td>exponential</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.3+/-0.5</td>\n",
              "      <td>66.3+/-0.5</td>\n",
              "      <td>26.3+/-0.5</td>\n",
              "      <td>47.7+/-0.5</td>\n",
              "      <td>26.0+/-0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>500</td>\n",
              "      <td>relu</td>\n",
              "      <td>categorical_hinge</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>65.7+/-3.3</td>\n",
              "      <td>65.7+/-3.3</td>\n",
              "      <td>27.2+/-0.9</td>\n",
              "      <td>24.9+/-1.7</td>\n",
              "      <td>25.6+/-1.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>500</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>64.7+/-0.7</td>\n",
              "      <td>64.7+/-0.7</td>\n",
              "      <td>26.1+/-1.9</td>\n",
              "      <td>46.6+/-3.1</td>\n",
              "      <td>25.5+/-2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>500</td>\n",
              "      <td>exponential</td>\n",
              "      <td>binary_crossentropy</td>\n",
              "      <td>adam</td>\n",
              "      <td>82.5+/-0.3</td>\n",
              "      <td>65.2+/-1.2</td>\n",
              "      <td>25.9+/-2.3</td>\n",
              "      <td>37.4+/-10.8</td>\n",
              "      <td>25.3+/-3.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>softmax</td>\n",
              "      <td>categorical_crossentropy</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>66.0+/-0.3</td>\n",
              "      <td>66.0+/-0.3</td>\n",
              "      <td>25.8+/-0.5</td>\n",
              "      <td>47.2+/-0.7</td>\n",
              "      <td>25.3+/-0.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>100</td>\n",
              "      <td>relu</td>\n",
              "      <td>mean_squared_error</td>\n",
              "      <td>Adadelta</td>\n",
              "      <td>65.7+/-1.7</td>\n",
              "      <td>65.7+/-1.7</td>\n",
              "      <td>25.6+/-1.4</td>\n",
              "      <td>48.3+/-1.8</td>\n",
              "      <td>25.1+/-1.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Dimensions   Activation                    Losses Optimizer  \\\n",
              "119         500  exponential         categorical_hinge  Adadelta   \n",
              "141        1000      sigmoid         categorical_hinge      adam   \n",
              "177        1000  exponential         categorical_hinge      adam   \n",
              "57          100  exponential         categorical_hinge      adam   \n",
              "117         500  exponential         categorical_hinge      adam   \n",
              "165        1000         tanh         categorical_hinge      adam   \n",
              "59          100  exponential         categorical_hinge  Adadelta   \n",
              "179        1000  exponential         categorical_hinge  Adadelta   \n",
              "132        1000      sigmoid  categorical_crossentropy      adam   \n",
              "81          500      sigmoid         categorical_hinge      adam   \n",
              "62          500      softmax  categorical_crossentropy  Adadelta   \n",
              "105         500         tanh         categorical_hinge      adam   \n",
              "66          500      softmax        mean_squared_error      adam   \n",
              "126        1000      softmax        mean_squared_error      adam   \n",
              "6           100      softmax        mean_squared_error      adam   \n",
              "60          500      softmax  categorical_crossentropy      adam   \n",
              "110         500  exponential  categorical_crossentropy  Adadelta   \n",
              "120        1000      softmax  categorical_crossentropy      adam   \n",
              "63          500      softmax       binary_crossentropy      adam   \n",
              "42          100         tanh        mean_squared_error      adam   \n",
              "3           100      softmax       binary_crossentropy      adam   \n",
              "123        1000      softmax       binary_crossentropy      adam   \n",
              "162        1000         tanh        mean_squared_error      adam   \n",
              "102         500         tanh        mean_squared_error      adam   \n",
              "45          100         tanh         categorical_hinge      adam   \n",
              "78          500      sigmoid        mean_squared_error      adam   \n",
              "12          100      sigmoid  categorical_crossentropy      adam   \n",
              "108         500  exponential  categorical_crossentropy      adam   \n",
              "134        1000      sigmoid  categorical_crossentropy  Adadelta   \n",
              "138        1000      sigmoid        mean_squared_error      adam   \n",
              "75          500      sigmoid       binary_crossentropy      adam   \n",
              "168        1000  exponential  categorical_crossentropy      adam   \n",
              "167        1000         tanh         categorical_hinge  Adadelta   \n",
              "170        1000  exponential  categorical_crossentropy  Adadelta   \n",
              "135        1000      sigmoid       binary_crossentropy      adam   \n",
              "107         500         tanh         categorical_hinge  Adadelta   \n",
              "72          500      sigmoid  categorical_crossentropy      adam   \n",
              "48          100  exponential  categorical_crossentropy      adam   \n",
              "18          100      sigmoid        mean_squared_error      adam   \n",
              "164        1000         tanh        mean_squared_error  Adadelta   \n",
              "0           100      softmax  categorical_crossentropy      adam   \n",
              "87          500         relu       binary_crossentropy      adam   \n",
              "15          100      sigmoid       binary_crossentropy      adam   \n",
              "122        1000      softmax  categorical_crossentropy  Adadelta   \n",
              "50          100  exponential  categorical_crossentropy  Adadelta   \n",
              "95          500         relu         categorical_hinge  Adadelta   \n",
              "74          500      sigmoid  categorical_crossentropy  Adadelta   \n",
              "111         500  exponential       binary_crossentropy      adam   \n",
              "2           100      softmax  categorical_crossentropy  Adadelta   \n",
              "32          100         relu        mean_squared_error  Adadelta   \n",
              "\n",
              "    K_Accurasy(+/-sd) Accurasy(+/-sd) Recall(+/-sd) Precision(+/-sd)  \\\n",
              "119        67.2+/-2.0      67.2+/-2.0    32.7+/-6.3      46.5+/-16.1   \n",
              "141        70.7+/-1.0      70.7+/-1.0    32.6+/-2.0       45.6+/-3.1   \n",
              "177        64.9+/-3.0      64.9+/-3.0    31.8+/-2.9       41.3+/-5.5   \n",
              "57         65.9+/-2.0      65.9+/-2.0    31.8+/-1.5       38.7+/-6.1   \n",
              "117        66.2+/-0.7      66.2+/-0.7    31.3+/-0.8       38.9+/-4.3   \n",
              "165        68.9+/-0.6      68.9+/-0.6    30.5+/-0.4       44.7+/-3.3   \n",
              "59         66.2+/-2.5      66.2+/-2.5    31.0+/-0.7       36.9+/-5.4   \n",
              "179        68.1+/-0.3      68.1+/-0.3    29.3+/-2.1       47.2+/-1.5   \n",
              "132        68.3+/-1.7      68.3+/-1.7    29.4+/-2.1       47.6+/-1.0   \n",
              "81         68.1+/-2.0      68.1+/-2.0    29.5+/-1.8       42.9+/-4.2   \n",
              "62         68.0+/-1.0      68.0+/-1.0    28.9+/-0.4       47.9+/-2.0   \n",
              "105        68.3+/-0.8      68.3+/-0.8    28.8+/-0.7       43.8+/-4.0   \n",
              "66         68.6+/-1.2      68.6+/-1.2    29.1+/-1.1       47.1+/-0.5   \n",
              "126        68.6+/-0.7      68.6+/-0.7    29.2+/-0.9       45.2+/-4.2   \n",
              "6          67.3+/-1.3      67.3+/-1.3    28.8+/-1.4       43.9+/-4.5   \n",
              "60         68.5+/-1.2      68.5+/-1.2    28.9+/-1.5       47.2+/-0.5   \n",
              "110        68.0+/-1.1      68.0+/-1.1    28.5+/-1.7       47.9+/-0.5   \n",
              "120        67.6+/-1.2      67.6+/-1.2    28.6+/-1.2       46.9+/-0.8   \n",
              "63         87.7+/-0.2      69.1+/-0.3    28.9+/-0.1       48.0+/-0.6   \n",
              "42         66.2+/-1.7      66.2+/-1.7    28.1+/-2.0       44.3+/-1.9   \n",
              "3          87.5+/-0.2      68.5+/-1.0    28.6+/-1.0       41.1+/-6.9   \n",
              "123        87.3+/-0.3      68.3+/-0.5    28.6+/-0.4       47.3+/-0.1   \n",
              "162        66.5+/-2.0      66.5+/-2.0    27.7+/-1.2       47.4+/-0.8   \n",
              "102        66.8+/-0.8      66.8+/-0.8    27.8+/-1.6       47.4+/-1.9   \n",
              "45         66.2+/-1.0      66.2+/-1.0    27.8+/-1.3       44.7+/-4.3   \n",
              "78         68.3+/-1.3      68.3+/-1.3    28.3+/-0.9       47.7+/-0.5   \n",
              "12         66.2+/-0.7      66.2+/-0.7    27.9+/-2.3       41.1+/-5.2   \n",
              "108        67.6+/-1.0      67.6+/-1.0    28.2+/-0.4       43.7+/-6.2   \n",
              "134        64.7+/-1.0      64.7+/-1.0    28.2+/-1.9       45.0+/-1.8   \n",
              "138        67.8+/-0.5      67.8+/-0.5    27.9+/-0.4       47.8+/-0.7   \n",
              "75         86.9+/-0.2      67.8+/-0.5    27.9+/-0.2       47.7+/-0.8   \n",
              "168        67.6+/-0.3      67.6+/-0.3    28.0+/-0.1       46.7+/-0.1   \n",
              "167        66.7+/-1.2      66.7+/-1.2    27.6+/-2.9       45.4+/-4.0   \n",
              "170        67.2+/-0.6      67.2+/-0.6    27.5+/-0.9       44.9+/-6.0   \n",
              "135        87.1+/-0.4      67.6+/-0.6    27.8+/-0.4       47.0+/-0.8   \n",
              "107        67.3+/-2.1      67.3+/-2.1    27.5+/-1.6       42.2+/-5.8   \n",
              "72         66.7+/-1.6      66.7+/-1.6    27.4+/-0.8       35.6+/-2.8   \n",
              "48         66.8+/-0.5      66.8+/-0.5    27.4+/-0.3       43.0+/-5.6   \n",
              "18         66.5+/-1.2      66.5+/-1.2    27.1+/-1.5       46.7+/-1.3   \n",
              "164        66.3+/-0.8      66.3+/-0.8    26.8+/-0.5       47.6+/-1.5   \n",
              "0          67.3+/-0.5      67.3+/-0.5    27.2+/-1.0      40.3+/-11.2   \n",
              "87         85.6+/-0.3      64.7+/-2.3    27.4+/-2.0       28.5+/-8.8   \n",
              "15         86.5+/-0.5      66.8+/-0.5    26.9+/-0.6      40.7+/-12.3   \n",
              "122        65.7+/-0.7      65.7+/-0.7    26.3+/-1.1       43.2+/-6.7   \n",
              "50         66.3+/-0.5      66.3+/-0.5    26.3+/-0.5       47.7+/-0.5   \n",
              "95         65.7+/-3.3      65.7+/-3.3    27.2+/-0.9       24.9+/-1.7   \n",
              "74         64.7+/-0.7      64.7+/-0.7    26.1+/-1.9       46.6+/-3.1   \n",
              "111        82.5+/-0.3      65.2+/-1.2    25.9+/-2.3      37.4+/-10.8   \n",
              "2          66.0+/-0.3      66.0+/-0.3    25.8+/-0.5       47.2+/-0.7   \n",
              "32         65.7+/-1.7      65.7+/-1.7    25.6+/-1.4       48.3+/-1.8   \n",
              "\n",
              "    F1_score(+/-sd)  \n",
              "119      34.7+/-8.9  \n",
              "141      34.3+/-2.0  \n",
              "177      33.2+/-3.7  \n",
              "57       33.1+/-2.5  \n",
              "117      32.4+/-1.3  \n",
              "165      32.0+/-0.3  \n",
              "59       31.2+/-1.7  \n",
              "179      30.3+/-2.8  \n",
              "132      30.2+/-2.9  \n",
              "81       30.2+/-2.5  \n",
              "62       30.0+/-0.5  \n",
              "105      29.7+/-1.1  \n",
              "66       29.6+/-1.5  \n",
              "126      29.6+/-1.3  \n",
              "6        29.5+/-1.8  \n",
              "60       29.3+/-2.1  \n",
              "110      29.1+/-2.2  \n",
              "120      29.1+/-1.8  \n",
              "63       29.1+/-0.2  \n",
              "42       28.9+/-2.9  \n",
              "3        28.9+/-1.4  \n",
              "123      28.8+/-0.3  \n",
              "162      28.6+/-1.3  \n",
              "102      28.4+/-2.2  \n",
              "45       28.4+/-1.9  \n",
              "78       28.4+/-1.0  \n",
              "12       28.3+/-3.2  \n",
              "108      28.3+/-0.5  \n",
              "134      28.1+/-2.3  \n",
              "138      28.1+/-0.4  \n",
              "75       28.1+/-0.2  \n",
              "168      28.1+/-0.1  \n",
              "167      27.9+/-3.8  \n",
              "170      27.9+/-1.4  \n",
              "135      27.9+/-0.5  \n",
              "107      27.7+/-2.3  \n",
              "72       27.4+/-0.9  \n",
              "48       27.4+/-0.2  \n",
              "18       27.3+/-2.3  \n",
              "164      26.9+/-0.9  \n",
              "0        26.8+/-1.6  \n",
              "87       26.6+/-3.4  \n",
              "15       26.5+/-1.3  \n",
              "122      26.1+/-1.5  \n",
              "50       26.0+/-0.6  \n",
              "95       25.6+/-1.1  \n",
              "74       25.5+/-2.6  \n",
              "111      25.3+/-3.3  \n",
              "2        25.3+/-0.8  \n",
              "32       25.1+/-1.9  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "WWRawvkJxsS_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.3.6.1 Conclusions\n",
        "Βased exclusively on reported Accuracy values I concluse the following:\n",
        "\n",
        "- the overwhelming majority of models perform relatively well (Accuracy > 60%) .\n",
        "- best performing models approache Accuracy of 70%.\n",
        "- with respect to critical parameters' values of best performing models:\n",
        "\t- optimizer: 'adam' is the clear winner.\n",
        "\t- output_dim: low vector's dimensionality (100) performs equally well compared to higher values (500 or 1000). This is expected given the relatively small size of the vocabulary used.\n",
        "\t- activation: in most cases 'softmax' performs very well followed by 'sigmoid' and 'tanh'.\n",
        "\t- losses: All examined values ('categorical_crossentropy', 'binary_crossentropy', 'mean_squared_error', 'categorical_hinge') perfom equally well."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "NsLqUoOfmFAo"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.7 Batch Size impact on model's performance\n",
        "\n",
        "Since I forgot to incude it to initial set of hyperparameters examined above, I perform a series of separate experiments concerning only batch_size. For other hyperparameters I use the best values found above."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wPceS9y4x9rQ"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.3.7.1 Model dedinition\n",
        "\n",
        "Slightly modified model than the one used above"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CyfTaXS1mFAw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def annDefBatch(batch):\n",
        "  # Define Model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=100, input_length=max_length))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(5, activation='softmax'))\n",
        "  print(model.summary())\n",
        "\n",
        "  # Compile Network\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  # Fit Network to training data\n",
        "  model.fit(X_train_pad, y_train, batch_size=batch, epochs=15, verbose=2, \n",
        "            validation_data=(X_test_pad, y_test))\n",
        "\n",
        "  # Evaluate Network during Training\n",
        "  loss, acc = model.evaluate(X_test_pad, y_test, verbose=2)\n",
        "  print('Test Accuracy: %f' % (acc*100))\n",
        "  \n",
        "  return(model, acc)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tEIexKrNmFBB"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.3.7.2 Model evaluation function\n",
        "\n",
        "I use exactly the same evaluation funcion defined above (3.3.3)"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "99vqJyb-mFBj"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.3.7.3 Execute models\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "d0dc00e9-55d2-4392-d1a9-27c005b05877",
        "id": "6xV4OiiYmFBu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16883
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
        "\n",
        "# Dataframe to store total results\n",
        "headers=['Batch Size','K_Accurasy(+/-sd)','Accurasy(+/-sd)','Recall(+/-sd)',\n",
        "         'Precision(+/-sd)','F1_score(+/-sd)']\n",
        "headers2 = ['K_Accurasy','Accurasy','Recall','Precision','F1_score']\n",
        "\n",
        "modelsResults2 = pd.DataFrame(columns=headers)\n",
        "\n",
        "# Model's parameters' range\n",
        "batch_size = [8,16,32,64,128,512,1024]\n",
        "\n",
        "\n",
        "for batch in batch_size:\n",
        "\n",
        "  parameters = [batch]\n",
        "  # List to store the results of each set\n",
        "  results = []\n",
        "  # Dataframe to store the results of each run\n",
        "  temp = pd.DataFrame(columns=headers2)\n",
        "\n",
        "  # Execute model 3 times for each batch size\n",
        "  for i in range(3):\n",
        "    model, kaccuracy = annDefBatch(batch)\n",
        "    (accuracy,recall,precision,f1) = evalModel(model)\n",
        "    # Store run results as a new row to dataframe\n",
        "    new_row = pd.Series([kaccuracy,accuracy,recall,precision,f1], index=headers2)\n",
        "    temp = temp.append(new_row, ignore_index=1)\n",
        "    # Clean memory for next run\n",
        "    del model,new_row,kaccuracy,accuracy,recall,precision,f1\n",
        "\n",
        "  # Get the mean statistics of the 3 executions\n",
        "  kacc = temp.K_Accurasy.mean()\n",
        "  kaccSd = temp.K_Accurasy.std()\n",
        "  acc = temp.Accurasy.mean()\n",
        "  accSd = temp.Accurasy.std()\n",
        "  rec = temp.Recall.mean()\n",
        "  recSd = temp.Recall.std()\n",
        "  pre = temp.Precision.mean()\n",
        "  preSd = temp.Precision.std()\n",
        "  f1 = temp.F1_score.mean()\n",
        "  f1Sd = temp.F1_score.std()\n",
        "\n",
        "  # Statistics as stings\n",
        "  kaccStr = '%04.1f+/-%.1f' % (100*kacc, 100*kaccSd)\n",
        "  accStr = '%04.1f+/-%.1f' % (100*acc, 100*accSd)\n",
        "  recStr = '%04.1f+/-%.1f' % (100*rec, 100*recSd)\n",
        "  preStr = '%04.1f+/-%.1f' % (100*pre, 100*preSd)\n",
        "  f1Str = '%04.1f+/-%.1f' % (100*f1, 100*f1Sd)\n",
        "\n",
        "  stats = [kaccStr,accStr,recStr,preStr,f1Str]\n",
        "\n",
        "  # Store the final results of set\n",
        "  results.extend(parameters)\n",
        "  results.extend(stats)\n",
        "\n",
        "  new_row = pd.Series(results, index=headers)\n",
        "  modelsResults2 = modelsResults2.append(new_row, ignore_index=1)\n",
        "\n",
        "  # Clean memory for next run\n",
        "  del temp,results,new_row,parameters,stats,accStr,recStr,preStr,\n",
        "  f1Str,acc,accSd,rec,recSd,pre,preSd,f1,f1Str,kacc,kaccSd\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_49 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_49 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.0754 - acc: 0.5866 - val_loss: 1.0509 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 1s - loss: 0.8653 - acc: 0.6537 - val_loss: 1.0020 - val_acc: 0.6146\n",
            "Epoch 3/15\n",
            " - 1s - loss: 0.4859 - acc: 0.8793 - val_loss: 0.9124 - val_acc: 0.6732\n",
            "Epoch 4/15\n",
            " - 1s - loss: 0.2031 - acc: 0.9659 - val_loss: 0.8836 - val_acc: 0.6780\n",
            "Epoch 5/15\n",
            " - 1s - loss: 0.0855 - acc: 0.9963 - val_loss: 0.8738 - val_acc: 0.6927\n",
            "Epoch 6/15\n",
            " - 1s - loss: 0.0421 - acc: 0.9988 - val_loss: 0.8760 - val_acc: 0.6878\n",
            "Epoch 7/15\n",
            " - 1s - loss: 0.0249 - acc: 0.9988 - val_loss: 0.8769 - val_acc: 0.6927\n",
            "Epoch 8/15\n",
            " - 1s - loss: 0.0163 - acc: 1.0000 - val_loss: 0.8896 - val_acc: 0.6878\n",
            "Epoch 9/15\n",
            " - 1s - loss: 0.0115 - acc: 1.0000 - val_loss: 0.8971 - val_acc: 0.6927\n",
            "Epoch 10/15\n",
            " - 1s - loss: 0.0085 - acc: 1.0000 - val_loss: 0.9055 - val_acc: 0.6927\n",
            "Epoch 11/15\n",
            " - 1s - loss: 0.0065 - acc: 1.0000 - val_loss: 0.9136 - val_acc: 0.6927\n",
            "Epoch 12/15\n",
            " - 1s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.9188 - val_acc: 0.6927\n",
            "Epoch 13/15\n",
            " - 1s - loss: 0.0042 - acc: 1.0000 - val_loss: 0.9251 - val_acc: 0.6976\n",
            "Epoch 14/15\n",
            " - 1s - loss: 0.0035 - acc: 1.0000 - val_loss: 0.9329 - val_acc: 0.6927\n",
            "Epoch 15/15\n",
            " - 1s - loss: 0.0030 - acc: 1.0000 - val_loss: 0.9397 - val_acc: 0.6976\n",
            "Test Accuracy: 69.756098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_50 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_50 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.0860 - acc: 0.5951 - val_loss: 1.0564 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 1s - loss: 0.8601 - acc: 0.6488 - val_loss: 0.9861 - val_acc: 0.6146\n",
            "Epoch 3/15\n",
            " - 1s - loss: 0.4835 - acc: 0.8878 - val_loss: 0.9048 - val_acc: 0.6732\n",
            "Epoch 4/15\n",
            " - 1s - loss: 0.2008 - acc: 0.9695 - val_loss: 0.8806 - val_acc: 0.6927\n",
            "Epoch 5/15\n",
            " - 1s - loss: 0.0857 - acc: 0.9963 - val_loss: 0.8713 - val_acc: 0.6878\n",
            "Epoch 6/15\n",
            " - 1s - loss: 0.0431 - acc: 0.9988 - val_loss: 0.8736 - val_acc: 0.6732\n",
            "Epoch 7/15\n",
            " - 1s - loss: 0.0256 - acc: 0.9988 - val_loss: 0.8820 - val_acc: 0.6780\n",
            "Epoch 8/15\n",
            " - 1s - loss: 0.0165 - acc: 1.0000 - val_loss: 0.8871 - val_acc: 0.6732\n",
            "Epoch 9/15\n",
            " - 1s - loss: 0.0115 - acc: 1.0000 - val_loss: 0.8966 - val_acc: 0.6780\n",
            "Epoch 10/15\n",
            " - 1s - loss: 0.0085 - acc: 1.0000 - val_loss: 0.9033 - val_acc: 0.6732\n",
            "Epoch 11/15\n",
            " - 1s - loss: 0.0066 - acc: 1.0000 - val_loss: 0.9143 - val_acc: 0.6780\n",
            "Epoch 12/15\n",
            " - 1s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.9232 - val_acc: 0.6780\n",
            "Epoch 13/15\n",
            " - 1s - loss: 0.0042 - acc: 1.0000 - val_loss: 0.9311 - val_acc: 0.6780\n",
            "Epoch 14/15\n",
            " - 1s - loss: 0.0035 - acc: 1.0000 - val_loss: 0.9395 - val_acc: 0.6732\n",
            "Epoch 15/15\n",
            " - 1s - loss: 0.0030 - acc: 1.0000 - val_loss: 0.9455 - val_acc: 0.6732\n",
            "Test Accuracy: 67.317073\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_51 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_51 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.0918 - acc: 0.5890 - val_loss: 1.0560 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 1s - loss: 0.8750 - acc: 0.6659 - val_loss: 0.9990 - val_acc: 0.6098\n",
            "Epoch 3/15\n",
            " - 1s - loss: 0.4929 - acc: 0.8829 - val_loss: 0.9271 - val_acc: 0.6683\n",
            "Epoch 4/15\n",
            " - 1s - loss: 0.2039 - acc: 0.9622 - val_loss: 0.9063 - val_acc: 0.6829\n",
            "Epoch 5/15\n",
            " - 1s - loss: 0.0859 - acc: 0.9976 - val_loss: 0.8818 - val_acc: 0.6683\n",
            "Epoch 6/15\n",
            " - 1s - loss: 0.0434 - acc: 0.9976 - val_loss: 0.8840 - val_acc: 0.6829\n",
            "Epoch 7/15\n",
            " - 1s - loss: 0.0256 - acc: 0.9988 - val_loss: 0.8938 - val_acc: 0.6878\n",
            "Epoch 8/15\n",
            " - 1s - loss: 0.0164 - acc: 1.0000 - val_loss: 0.8971 - val_acc: 0.6829\n",
            "Epoch 9/15\n",
            " - 1s - loss: 0.0115 - acc: 1.0000 - val_loss: 0.9069 - val_acc: 0.6829\n",
            "Epoch 10/15\n",
            " - 1s - loss: 0.0085 - acc: 1.0000 - val_loss: 0.9147 - val_acc: 0.6829\n",
            "Epoch 11/15\n",
            " - 1s - loss: 0.0066 - acc: 1.0000 - val_loss: 0.9202 - val_acc: 0.6829\n",
            "Epoch 12/15\n",
            " - 1s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.9302 - val_acc: 0.6829\n",
            "Epoch 13/15\n",
            " - 1s - loss: 0.0043 - acc: 1.0000 - val_loss: 0.9378 - val_acc: 0.6829\n",
            "Epoch 14/15\n",
            " - 1s - loss: 0.0035 - acc: 1.0000 - val_loss: 0.9428 - val_acc: 0.6829\n",
            "Epoch 15/15\n",
            " - 1s - loss: 0.0030 - acc: 1.0000 - val_loss: 0.9507 - val_acc: 0.6829\n",
            "Test Accuracy: 68.292683\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_52 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_52 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.0952 - acc: 0.5951 - val_loss: 1.0623 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.9414 - acc: 0.6037 - val_loss: 1.0266 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.7181 - acc: 0.8000 - val_loss: 0.9691 - val_acc: 0.6293\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.4347 - acc: 0.9000 - val_loss: 0.9166 - val_acc: 0.6732\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.2342 - acc: 0.9524 - val_loss: 0.8837 - val_acc: 0.6683\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.1258 - acc: 0.9890 - val_loss: 0.8807 - val_acc: 0.6780\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.0711 - acc: 0.9976 - val_loss: 0.8714 - val_acc: 0.6780\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.0439 - acc: 0.9988 - val_loss: 0.8706 - val_acc: 0.6780\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0299 - acc: 0.9988 - val_loss: 0.8742 - val_acc: 0.6780\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0215 - acc: 0.9988 - val_loss: 0.8795 - val_acc: 0.6780\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0163 - acc: 1.0000 - val_loss: 0.8852 - val_acc: 0.6829\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0127 - acc: 1.0000 - val_loss: 0.8929 - val_acc: 0.6780\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0102 - acc: 1.0000 - val_loss: 0.8972 - val_acc: 0.6829\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 0.9057 - val_acc: 0.6780\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 0.9093 - val_acc: 0.6780\n",
            "Test Accuracy: 67.804878\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_53 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_53 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.1179 - acc: 0.5890 - val_loss: 1.0455 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.9391 - acc: 0.6012 - val_loss: 1.0237 - val_acc: 0.5951\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.7282 - acc: 0.7524 - val_loss: 0.9715 - val_acc: 0.6341\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.4351 - acc: 0.9061 - val_loss: 0.9245 - val_acc: 0.6683\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.2307 - acc: 0.9549 - val_loss: 0.9001 - val_acc: 0.6732\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.1247 - acc: 0.9902 - val_loss: 0.8817 - val_acc: 0.6829\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.0710 - acc: 0.9976 - val_loss: 0.8818 - val_acc: 0.7024\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.0441 - acc: 0.9988 - val_loss: 0.8768 - val_acc: 0.6878\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0298 - acc: 0.9988 - val_loss: 0.8790 - val_acc: 0.6780\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0214 - acc: 0.9988 - val_loss: 0.8819 - val_acc: 0.6780\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0161 - acc: 1.0000 - val_loss: 0.8874 - val_acc: 0.6780\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0126 - acc: 1.0000 - val_loss: 0.8936 - val_acc: 0.6780\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0102 - acc: 1.0000 - val_loss: 0.8961 - val_acc: 0.6780\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 0.8992 - val_acc: 0.6780\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 0.9065 - val_acc: 0.6829\n",
            "Test Accuracy: 68.292683\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_54 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_54 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.1220 - acc: 0.5878 - val_loss: 1.0515 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.9423 - acc: 0.6000 - val_loss: 1.0308 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.7273 - acc: 0.7512 - val_loss: 0.9825 - val_acc: 0.6244\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.4385 - acc: 0.9024 - val_loss: 0.9395 - val_acc: 0.6390\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.2404 - acc: 0.9549 - val_loss: 0.9192 - val_acc: 0.6488\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.1268 - acc: 0.9927 - val_loss: 0.8992 - val_acc: 0.6683\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.0724 - acc: 0.9976 - val_loss: 0.8896 - val_acc: 0.6780\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.0447 - acc: 0.9988 - val_loss: 0.8904 - val_acc: 0.6732\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0304 - acc: 0.9988 - val_loss: 0.8934 - val_acc: 0.6878\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0220 - acc: 0.9988 - val_loss: 0.8943 - val_acc: 0.6878\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0165 - acc: 1.0000 - val_loss: 0.9017 - val_acc: 0.6829\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0129 - acc: 1.0000 - val_loss: 0.9033 - val_acc: 0.6829\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0103 - acc: 1.0000 - val_loss: 0.9119 - val_acc: 0.6829\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0084 - acc: 1.0000 - val_loss: 0.9167 - val_acc: 0.6829\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0071 - acc: 1.0000 - val_loss: 0.9242 - val_acc: 0.6829\n",
            "Test Accuracy: 68.292683\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_55 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_55 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.1650 - acc: 0.5634 - val_loss: 1.0560 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.9831 - acc: 0.6000 - val_loss: 1.0387 - val_acc: 0.5951\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.8710 - acc: 0.6061 - val_loss: 1.0217 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.6935 - acc: 0.8573 - val_loss: 0.9868 - val_acc: 0.6195\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.4868 - acc: 0.8878 - val_loss: 0.9564 - val_acc: 0.6341\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.3232 - acc: 0.9268 - val_loss: 0.9368 - val_acc: 0.6585\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.2089 - acc: 0.9707 - val_loss: 0.9123 - val_acc: 0.6634\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.1340 - acc: 0.9902 - val_loss: 0.9034 - val_acc: 0.6634\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0890 - acc: 0.9951 - val_loss: 0.8963 - val_acc: 0.6585\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0620 - acc: 0.9988 - val_loss: 0.8928 - val_acc: 0.6488\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0445 - acc: 0.9988 - val_loss: 0.8911 - val_acc: 0.6439\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0335 - acc: 0.9988 - val_loss: 0.8913 - val_acc: 0.6439\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0263 - acc: 0.9988 - val_loss: 0.8933 - val_acc: 0.6488\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0211 - acc: 0.9988 - val_loss: 0.8949 - val_acc: 0.6488\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0173 - acc: 1.0000 - val_loss: 0.8972 - val_acc: 0.6488\n",
            "Test Accuracy: 64.878049\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_56 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_56 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.1663 - acc: 0.5646 - val_loss: 1.0565 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.9783 - acc: 0.6000 - val_loss: 1.0410 - val_acc: 0.5951\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.8685 - acc: 0.6159 - val_loss: 1.0198 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.6894 - acc: 0.8207 - val_loss: 0.9878 - val_acc: 0.6146\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.4852 - acc: 0.8890 - val_loss: 0.9503 - val_acc: 0.6537\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.3218 - acc: 0.9354 - val_loss: 0.9257 - val_acc: 0.6585\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.2055 - acc: 0.9646 - val_loss: 0.9133 - val_acc: 0.6732\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.1330 - acc: 0.9915 - val_loss: 0.9021 - val_acc: 0.6780\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0878 - acc: 0.9951 - val_loss: 0.8965 - val_acc: 0.6780\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0605 - acc: 0.9988 - val_loss: 0.8944 - val_acc: 0.6780\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0440 - acc: 0.9988 - val_loss: 0.8942 - val_acc: 0.6683\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0329 - acc: 0.9988 - val_loss: 0.8942 - val_acc: 0.6683\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0255 - acc: 0.9988 - val_loss: 0.8967 - val_acc: 0.6683\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0206 - acc: 1.0000 - val_loss: 0.8999 - val_acc: 0.6683\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0170 - acc: 1.0000 - val_loss: 0.9035 - val_acc: 0.6683\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_57 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_57 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.1604 - acc: 0.5854 - val_loss: 1.0559 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 0s - loss: 0.9773 - acc: 0.6000 - val_loss: 1.0429 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.8728 - acc: 0.6232 - val_loss: 1.0263 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.7010 - acc: 0.7939 - val_loss: 1.0017 - val_acc: 0.6244\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.4971 - acc: 0.8854 - val_loss: 0.9591 - val_acc: 0.6390\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.3290 - acc: 0.9329 - val_loss: 0.9394 - val_acc: 0.6488\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.2127 - acc: 0.9671 - val_loss: 0.9222 - val_acc: 0.6732\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.1374 - acc: 0.9854 - val_loss: 0.9121 - val_acc: 0.6683\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.0922 - acc: 0.9963 - val_loss: 0.9066 - val_acc: 0.6780\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.0631 - acc: 0.9976 - val_loss: 0.9011 - val_acc: 0.6780\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.0453 - acc: 0.9988 - val_loss: 0.8976 - val_acc: 0.6780\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.0343 - acc: 0.9988 - val_loss: 0.9009 - val_acc: 0.6829\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0264 - acc: 0.9988 - val_loss: 0.9039 - val_acc: 0.6732\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0212 - acc: 0.9988 - val_loss: 0.9051 - val_acc: 0.6732\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0177 - acc: 1.0000 - val_loss: 0.9040 - val_acc: 0.6683\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_58 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_58 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.2447 - acc: 0.5476 - val_loss: 1.0653 - val_acc: 0.6000\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.0232 - acc: 0.6000 - val_loss: 1.0562 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.9491 - acc: 0.6000 - val_loss: 1.0366 - val_acc: 0.5951\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.8714 - acc: 0.6061 - val_loss: 1.0250 - val_acc: 0.5951\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.7617 - acc: 0.7110 - val_loss: 1.0072 - val_acc: 0.6146\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.6351 - acc: 0.8683 - val_loss: 0.9843 - val_acc: 0.6244\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.5020 - acc: 0.8854 - val_loss: 0.9664 - val_acc: 0.6293\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.3841 - acc: 0.9146 - val_loss: 0.9456 - val_acc: 0.6390\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.2878 - acc: 0.9439 - val_loss: 0.9328 - val_acc: 0.6439\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.2143 - acc: 0.9634 - val_loss: 0.9221 - val_acc: 0.6488\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.1591 - acc: 0.9805 - val_loss: 0.9143 - val_acc: 0.6537\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.1192 - acc: 0.9927 - val_loss: 0.9068 - val_acc: 0.6537\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0910 - acc: 0.9951 - val_loss: 0.9048 - val_acc: 0.6585\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0700 - acc: 0.9988 - val_loss: 0.9001 - val_acc: 0.6634\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0552 - acc: 0.9988 - val_loss: 0.8966 - val_acc: 0.6634\n",
            "Test Accuracy: 66.341464\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_59 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_59 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.2567 - acc: 0.4841 - val_loss: 1.0684 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.0259 - acc: 0.6000 - val_loss: 1.0567 - val_acc: 0.5951\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.9514 - acc: 0.6000 - val_loss: 1.0433 - val_acc: 0.5951\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.8704 - acc: 0.6037 - val_loss: 1.0312 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.7654 - acc: 0.6646 - val_loss: 1.0173 - val_acc: 0.6098\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.6386 - acc: 0.8732 - val_loss: 0.9985 - val_acc: 0.6244\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.5024 - acc: 0.8878 - val_loss: 0.9765 - val_acc: 0.6390\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.3812 - acc: 0.9171 - val_loss: 0.9591 - val_acc: 0.6439\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.2850 - acc: 0.9463 - val_loss: 0.9465 - val_acc: 0.6439\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.2110 - acc: 0.9695 - val_loss: 0.9389 - val_acc: 0.6537\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.1562 - acc: 0.9902 - val_loss: 0.9295 - val_acc: 0.6537\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.1161 - acc: 0.9939 - val_loss: 0.9213 - val_acc: 0.6585\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0879 - acc: 0.9976 - val_loss: 0.9176 - val_acc: 0.6634\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0684 - acc: 0.9988 - val_loss: 0.9174 - val_acc: 0.6634\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0539 - acc: 0.9988 - val_loss: 0.9146 - val_acc: 0.6732\n",
            "Test Accuracy: 67.317073\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_60 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_60 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.2455 - acc: 0.5500 - val_loss: 1.0662 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.0304 - acc: 0.6000 - val_loss: 1.0606 - val_acc: 0.5951\n",
            "Epoch 3/15\n",
            " - 0s - loss: 0.9595 - acc: 0.6000 - val_loss: 1.0423 - val_acc: 0.5951\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.8780 - acc: 0.6049 - val_loss: 1.0301 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.7676 - acc: 0.6866 - val_loss: 1.0138 - val_acc: 0.6049\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.6368 - acc: 0.8549 - val_loss: 0.9958 - val_acc: 0.6244\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.5028 - acc: 0.8939 - val_loss: 0.9734 - val_acc: 0.6293\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.3828 - acc: 0.9061 - val_loss: 0.9522 - val_acc: 0.6439\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.2853 - acc: 0.9476 - val_loss: 0.9453 - val_acc: 0.6585\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.2110 - acc: 0.9683 - val_loss: 0.9251 - val_acc: 0.6585\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.1552 - acc: 0.9793 - val_loss: 0.9156 - val_acc: 0.6634\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.1153 - acc: 0.9927 - val_loss: 0.9103 - val_acc: 0.6683\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.0881 - acc: 0.9976 - val_loss: 0.9065 - val_acc: 0.6683\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.0678 - acc: 0.9976 - val_loss: 0.9001 - val_acc: 0.6683\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.0534 - acc: 0.9988 - val_loss: 0.8980 - val_acc: 0.6683\n",
            "Test Accuracy: 66.829268\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_61 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_61 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.3761 - acc: 0.5439 - val_loss: 1.1261 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.0575 - acc: 0.6000 - val_loss: 1.0659 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.0111 - acc: 0.6000 - val_loss: 1.0602 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.9568 - acc: 0.6000 - val_loss: 1.0414 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.9026 - acc: 0.6000 - val_loss: 1.0368 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.8456 - acc: 0.6073 - val_loss: 1.0271 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.7725 - acc: 0.6780 - val_loss: 1.0140 - val_acc: 0.6049\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.6905 - acc: 0.8366 - val_loss: 1.0008 - val_acc: 0.6146\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.6056 - acc: 0.8720 - val_loss: 0.9855 - val_acc: 0.6146\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.5179 - acc: 0.8854 - val_loss: 0.9717 - val_acc: 0.6293\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.4374 - acc: 0.9049 - val_loss: 0.9603 - val_acc: 0.6341\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.3652 - acc: 0.9232 - val_loss: 0.9481 - val_acc: 0.6439\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.3016 - acc: 0.9439 - val_loss: 0.9359 - val_acc: 0.6488\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.2473 - acc: 0.9573 - val_loss: 0.9266 - val_acc: 0.6585\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.2026 - acc: 0.9659 - val_loss: 0.9206 - val_acc: 0.6585\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_62 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_62 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.3496 - acc: 0.5878 - val_loss: 1.1022 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.0419 - acc: 0.6000 - val_loss: 1.0714 - val_acc: 0.5951\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.0089 - acc: 0.6000 - val_loss: 1.0672 - val_acc: 0.5951\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.9559 - acc: 0.6000 - val_loss: 1.0461 - val_acc: 0.5951\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.8991 - acc: 0.6000 - val_loss: 1.0437 - val_acc: 0.5951\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.8390 - acc: 0.6110 - val_loss: 1.0341 - val_acc: 0.5951\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.7646 - acc: 0.7183 - val_loss: 1.0240 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.6848 - acc: 0.8390 - val_loss: 1.0135 - val_acc: 0.6049\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.5995 - acc: 0.8707 - val_loss: 1.0008 - val_acc: 0.6146\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.5134 - acc: 0.8817 - val_loss: 0.9897 - val_acc: 0.6244\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.4342 - acc: 0.9000 - val_loss: 0.9796 - val_acc: 0.6341\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.3639 - acc: 0.9220 - val_loss: 0.9695 - val_acc: 0.6390\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.3003 - acc: 0.9378 - val_loss: 0.9593 - val_acc: 0.6390\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.2474 - acc: 0.9537 - val_loss: 0.9528 - val_acc: 0.6439\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.2026 - acc: 0.9683 - val_loss: 0.9462 - val_acc: 0.6488\n",
            "Test Accuracy: 64.878049\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_63 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_63 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.3803 - acc: 0.4915 - val_loss: 1.1308 - val_acc: 0.5951\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.0541 - acc: 0.6000 - val_loss: 1.0724 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.0131 - acc: 0.6000 - val_loss: 1.0617 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 0.9557 - acc: 0.6000 - val_loss: 1.0445 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 0.8960 - acc: 0.6012 - val_loss: 1.0360 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.8364 - acc: 0.6146 - val_loss: 1.0279 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.7644 - acc: 0.6988 - val_loss: 1.0175 - val_acc: 0.6098\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.6825 - acc: 0.8280 - val_loss: 1.0047 - val_acc: 0.6098\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.5982 - acc: 0.8732 - val_loss: 0.9927 - val_acc: 0.6195\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.5123 - acc: 0.8841 - val_loss: 0.9800 - val_acc: 0.6341\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.4332 - acc: 0.9024 - val_loss: 0.9679 - val_acc: 0.6439\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.3612 - acc: 0.9171 - val_loss: 0.9564 - val_acc: 0.6439\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.2998 - acc: 0.9415 - val_loss: 0.9459 - val_acc: 0.6488\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.2471 - acc: 0.9561 - val_loss: 0.9391 - val_acc: 0.6439\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.2028 - acc: 0.9720 - val_loss: 0.9321 - val_acc: 0.6585\n",
            "Test Accuracy: 65.853659\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_64 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_64 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.5725 - acc: 0.2134 - val_loss: 1.4247 - val_acc: 0.4244\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.3810 - acc: 0.6024 - val_loss: 1.2806 - val_acc: 0.6000\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.2319 - acc: 0.6451 - val_loss: 1.1691 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.1143 - acc: 0.6000 - val_loss: 1.0930 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.0366 - acc: 0.6000 - val_loss: 1.0593 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 0.9965 - acc: 0.6000 - val_loss: 1.0585 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.9872 - acc: 0.6000 - val_loss: 1.0663 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.9786 - acc: 0.6000 - val_loss: 1.0678 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.9605 - acc: 0.6000 - val_loss: 1.0638 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.9339 - acc: 0.6000 - val_loss: 1.0588 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.9069 - acc: 0.6037 - val_loss: 1.0537 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.8769 - acc: 0.6195 - val_loss: 1.0449 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.8442 - acc: 0.6366 - val_loss: 1.0349 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.8107 - acc: 0.6646 - val_loss: 1.0271 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.7767 - acc: 0.6829 - val_loss: 1.0224 - val_acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_65 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_65 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.5926 - acc: 0.1720 - val_loss: 1.4492 - val_acc: 0.3317\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.4033 - acc: 0.5659 - val_loss: 1.3079 - val_acc: 0.5659\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.2550 - acc: 0.6951 - val_loss: 1.1945 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.1370 - acc: 0.6049 - val_loss: 1.1108 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.0506 - acc: 0.6000 - val_loss: 1.0643 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.0002 - acc: 0.6000 - val_loss: 1.0544 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.9837 - acc: 0.6000 - val_loss: 1.0609 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.9762 - acc: 0.6000 - val_loss: 1.0647 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.9603 - acc: 0.6000 - val_loss: 1.0620 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.9352 - acc: 0.6000 - val_loss: 1.0583 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.9092 - acc: 0.6049 - val_loss: 1.0553 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.8820 - acc: 0.6232 - val_loss: 1.0485 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.8510 - acc: 0.6598 - val_loss: 1.0383 - val_acc: 0.6049\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.8150 - acc: 0.6866 - val_loss: 1.0287 - val_acc: 0.6049\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.7796 - acc: 0.6939 - val_loss: 1.0223 - val_acc: 0.6049\n",
            "Test Accuracy: 60.487805\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_66 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_66 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.6031 - acc: 0.2512 - val_loss: 1.4567 - val_acc: 0.2683\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.4107 - acc: 0.3317 - val_loss: 1.3101 - val_acc: 0.4146\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.2535 - acc: 0.7207 - val_loss: 1.1943 - val_acc: 0.6000\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.1339 - acc: 0.6061 - val_loss: 1.1095 - val_acc: 0.5951\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.0454 - acc: 0.6000 - val_loss: 1.0667 - val_acc: 0.5951\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.0006 - acc: 0.6000 - val_loss: 1.0612 - val_acc: 0.5951\n",
            "Epoch 7/15\n",
            " - 0s - loss: 0.9903 - acc: 0.6000 - val_loss: 1.0693 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 0.9802 - acc: 0.6000 - val_loss: 1.0707 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 0.9617 - acc: 0.6000 - val_loss: 1.0667 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.9360 - acc: 0.6000 - val_loss: 1.0632 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.9098 - acc: 0.6024 - val_loss: 1.0607 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.8818 - acc: 0.6268 - val_loss: 1.0546 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.8512 - acc: 0.6732 - val_loss: 1.0449 - val_acc: 0.5951\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.8158 - acc: 0.7207 - val_loss: 1.0356 - val_acc: 0.5951\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.7813 - acc: 0.7415 - val_loss: 1.0291 - val_acc: 0.5951\n",
            "Test Accuracy: 59.512195\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_67 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_67 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.6321 - acc: 0.0354 - val_loss: 1.5360 - val_acc: 0.4829\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.5267 - acc: 0.5939 - val_loss: 1.4504 - val_acc: 0.5122\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.4318 - acc: 0.7012 - val_loss: 1.3738 - val_acc: 0.5561\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.3461 - acc: 0.7146 - val_loss: 1.3055 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.2690 - acc: 0.6854 - val_loss: 1.2452 - val_acc: 0.6098\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.2001 - acc: 0.6451 - val_loss: 1.1922 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.1388 - acc: 0.6268 - val_loss: 1.1466 - val_acc: 0.5951\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.0853 - acc: 0.6110 - val_loss: 1.1094 - val_acc: 0.5951\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.0403 - acc: 0.6000 - val_loss: 1.0815 - val_acc: 0.5951\n",
            "Epoch 10/15\n",
            " - 0s - loss: 1.0048 - acc: 0.6000 - val_loss: 1.0636 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.9792 - acc: 0.6000 - val_loss: 1.0549 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.9626 - acc: 0.6000 - val_loss: 1.0535 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.9525 - acc: 0.6000 - val_loss: 1.0562 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.9455 - acc: 0.6000 - val_loss: 1.0596 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.9381 - acc: 0.6000 - val_loss: 1.0619 - val_acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_68 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_68 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.6313 - acc: 0.0402 - val_loss: 1.5280 - val_acc: 0.4585\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.5200 - acc: 0.4878 - val_loss: 1.4375 - val_acc: 0.4976\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.4204 - acc: 0.6280 - val_loss: 1.3574 - val_acc: 0.5659\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.3315 - acc: 0.7110 - val_loss: 1.2872 - val_acc: 0.6000\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.2528 - acc: 0.7049 - val_loss: 1.2261 - val_acc: 0.5951\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.1833 - acc: 0.6378 - val_loss: 1.1733 - val_acc: 0.5951\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.1226 - acc: 0.6171 - val_loss: 1.1293 - val_acc: 0.5951\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.0710 - acc: 0.6024 - val_loss: 1.0954 - val_acc: 0.5951\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.0295 - acc: 0.6000 - val_loss: 1.0723 - val_acc: 0.5951\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.9990 - acc: 0.6000 - val_loss: 1.0600 - val_acc: 0.5951\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.9791 - acc: 0.6000 - val_loss: 1.0566 - val_acc: 0.5951\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.9674 - acc: 0.6000 - val_loss: 1.0587 - val_acc: 0.5951\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.9604 - acc: 0.6000 - val_loss: 1.0626 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.9541 - acc: 0.6000 - val_loss: 1.0658 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.9459 - acc: 0.6000 - val_loss: 1.0670 - val_acc: 0.6000\n",
            "Test Accuracy: 60.000000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_69 (Embedding)     (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_69 (Flatten)         (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/15\n",
            " - 3s - loss: 1.6168 - acc: 0.2354 - val_loss: 1.5104 - val_acc: 0.2683\n",
            "Epoch 2/15\n",
            " - 0s - loss: 1.4990 - acc: 0.2854 - val_loss: 1.4166 - val_acc: 0.2683\n",
            "Epoch 3/15\n",
            " - 0s - loss: 1.3962 - acc: 0.3378 - val_loss: 1.3356 - val_acc: 0.3268\n",
            "Epoch 4/15\n",
            " - 0s - loss: 1.3067 - acc: 0.5366 - val_loss: 1.2648 - val_acc: 0.5561\n",
            "Epoch 5/15\n",
            " - 0s - loss: 1.2278 - acc: 0.7585 - val_loss: 1.2028 - val_acc: 0.6000\n",
            "Epoch 6/15\n",
            " - 0s - loss: 1.1582 - acc: 0.6110 - val_loss: 1.1504 - val_acc: 0.6000\n",
            "Epoch 7/15\n",
            " - 0s - loss: 1.0986 - acc: 0.6000 - val_loss: 1.1092 - val_acc: 0.6000\n",
            "Epoch 8/15\n",
            " - 0s - loss: 1.0506 - acc: 0.6000 - val_loss: 1.0808 - val_acc: 0.6000\n",
            "Epoch 9/15\n",
            " - 0s - loss: 1.0154 - acc: 0.6000 - val_loss: 1.0649 - val_acc: 0.6000\n",
            "Epoch 10/15\n",
            " - 0s - loss: 0.9925 - acc: 0.6000 - val_loss: 1.0593 - val_acc: 0.6000\n",
            "Epoch 11/15\n",
            " - 0s - loss: 0.9793 - acc: 0.6000 - val_loss: 1.0599 - val_acc: 0.6000\n",
            "Epoch 12/15\n",
            " - 0s - loss: 0.9714 - acc: 0.6000 - val_loss: 1.0628 - val_acc: 0.6000\n",
            "Epoch 13/15\n",
            " - 0s - loss: 0.9646 - acc: 0.6000 - val_loss: 1.0653 - val_acc: 0.6000\n",
            "Epoch 14/15\n",
            " - 0s - loss: 0.9562 - acc: 0.6000 - val_loss: 1.0661 - val_acc: 0.6000\n",
            "Epoch 15/15\n",
            " - 0s - loss: 0.9450 - acc: 0.6000 - val_loss: 1.0655 - val_acc: 0.6000\n",
            "Test Accuracy: 60.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "c45b28e9-2bb6-4c5b-8b12-a9bb5519209d",
        "id": "WfZdExzYpT9Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# Sort results according to test set Accuracy reported from Keras or Sklearn\n",
        "modelsResults2.sort_values('K_Accurasy(+/-sd)',ascending=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>K_Accurasy(+/-sd)</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>30.1+/-2.5</td>\n",
              "      <td>48.7+/-18.2</td>\n",
              "      <td>31.4+/-4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>29.3+/-1.1</td>\n",
              "      <td>44.5+/-3.1</td>\n",
              "      <td>30.0+/-1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>27.2+/-1.5</td>\n",
              "      <td>40.1+/-11.1</td>\n",
              "      <td>27.1+/-2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>66.2+/-1.1</td>\n",
              "      <td>66.2+/-1.1</td>\n",
              "      <td>28.0+/-1.2</td>\n",
              "      <td>43.3+/-4.1</td>\n",
              "      <td>28.3+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128</td>\n",
              "      <td>65.5+/-0.6</td>\n",
              "      <td>65.5+/-0.6</td>\n",
              "      <td>24.7+/-0.5</td>\n",
              "      <td>26.7+/-0.3</td>\n",
              "      <td>23.2+/-0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>512</td>\n",
              "      <td>60.0+/-0.5</td>\n",
              "      <td>60.0+/-0.5</td>\n",
              "      <td>20.1+/-0.3</td>\n",
              "      <td>18.7+/-11.6</td>\n",
              "      <td>15.2+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1024</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Batch Size K_Accurasy(+/-sd) Accurasy(+/-sd) Recall(+/-sd) Precision(+/-sd)  \\\n",
              "0          8        68.5+/-1.2      68.5+/-1.2    30.1+/-2.5      48.7+/-18.2   \n",
              "1         16        68.1+/-0.3      68.1+/-0.3    29.3+/-1.1       44.5+/-3.1   \n",
              "3         64        66.8+/-0.5      66.8+/-0.5    27.2+/-1.5      40.1+/-11.1   \n",
              "2         32        66.2+/-1.1      66.2+/-1.1    28.0+/-1.2       43.3+/-4.1   \n",
              "4        128        65.5+/-0.6      65.5+/-0.6    24.7+/-0.5       26.7+/-0.3   \n",
              "5        512        60.0+/-0.5      60.0+/-0.5    20.1+/-0.3      18.7+/-11.6   \n",
              "6       1024        60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "\n",
              "  F1_score(+/-sd)  \n",
              "0      31.4+/-4.1  \n",
              "1      30.0+/-1.6  \n",
              "3      27.1+/-2.6  \n",
              "2      28.3+/-1.5  \n",
              "4      23.2+/-0.6  \n",
              "5      15.2+/-0.5  \n",
              "6      15.0+/-0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "4dbbeea0-b2ac-494d-b2ba-b9287ae413fc",
        "id": "IY5xr-4apT92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# Sort results according to test set F1 score reported from Sklearn\n",
        "\n",
        "modelsResults2.sort_values('F1_score(+/-sd)',ascending=False)[:50]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Batch Size</th>\n",
              "      <th>K_Accurasy(+/-sd)</th>\n",
              "      <th>Accurasy(+/-sd)</th>\n",
              "      <th>Recall(+/-sd)</th>\n",
              "      <th>Precision(+/-sd)</th>\n",
              "      <th>F1_score(+/-sd)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>68.5+/-1.2</td>\n",
              "      <td>30.1+/-2.5</td>\n",
              "      <td>48.7+/-18.2</td>\n",
              "      <td>31.4+/-4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>68.1+/-0.3</td>\n",
              "      <td>29.3+/-1.1</td>\n",
              "      <td>44.5+/-3.1</td>\n",
              "      <td>30.0+/-1.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>32</td>\n",
              "      <td>66.2+/-1.1</td>\n",
              "      <td>66.2+/-1.1</td>\n",
              "      <td>28.0+/-1.2</td>\n",
              "      <td>43.3+/-4.1</td>\n",
              "      <td>28.3+/-1.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>64</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>66.8+/-0.5</td>\n",
              "      <td>27.2+/-1.5</td>\n",
              "      <td>40.1+/-11.1</td>\n",
              "      <td>27.1+/-2.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>128</td>\n",
              "      <td>65.5+/-0.6</td>\n",
              "      <td>65.5+/-0.6</td>\n",
              "      <td>24.7+/-0.5</td>\n",
              "      <td>26.7+/-0.3</td>\n",
              "      <td>23.2+/-0.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>512</td>\n",
              "      <td>60.0+/-0.5</td>\n",
              "      <td>60.0+/-0.5</td>\n",
              "      <td>20.1+/-0.3</td>\n",
              "      <td>18.7+/-11.6</td>\n",
              "      <td>15.2+/-0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1024</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>60.0+/-0.0</td>\n",
              "      <td>20.0+/-0.0</td>\n",
              "      <td>12.0+/-0.0</td>\n",
              "      <td>15.0+/-0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Batch Size K_Accurasy(+/-sd) Accurasy(+/-sd) Recall(+/-sd) Precision(+/-sd)  \\\n",
              "0          8        68.5+/-1.2      68.5+/-1.2    30.1+/-2.5      48.7+/-18.2   \n",
              "1         16        68.1+/-0.3      68.1+/-0.3    29.3+/-1.1       44.5+/-3.1   \n",
              "2         32        66.2+/-1.1      66.2+/-1.1    28.0+/-1.2       43.3+/-4.1   \n",
              "3         64        66.8+/-0.5      66.8+/-0.5    27.2+/-1.5      40.1+/-11.1   \n",
              "4        128        65.5+/-0.6      65.5+/-0.6    24.7+/-0.5       26.7+/-0.3   \n",
              "5        512        60.0+/-0.5      60.0+/-0.5    20.1+/-0.3      18.7+/-11.6   \n",
              "6       1024        60.0+/-0.0      60.0+/-0.0    20.0+/-0.0       12.0+/-0.0   \n",
              "\n",
              "  F1_score(+/-sd)  \n",
              "0      31.4+/-4.1  \n",
              "1      30.0+/-1.6  \n",
              "2      28.3+/-1.5  \n",
              "3      27.1+/-2.6  \n",
              "4      23.2+/-0.6  \n",
              "5      15.2+/-0.5  \n",
              "6      15.0+/-0.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nUYMX4tApT-U"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.3.7.4 Conclusions\n",
        "Examining the results printed during the training procedure I notice that using small batch size values causes the model to converge much faster.\n",
        "\n",
        "Βased on reported metrics' values I concluse the following:\n",
        "- batch size clearly affects model's performance (no meter what metric we choose to monitor performance)\n",
        "  - the smaller the batch size the better the performance\n",
        "\n",
        "This result is expected since small batch size means that model adjusts its weights more frequently reasulting to increased overall performance. A minor side effect is that execution time increases too.\n"
      ]
    },
    {
      "metadata": {
        "id": "lQMXpMZ97BHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3.8 Plots of learning curves on train & test data\n",
        "\n",
        "I will demostrate the plot of learning curves optained during the training of a representative model\n",
        "\n",
        "I will take advance of the capability provided by Keras to register *callbacks* when training a NN model. One of the default callbacks is the *history* callback which records training metrics (accuracy and loss) on both train and validation data for each training epoch."
      ]
    },
    {
      "metadata": {
        "id": "yBUKQAxA9npe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3.3.8.1 Libraries, Functions and Model definition"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bU7Jv0lj7amt",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Slightly modified model definition that registers and returns history callback,\n",
        "# object used to plot Accuracy and Loss values during training on both train and\n",
        "# validation (test) data\n",
        "def annDef_history(od, act, los, opt, epochs):\n",
        "  # Define Model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, output_dim=od, input_length=max_length))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(5, activation=act))\n",
        "  print(model.summary())\n",
        "\n",
        "  # Compile Network\n",
        "  model.compile(loss=los, optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  # Fit Network to training data, register history callback\n",
        "  CSVLohistory = model.fit(X_train_pad, y_train, epochs=epochs, verbose=2, \n",
        "                      validation_data=(X_test_pad, y_test))\n",
        "  \n",
        "  # Print all data available in history\n",
        "  print(history.history.keys())\n",
        "  \n",
        "  # Return history callback object\n",
        "  return(history)\n",
        "\n",
        "  \n",
        "# Plot history for Accuracy on both train and validation data\n",
        "def plot_acc(history):\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('model accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "# Plot history for Loss on both train and validation data\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper right')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88rtYtLDd1nA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.3.8.2 Train model and plot training history data"
      ]
    },
    {
      "metadata": {
        "id": "8UfcVVSrd1QT",
        "colab_type": "code",
        "outputId": "23246d72-c982-47ad-ecb1-bc209a3d87cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the model and keep the training history\n",
        "history = annDef_history(100, 'sigmoid', 'mean_squared_error', 'adam', 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 54, 100)           392100    \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 5400)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 27005     \n",
            "=================================================================\n",
            "Total params: 419,105\n",
            "Trainable params: 419,105\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 820 samples, validate on 205 samples\n",
            "Epoch 1/10\n",
            " - 1s - loss: 0.1441 - acc: 0.5854 - val_loss: 0.1148 - val_acc: 0.6000\n",
            "Epoch 2/10\n",
            " - 0s - loss: 0.1086 - acc: 0.6000 - val_loss: 0.1125 - val_acc: 0.5951\n",
            "Epoch 3/10\n",
            " - 0s - loss: 0.0997 - acc: 0.6000 - val_loss: 0.1110 - val_acc: 0.5951\n",
            "Epoch 4/10\n",
            " - 0s - loss: 0.0849 - acc: 0.7024 - val_loss: 0.1074 - val_acc: 0.6098\n",
            "Epoch 5/10\n",
            " - 0s - loss: 0.0644 - acc: 0.8622 - val_loss: 0.1037 - val_acc: 0.6488\n",
            "Epoch 6/10\n",
            " - 0s - loss: 0.0466 - acc: 0.8756 - val_loss: 0.1001 - val_acc: 0.6537\n",
            "Epoch 7/10\n",
            " - 0s - loss: 0.0348 - acc: 0.9000 - val_loss: 0.0982 - val_acc: 0.6585\n",
            "Epoch 8/10\n",
            " - 0s - loss: 0.0270 - acc: 0.9220 - val_loss: 0.0973 - val_acc: 0.6585\n",
            "Epoch 9/10\n",
            " - 0s - loss: 0.0209 - acc: 0.9451 - val_loss: 0.0963 - val_acc: 0.6780\n",
            "Epoch 10/10\n",
            " - 0s - loss: 0.0162 - acc: 0.9634 - val_loss: 0.0956 - val_acc: 0.6683\n",
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_JyjJmA6cd39",
        "colab_type": "code",
        "outputId": "82a39cb5-edf6-46f2-aab9-743cb0df9ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot history for Accuracy values on both train and validation data\n",
        "plot_acc(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "V6YDYcZAeUqV",
        "colab_type": "code",
        "outputId": "943b438a-e2c2-43f7-90e1-ef56031673e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot history for Loss values on both train and validation data\n",
        "plot_loss(history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFnCAYAAAC/5tBZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npVIMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "yPRiNoPf-D3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####3.3.8.3 Conclusions\n",
        "- Train accuracy is growing rapidly during the first 4 epochs and then the rate of growth is decreasing\n",
        "- Validation accuracy shows exactly the same change profile as the train set\n",
        "- Both train and validation Loss constantly devrease during training but with decreasing rate after the 5th epoch"
      ]
    },
    {
      "metadata": {
        "id": "NeM4vMV3LSwa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}